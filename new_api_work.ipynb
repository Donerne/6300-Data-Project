{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMP News API Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully from .env file.\n"
     ]
    }
   ],
   "source": [
    "# loading in api file\n",
    "load_dotenv(r\"api.env\")\n",
    "\n",
    "# Obtaining key for API file\n",
    "fmp_api_key = os.getenv(\"fmp_api_key\")\n",
    "if not fmp_api_key:\n",
    "    raise ValueError(\"No API key set for fmp_api_key in .env file\")\n",
    "\n",
    "print(\"API key loaded successfully from .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up dictionaries in lists to facilitate looping through\n",
    "stock_news = [\n",
    "    {\"symbol\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"symbol\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"symbol\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"symbol\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"symbol\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"symbol\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "press_releases = [\n",
    "    {\"company\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"company\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"company\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"company\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"company\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"company\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "historical_social_sentiment = [\n",
    "    {\"symbol\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"symbol\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"symbol\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"symbol\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"symbol\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"symbol\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# setting up lists to loop through for sentiment score and weighted scores analysis\n",
    "stock_news_datasets = [\n",
    "    \"AMZN_news_data.csv\",\n",
    "    \"AAPL_news_data.csv\",\n",
    "    \"GOOG_news_data.csv\",\n",
    "    \"MSFT_news_data.csv\",\n",
    "    \"META_news_data.csv\",\n",
    "    \"NVDA_news_data.csv\"\n",
    "]\n",
    "\n",
    "news_press_releases_datasets = [\n",
    "    \"AMZN_press_release_data.csv\",\n",
    "    \"AAPL_press_release_data.csv\",\n",
    "    \"GOOG_press_release_data.csv\",\n",
    "    \"MSFT_press_release_data.csv\",\n",
    "    \"META_press_release_data.csv\",\n",
    "    \"NVDA_press_release_data.csv\"\n",
    "]\n",
    "\n",
    "social_sentiments_datasets = [\n",
    "    \"AMZN_social_sentiment_data.csv\",\n",
    "    \"AAPL_social_sentiment_data.csv\",\n",
    "    \"GOOG_social_sentiment_data.csv\",\n",
    "    \"MSFT_social_sentiment_data.csv\",\n",
    "    \"META_social_sentiment_data.csv\",\n",
    "    \"NVDA_social_sentiment_data.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock News Data Pull "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for AMZN written to AMZN_news_data.csv with 29416 articles!\n",
      "Data for AAPL written to AAPL_news_data.csv with 26571 articles!\n",
      "Data for GOOG written to GOOG_news_data.csv with 17259 articles!\n",
      "Data for MSFT written to MSFT_news_data.csv with 16818 articles!\n",
      "Data for META written to META_news_data.csv with 18497 articles!\n",
      "Data for NVDA written to NVDA_news_data.csv with 19356 articles!\n",
      "All stock data successfully written to their respective CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Obtaining today's date\n",
    "current_date =date.today()\n",
    "\n",
    "# Obtaining stock news data for each tech company\n",
    "for news in stock_news:\n",
    "\n",
    "    # setting up url path & parameter values\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/stock_news?\"\n",
    "    page = 0\n",
    "    limit = 5000\n",
    "\n",
    "    # creating empty list to capture news data\n",
    "    all_news = []\n",
    "\n",
    "    # looping through pages of data to obtain stock news data\n",
    "    while True:\n",
    "        \n",
    "        # applying parameter values to required params\n",
    "        params = {\n",
    "        \"apikey\" : fmp_api_key,\n",
    "        \"tickers\" : news['symbol'],\n",
    "        \"page\": page,\n",
    "        'from': news['ipo_date'],\n",
    "        'to': current_date,\n",
    "        'limit': limit\n",
    "        }\n",
    "\n",
    "        # requesting data\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {news['symbol']} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        # setting successfully requested data to variable data\n",
    "        data = response.json()\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # appending additional data to list\n",
    "        all_news.extend(data)\n",
    "\n",
    "        # moving to next page\n",
    "        page += 1\n",
    "\n",
    "    csv_file = f\"{news['symbol']}_news_data.csv\"\n",
    "    headers = [\"published_date\", \"Headline\", \"Brief\", \"URL\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    # writing all pulled news data to their respective companies in csv files\n",
    "    if all_news:\n",
    "        for record in all_news:\n",
    "\n",
    "            date_str = record.get(\"publishedDate\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "            formatted_data.append({\n",
    "            \"published_date\": date_str,\n",
    "            \"Headline\": record.get(\"title\", \"\"),\n",
    "            \"Brief\": record.get(\"text\", \"\"),\n",
    "            \"URL\": record.get(\"url\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "        \n",
    "        print(f\"Data for {news['symbol']} written to {csv_file} with {len(formatted_data)} articles!\")\n",
    "    else:\n",
    "        print(f\"No data found for {news['symbol']}.\")\n",
    "\n",
    "print(\"All stock data successfully written to their respective CSV files.\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Press Release Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for AMZN - Page 0...\n",
      "Fetching data for AMZN - Page 1...\n",
      "Fetching data for AMZN - Page 2...\n",
      "Fetching data for AMZN - Page 3...\n",
      "Fetching data for AMZN - Page 4...\n",
      "Fetching data for AMZN - Page 5...\n",
      "Fetching data for AMZN - Page 6...\n",
      "Fetching data for AMZN - Page 7...\n",
      "Fetching data for AMZN - Page 8...\n",
      "Fetching data for AMZN - Page 9...\n",
      "Fetching data for AMZN - Page 10...\n",
      "Fetching data for AMZN - Page 11...\n",
      "Fetching data for AMZN - Page 12...\n",
      "Fetching data for AMZN - Page 13...\n",
      "Fetching data for AMZN - Page 14...\n",
      "Fetching data for AMZN - Page 15...\n",
      "Fetching data for AMZN - Page 16...\n",
      "Fetching data for AMZN - Page 17...\n",
      "Fetching data for AMZN - Page 18...\n",
      "Fetching data for AMZN - Page 19...\n",
      "Fetching data for AMZN - Page 20...\n",
      "Fetching data for AMZN - Page 21...\n",
      "Fetching data for AMZN - Page 22...\n",
      "Fetching data for AMZN - Page 23...\n",
      "Fetching data for AMZN - Page 24...\n",
      "Fetching data for AMZN - Page 25...\n",
      "Fetching data for AMZN - Page 26...\n",
      "Fetching data for AMZN - Page 27...\n",
      "Fetching data for AMZN - Page 28...\n",
      "Fetching data for AMZN - Page 29...\n",
      "Fetching data for AMZN - Page 30...\n",
      "Fetching data for AMZN - Page 31...\n",
      "Fetching data for AMZN - Page 32...\n",
      "Fetching data for AMZN - Page 33...\n",
      "Fetching data for AMZN - Page 34...\n",
      "Fetching data for AMZN - Page 35...\n",
      "Fetching data for AMZN - Page 36...\n",
      "Fetching data for AMZN - Page 37...\n",
      "Fetching data for AMZN - Page 38...\n",
      "Fetching data for AMZN - Page 39...\n",
      "Fetching data for AMZN - Page 40...\n",
      "Fetching data for AMZN - Page 41...\n",
      "Fetching data for AMZN - Page 42...\n",
      "Fetching data for AMZN - Page 43...\n",
      "Fetching data for AMZN - Page 44...\n",
      "Fetching data for AMZN - Page 45...\n",
      "Fetching data for AMZN - Page 46...\n",
      "Fetching data for AMZN - Page 47...\n",
      "Fetching data for AMZN - Page 48...\n",
      "Fetching data for AMZN - Page 49...\n",
      "Fetching data for AMZN - Page 50...\n",
      "Fetching data for AMZN - Page 51...\n",
      "Fetching data for AMZN - Page 52...\n",
      "Fetching data for AMZN - Page 53...\n",
      "Fetching data for AMZN - Page 54...\n",
      "Fetching data for AMZN - Page 55...\n",
      "Fetching data for AMZN - Page 56...\n",
      "Fetching data for AMZN - Page 57...\n",
      "Fetching data for AMZN - Page 58...\n",
      "Fetching data for AMZN - Page 59...\n",
      "Fetching data for AMZN - Page 60...\n",
      "Fetching data for AMZN - Page 61...\n",
      "Fetching data for AMZN - Page 62...\n",
      "Fetching data for AMZN - Page 63...\n",
      "Fetching data for AMZN - Page 64...\n",
      "Fetching data for AMZN - Page 65...\n",
      "Fetching data for AMZN - Page 66...\n",
      "Fetching data for AMZN - Page 67...\n",
      "Fetching data for AMZN - Page 68...\n",
      "Fetching data for AMZN - Page 69...\n",
      "Fetching data for AMZN - Page 70...\n",
      "Fetching data for AMZN - Page 71...\n",
      "Fetching data for AMZN - Page 72...\n",
      "Fetching data for AMZN - Page 73...\n",
      "Fetching data for AMZN - Page 74...\n",
      "Fetching data for AMZN - Page 75...\n",
      "Fetching data for AMZN - Page 76...\n",
      "Fetching data for AMZN - Page 77...\n",
      "Fetching data for AMZN - Page 78...\n",
      "Fetching data for AMZN - Page 79...\n",
      "Fetching data for AMZN - Page 80...\n",
      "Fetching data for AMZN - Page 81...\n",
      "Fetching data for AMZN - Page 82...\n",
      "Fetching data for AMZN - Page 83...\n",
      "Fetching data for AMZN - Page 84...\n",
      "Fetching data for AMZN - Page 85...\n",
      "Fetching data for AMZN - Page 86...\n",
      "Fetching data for AMZN - Page 87...\n",
      "Fetching data for AMZN - Page 88...\n",
      "Fetching data for AMZN - Page 89...\n",
      "Fetching data for AMZN - Page 90...\n",
      "Fetching data for AMZN - Page 91...\n",
      "Fetching data for AMZN - Page 92...\n",
      "Fetching data for AMZN - Page 93...\n",
      "Fetching data for AMZN - Page 94...\n",
      "Fetching data for AMZN - Page 95...\n",
      "Fetching data for AMZN - Page 96...\n",
      "Fetching data for AMZN - Page 97...\n",
      "Fetching data for AMZN - Page 98...\n",
      "Fetching data for AMZN - Page 99...\n",
      "Fetching data for AMZN - Page 100...\n",
      "Fetching data for AMZN - Page 101...\n",
      "Fetching data for AMZN - Page 102...\n",
      "Fetching data for AMZN - Page 103...\n",
      "Fetching data for AMZN - Page 104...\n",
      "Fetching data for AMZN - Page 105...\n",
      "Fetching data for AMZN - Page 106...\n",
      "Fetching data for AMZN - Page 107...\n",
      "Fetching data for AMZN - Page 108...\n",
      "Fetching data for AMZN - Page 109...\n",
      "Fetching data for AMZN - Page 110...\n",
      "Fetching data for AMZN - Page 111...\n",
      "Fetching data for AMZN - Page 112...\n",
      "Fetching data for AMZN - Page 113...\n",
      "Fetching data for AMZN - Page 114...\n",
      "Fetching data for AMZN - Page 115...\n",
      "Fetching data for AMZN - Page 116...\n",
      "Fetching data for AMZN - Page 117...\n",
      "Fetching data for AMZN - Page 118...\n",
      "Fetching data for AMZN - Page 119...\n",
      "Fetching data for AMZN - Page 120...\n",
      "Fetching data for AMZN - Page 121...\n",
      "Fetching data for AMZN - Page 122...\n",
      "Fetching data for AMZN - Page 123...\n",
      "Fetching data for AMZN - Page 124...\n",
      "Fetching data for AMZN - Page 125...\n",
      "Fetching data for AMZN - Page 126...\n",
      "Fetching data for AMZN - Page 127...\n",
      "Fetching data for AMZN - Page 128...\n",
      "Fetching data for AMZN - Page 129...\n",
      "Fetching data for AMZN - Page 130...\n",
      "Fetching data for AMZN - Page 131...\n",
      "Fetching data for AMZN - Page 132...\n",
      "Fetching data for AMZN - Page 133...\n",
      "Fetching data for AMZN - Page 134...\n",
      "Fetching data for AMZN - Page 135...\n",
      "Fetching data for AMZN - Page 136...\n",
      "Fetching data for AMZN - Page 137...\n",
      "Fetching data for AMZN - Page 138...\n",
      "Fetching data for AMZN - Page 139...\n",
      "Fetching data for AMZN - Page 140...\n",
      "Fetching data for AMZN - Page 141...\n",
      "Fetching data for AMZN - Page 142...\n",
      "Fetching data for AMZN - Page 143...\n",
      "Fetching data for AMZN - Page 144...\n",
      "Fetching data for AMZN - Page 145...\n",
      "Fetching data for AMZN - Page 146...\n",
      "Fetching data for AMZN - Page 147...\n",
      "Fetching data for AMZN - Page 148...\n",
      "Fetching data for AMZN - Page 149...\n",
      "Fetching data for AMZN - Page 150...\n",
      "Fetching data for AMZN - Page 151...\n",
      "Fetching data for AMZN - Page 152...\n",
      "Fetching data for AMZN - Page 153...\n",
      "Fetching data for AMZN - Page 154...\n",
      "Fetching data for AMZN - Page 155...\n",
      "Fetching data for AMZN - Page 156...\n",
      "Fetching data for AMZN - Page 157...\n",
      "Fetching data for AMZN - Page 158...\n",
      "Fetching data for AMZN - Page 159...\n",
      "Fetching data for AMZN - Page 160...\n",
      "Fetching data for AMZN - Page 161...\n",
      "Fetching data for AMZN - Page 162...\n",
      "Fetching data for AMZN - Page 163...\n",
      "Fetching data for AMZN - Page 164...\n",
      "Fetching data for AMZN - Page 165...\n",
      "Fetching data for AMZN - Page 166...\n",
      "Fetching data for AMZN - Page 167...\n",
      "Fetching data for AMZN - Page 168...\n",
      "Fetching data for AMZN - Page 169...\n",
      "Fetching data for AMZN - Page 170...\n",
      "Fetching data for AMZN - Page 171...\n",
      "Fetching data for AMZN - Page 172...\n",
      "Fetching data for AMZN - Page 173...\n",
      "Fetching data for AMZN - Page 174...\n",
      "Fetching data for AMZN - Page 175...\n",
      "Fetching data for AMZN - Page 176...\n",
      "Fetching data for AMZN - Page 177...\n",
      "Fetching data for AMZN - Page 178...\n",
      "Fetching data for AMZN - Page 179...\n",
      "Fetching data for AMZN - Page 180...\n",
      "Fetching data for AMZN - Page 181...\n",
      "Fetching data for AMZN - Page 182...\n",
      "Fetching data for AMZN - Page 183...\n",
      "Fetching data for AMZN - Page 184...\n",
      "Fetching data for AMZN - Page 185...\n",
      "Fetching data for AMZN - Page 186...\n",
      "Fetching data for AMZN - Page 187...\n",
      "Fetching data for AMZN - Page 188...\n",
      "Fetching data for AMZN - Page 189...\n",
      "Fetching data for AMZN - Page 190...\n",
      "Fetching data for AMZN - Page 191...\n",
      "Fetching data for AMZN - Page 192...\n",
      "Fetching data for AMZN - Page 193...\n",
      "Fetching data for AMZN - Page 194...\n",
      "Fetching data for AMZN - Page 195...\n",
      "Fetching data for AMZN - Page 196...\n",
      "Fetching data for AMZN - Page 197...\n",
      "Fetching data for AMZN - Page 198...\n",
      "Fetching data for AMZN - Page 199...\n",
      "Fetching data for AMZN - Page 200...\n",
      "Fetching data for AMZN - Page 201...\n",
      "Fetching data for AMZN - Page 202...\n",
      "Fetching data for AMZN - Page 203...\n",
      "Fetching data for AMZN - Page 204...\n",
      "Fetching data for AMZN - Page 205...\n",
      "Fetching data for AMZN - Page 206...\n",
      "Fetching data for AMZN - Page 207...\n",
      "Fetching data for AMZN - Page 208...\n",
      "Fetching data for AMZN - Page 209...\n",
      "Fetching data for AMZN - Page 210...\n",
      "Fetching data for AMZN - Page 211...\n",
      "Fetching data for AMZN - Page 212...\n",
      "Fetching data for AMZN - Page 213...\n",
      "Fetching data for AMZN - Page 214...\n",
      "Fetching data for AMZN - Page 215...\n",
      "Fetching data for AMZN - Page 216...\n",
      "Fetching data for AMZN - Page 217...\n",
      "Fetching data for AMZN - Page 218...\n",
      "Fetching data for AMZN - Page 219...\n",
      "Fetching data for AMZN - Page 220...\n",
      "Fetching data for AMZN - Page 221...\n",
      "Fetching data for AMZN - Page 222...\n",
      "Fetching data for AMZN - Page 223...\n",
      "Fetching data for AMZN - Page 224...\n",
      "Fetching data for AMZN - Page 225...\n",
      "Fetching data for AMZN - Page 226...\n",
      "Fetching data for AMZN - Page 227...\n",
      "Fetching data for AMZN - Page 228...\n",
      "Fetching data for AMZN - Page 229...\n",
      "Fetching data for AMZN - Page 230...\n",
      "Fetching data for AMZN - Page 231...\n",
      "Fetching data for AMZN - Page 232...\n",
      "Fetching data for AMZN - Page 233...\n",
      "Fetching data for AMZN - Page 234...\n",
      "Fetching data for AMZN - Page 235...\n",
      "Fetching data for AMZN - Page 236...\n",
      "Fetching data for AMZN - Page 237...\n",
      "Fetching data for AMZN - Page 238...\n",
      "Fetching data for AMZN - Page 239...\n",
      "Fetching data for AMZN - Page 240...\n",
      "Fetching data for AMZN - Page 241...\n",
      "Fetching data for AMZN - Page 242...\n",
      "Fetching data for AMZN - Page 243...\n",
      "Fetching data for AMZN - Page 244...\n",
      "Fetching data for AMZN - Page 245...\n",
      "Fetching data for AMZN - Page 246...\n",
      "Fetching data for AMZN - Page 247...\n",
      "Fetching data for AMZN - Page 248...\n",
      "Fetching data for AMZN - Page 249...\n",
      "Fetching data for AMZN - Page 250...\n",
      "Fetching data for AMZN - Page 251...\n",
      "Fetching data for AMZN - Page 252...\n",
      "Fetching data for AMZN - Page 253...\n",
      "Fetching data for AMZN - Page 254...\n",
      "Fetching data for AMZN - Page 255...\n",
      "Fetching data for AMZN - Page 256...\n",
      "Fetching data for AMZN - Page 257...\n",
      "Fetching data for AMZN - Page 258...\n",
      "Fetching data for AMZN - Page 259...\n",
      "Fetching data for AMZN - Page 260...\n",
      "Fetching data for AMZN - Page 261...\n",
      "Fetching data for AMZN - Page 262...\n",
      "Fetching data for AMZN - Page 263...\n",
      "Fetching data for AMZN - Page 264...\n",
      "Fetching data for AMZN - Page 265...\n",
      "Fetching data for AMZN - Page 266...\n",
      "Fetching data for AMZN - Page 267...\n",
      "Fetching data for AMZN - Page 268...\n",
      "Fetching data for AMZN - Page 269...\n",
      "Fetching data for AMZN - Page 270...\n",
      "Fetching data for AMZN - Page 271...\n",
      "Fetching data for AMZN - Page 272...\n",
      "Fetching data for AMZN - Page 273...\n",
      "Fetching data for AMZN - Page 274...\n",
      "Fetching data for AMZN - Page 275...\n",
      "Fetching data for AMZN - Page 276...\n",
      "Fetching data for AMZN - Page 277...\n",
      "Fetching data for AMZN - Page 278...\n",
      "Fetching data for AMZN - Page 279...\n",
      "Fetching data for AMZN - Page 280...\n",
      "Fetching data for AMZN - Page 281...\n",
      "Fetching data for AMZN - Page 282...\n",
      "Fetching data for AMZN - Page 283...\n",
      "Fetching data for AMZN - Page 284...\n",
      "Fetching data for AMZN - Page 285...\n",
      "Fetching data for AMZN - Page 286...\n",
      "Fetching data for AMZN - Page 287...\n",
      "Fetching data for AMZN - Page 288...\n",
      "Fetching data for AMZN - Page 289...\n",
      "Fetching data for AMZN - Page 290...\n",
      "Fetching data for AMZN - Page 291...\n",
      "Fetching data for AMZN - Page 292...\n",
      "Fetching data for AMZN - Page 293...\n",
      "Fetching data for AMZN - Page 294...\n",
      "Fetching data for AMZN - Page 295...\n",
      "Fetching data for AMZN - Page 296...\n",
      "Fetching data for AMZN - Page 297...\n",
      "Fetching data for AMZN - Page 298...\n",
      "Fetching data for AMZN - Page 299...\n",
      "Data for AMZN written to AMZN_press_release_data.csv with 29609 press releases!\n",
      "Fetching data for AAPL - Page 0...\n",
      "Fetching data for AAPL - Page 1...\n",
      "Fetching data for AAPL - Page 2...\n",
      "Fetching data for AAPL - Page 3...\n",
      "Fetching data for AAPL - Page 4...\n",
      "Fetching data for AAPL - Page 5...\n",
      "Fetching data for AAPL - Page 6...\n",
      "Fetching data for AAPL - Page 7...\n",
      "Fetching data for AAPL - Page 8...\n",
      "Fetching data for AAPL - Page 9...\n",
      "Fetching data for AAPL - Page 10...\n",
      "Fetching data for AAPL - Page 11...\n",
      "Data for AAPL written to AAPL_press_release_data.csv with 888 press releases!\n",
      "Fetching data for GOOG - Page 0...\n",
      "Fetching data for GOOG - Page 1...\n",
      "Fetching data for GOOG - Page 2...\n",
      "Fetching data for GOOG - Page 3...\n",
      "Data for GOOG written to GOOG_press_release_data.csv with 189 press releases!\n",
      "Fetching data for MSFT - Page 0...\n",
      "Fetching data for MSFT - Page 1...\n",
      "Fetching data for MSFT - Page 2...\n",
      "Fetching data for MSFT - Page 3...\n",
      "Fetching data for MSFT - Page 4...\n",
      "Fetching data for MSFT - Page 5...\n",
      "Fetching data for MSFT - Page 6...\n",
      "Fetching data for MSFT - Page 7...\n",
      "Fetching data for MSFT - Page 8...\n",
      "Fetching data for MSFT - Page 9...\n",
      "Data for MSFT written to MSFT_press_release_data.csv with 851 press releases!\n",
      "Fetching data for META - Page 0...\n",
      "Fetching data for META - Page 1...\n",
      "Fetching data for META - Page 2...\n",
      "Data for META written to META_press_release_data.csv with 137 press releases!\n",
      "Fetching data for NVDA - Page 0...\n",
      "Fetching data for NVDA - Page 1...\n",
      "Fetching data for NVDA - Page 2...\n",
      "Fetching data for NVDA - Page 3...\n",
      "Fetching data for NVDA - Page 4...\n",
      "Fetching data for NVDA - Page 5...\n",
      "Fetching data for NVDA - Page 6...\n",
      "Fetching data for NVDA - Page 7...\n",
      "Data for NVDA written to NVDA_press_release_data.csv with 604 press releases!\n",
      "All stock data successfully written to their respective CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Obtaining stock press release data for each tech company\n",
    "for press_release in press_releases:\n",
    "\n",
    "    # setting up url path & parameter values\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/press-releases/{press_release['company']}?\"\n",
    "\n",
    "    # setting page to zero\n",
    "    page = 0\n",
    "\n",
    "    # creating empty list to capture all press release data\n",
    "    all_press_releases = []\n",
    "\n",
    "    max_pages = 300\n",
    "\n",
    "    # looping through pages of data to obtain stock press release data\n",
    "    while page < max_pages:\n",
    "\n",
    "        # applying parameter values to required params\n",
    "        params = {\n",
    "        \"apikey\" : fmp_api_key,\n",
    "        \"page\": page\n",
    "        }\n",
    "\n",
    "        # requesting data\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {press_release['company']} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # setting successfully requested data to variable data\n",
    "        data = response.json()\n",
    "        \n",
    "        print(f\"Fetching data for {press_release['company']} - Page {page}...\")\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "\n",
    "        # appending additional data to list\n",
    "        all_press_releases.extend(data)\n",
    "\n",
    "        # moving to next page\n",
    "        page += 1\n",
    "\n",
    "    csv_file = f\"{press_release['company']}_press_release_data.csv\"\n",
    "    headers = [\"release_date\", \"Headline\", \"Brief\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    # writing all pulled press release data to their respective companies in csv files\n",
    "    if all_press_releases:\n",
    "        for record in all_press_releases:\n",
    "\n",
    "            date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "            formatted_data.append({\n",
    "            \"release_date\": date_str,\n",
    "            \"Headline\": record.get(\"title\", \"\"),\n",
    "            \"Brief\": record.get(\"text\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "        \n",
    "        print(f\"Data for {press_release['company']} written to {csv_file} with {len(formatted_data)} press releases!\")\n",
    "    else:\n",
    "        print(f\"No data found for {press_release['company']}.\")\n",
    "\n",
    "print(\"All stock data successfully written to their respective CSV files.\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiments Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Response for AMZN, Page 0: 68 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 1: 63 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 2: 67 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 3: 63 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 4: 71 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 5: 72 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 6: 67 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 7: 68 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 8: 62 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 9: 62 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 10: 68 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 11: 66 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 12: 68 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 13: 69 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 14: 67 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 15: 72 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 16: 65 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 17: 72 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 18: 70 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 19: 77 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 20: 73 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 21: 74 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 22: 79 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 23: 74 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 24: 74 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 25: 167 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 26: 65 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 27: 65 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 28: 69 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 29: 64 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 30: 22 records\n",
      "\n",
      "ðŸ” Response for AMZN, Page 31: 0 records\n",
      "No sentiment data found for AMZN.\n",
      "Sentiment data for AMZN written to AMZN_social_sentiment_data.csv with 2183 records!\n",
      "\n",
      "ðŸ” Response for AAPL, Page 0: 67 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 1: 64 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 2: 68 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 3: 66 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 4: 68 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 5: 68 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 6: 64 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 7: 66 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 8: 63 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 9: 62 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 10: 61 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 11: 62 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 12: 61 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 13: 65 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 14: 61 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 15: 68 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 16: 66 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 17: 63 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 18: 68 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 19: 67 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 20: 79 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 21: 66 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 22: 69 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 23: 70 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 24: 68 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 25: 66 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 26: 166 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 27: 67 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 28: 62 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 29: 72 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 30: 67 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 31: 37 records\n",
      "\n",
      "ðŸ” Response for AAPL, Page 32: 0 records\n",
      "No sentiment data found for AAPL.\n",
      "Sentiment data for AAPL written to AAPL_social_sentiment_data.csv with 2187 records!\n",
      "\n",
      "ðŸ” Response for GOOG, Page 0: 0 records\n",
      "No sentiment data found for GOOG.\n",
      "No sentiment data found for GOOG.\n",
      "\n",
      "ðŸ” Response for MSFT, Page 0: 76 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 1: 64 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 2: 71 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 3: 68 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 4: 82 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 5: 67 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 6: 72 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 7: 73 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 8: 64 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 9: 66 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 10: 63 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 11: 61 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 12: 69 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 13: 64 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 14: 83 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 15: 72 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 16: 74 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 17: 79 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 18: 77 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 19: 80 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 20: 90 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 21: 88 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 22: 86 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 23: 94 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 24: 150 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 25: 66 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 26: 75 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 27: 70 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 28: 38 records\n",
      "\n",
      "ðŸ” Response for MSFT, Page 29: 0 records\n",
      "No sentiment data found for MSFT.\n",
      "Sentiment data for MSFT written to MSFT_social_sentiment_data.csv with 2182 records!\n",
      "\n",
      "ðŸ” Response for META, Page 0: 74 records\n",
      "\n",
      "ðŸ” Response for META, Page 1: 65 records\n",
      "\n",
      "ðŸ” Response for META, Page 2: 70 records\n",
      "\n",
      "ðŸ” Response for META, Page 3: 64 records\n",
      "\n",
      "ðŸ” Response for META, Page 4: 73 records\n",
      "\n",
      "ðŸ” Response for META, Page 5: 65 records\n",
      "\n",
      "ðŸ” Response for META, Page 6: 65 records\n",
      "\n",
      "ðŸ” Response for META, Page 7: 70 records\n",
      "\n",
      "ðŸ” Response for META, Page 8: 65 records\n",
      "\n",
      "ðŸ” Response for META, Page 9: 64 records\n",
      "\n",
      "ðŸ” Response for META, Page 10: 67 records\n",
      "\n",
      "ðŸ” Response for META, Page 11: 61 records\n",
      "\n",
      "ðŸ” Response for META, Page 12: 63 records\n",
      "\n",
      "ðŸ” Response for META, Page 13: 69 records\n",
      "\n",
      "ðŸ” Response for META, Page 14: 63 records\n",
      "\n",
      "ðŸ” Response for META, Page 15: 65 records\n",
      "\n",
      "ðŸ” Response for META, Page 16: 65 records\n",
      "\n",
      "ðŸ” Response for META, Page 17: 69 records\n",
      "\n",
      "ðŸ” Response for META, Page 18: 72 records\n",
      "\n",
      "ðŸ” Response for META, Page 19: 70 records\n",
      "\n",
      "ðŸ” Response for META, Page 20: 84 records\n",
      "\n",
      "ðŸ” Response for META, Page 21: 95 records\n",
      "\n",
      "ðŸ” Response for META, Page 22: 86 records\n",
      "\n",
      "ðŸ” Response for META, Page 23: 93 records\n",
      "\n",
      "ðŸ” Response for META, Page 24: 180 records\n",
      "\n",
      "ðŸ” Response for META, Page 25: 74 records\n",
      "\n",
      "ðŸ” Response for META, Page 26: 76 records\n",
      "\n",
      "ðŸ” Response for META, Page 27: 88 records\n",
      "\n",
      "ðŸ” Response for META, Page 28: 69 records\n",
      "\n",
      "ðŸ” Response for META, Page 29: 1 records\n",
      "\n",
      "ðŸ” Response for META, Page 30: 0 records\n",
      "No sentiment data found for META.\n",
      "Sentiment data for META written to META_social_sentiment_data.csv with 2185 records!\n",
      "\n",
      "ðŸ” Response for NVDA, Page 0: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 1: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 2: 62 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 3: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 4: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 5: 63 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 6: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 7: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 8: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 9: 60 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 10: 60 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 11: 60 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 12: 60 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 13: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 14: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 15: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 16: 62 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 17: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 18: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 19: 62 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 20: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 21: 92 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 22: 62 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 23: 62 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 24: 62 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 25: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 26: 62 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 27: 62 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 28: 154 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 29: 62 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 30: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 31: 62 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 32: 61 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 33: 42 records\n",
      "\n",
      "ðŸ” Response for NVDA, Page 34: 0 records\n",
      "No sentiment data found for NVDA.\n",
      "Sentiment data for NVDA written to NVDA_social_sentiment_data.csv with 2187 records!\n",
      "\n",
      "All sentiment data successfully written to CSV files.\n"
     ]
    }
   ],
   "source": [
    "# current_date = date.today()\n",
    " \n",
    "# Loop through each stock symbol to fetch sentiment data\n",
    "for social_sentiment in historical_social_sentiment:\n",
    "    symbol = social_sentiment[\"symbol\"]\n",
    "    ipo_date = social_sentiment[\"ipo_date\"]\n",
    " \n",
    "    # Correct API URL\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/historical/social-sentiment?symbol={symbol}\"\n",
    " \n",
    "    page = 0\n",
    "    all_historical_social_sentiment = []\n",
    " \n",
    "    while True:\n",
    "        social_params = {\n",
    "            \"apikey\": fmp_api_key,\n",
    "            \"page\": page\n",
    "        }\n",
    " \n",
    "        try:\n",
    "            response = requests.get(url, params=social_params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    " \n",
    "            # Print API response for debugging\n",
    "            print(f\"\\nðŸ” Response for {symbol}, Page {page}: {len(data)} records\")\n",
    " \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {symbol} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "        time.sleep(1)\n",
    " \n",
    "        # Stop if no data is returned\n",
    "        if not data:\n",
    "            print(f\"No sentiment data found for {symbol}.\")\n",
    "            break\n",
    " \n",
    "        all_historical_social_sentiment.extend(data)\n",
    "        page += 1  # Move to the next page\n",
    " \n",
    "    # CSV file name (each company has its own file)\n",
    "    csv_file = f\"{symbol}_social_sentiment_data.csv\"\n",
    " \n",
    "    # Updated headers to match API response\n",
    "    headers = [\n",
    "        \"published_date\", \"twitter_posts\", \"twitter_likes\", \"twitter_sentiment\",\n",
    "        \"stock_twitter_posts\", \"stock_twitter_likes\", \"sentiment_score\"\n",
    "    ]\n",
    " \n",
    "    formatted_data = []\n",
    " \n",
    "    if all_historical_social_sentiment:\n",
    "        for record in all_historical_social_sentiment:\n",
    "            # Extract only the date part\n",
    "            date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    " \n",
    "            formatted_data.append({\n",
    "                \"published_date\": date_str,\n",
    "                \"twitter_posts\": record.get(\"twitterPosts\", \"\"),\n",
    "                \"twitter_likes\": record.get(\"twitterLikes\", \"\"),\n",
    "                \"twitter_sentiment\": record.get(\"twitterSentiment\", \"\"),\n",
    "                \"stock_twitter_posts\": record.get(\"stocktwitsPosts\", \"\"),\n",
    "                \"stock_twitter_likes\": record.get(\"stocktwitsLikes\", \"\"),  # Fixed duplicate key\n",
    "                \"sentiment_score\": record.get(\"stocktwitsSentiment\", \"\")\n",
    "            })\n",
    " \n",
    "        # Write data to a CSV file for this company\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    " \n",
    "        print(f\"Sentiment data for {symbol} written to {csv_file} with {len(formatted_data)} records!\")\n",
    "    else:\n",
    "        print(f\"No sentiment data found for {symbol}.\")\n",
    " \n",
    "print(\"\\nAll sentiment data successfully written to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text PreProcessing & Function Creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating a function to clean text for VADER sentiment analysis\n",
    "def text_cleaning(text, dataset):\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace(r\"http\\S+|www\\S+\", \"\", regex=True) # removing URLS\n",
    "    text = text.str.replace(r\"[^a-zA-Z0-9$%.,!?'\\s-]\", \"\", regex=True) # keeping relevant characters\n",
    "    text = text.str.replace(r\"\\s+\", \" \", regex=True).str.strip() # removing extra spaces\n",
    "    text = text.str.replace(r\"\\b(\\d+)%\", r\"\\1 percent\", regex=True) # converting percentages\n",
    "    text = text.str.replace(r\"\\b(\\d+)M\\b\", r\"\\1 million\", regex=True)  # Convert M to million\n",
    "    text = text.str.replace(r\"\\b(\\d+)B\\b\", r\"\\1 billion\", regex=True)  # Convert B to billion\n",
    "\n",
    "    # splitting texts manually based on \".!?\"\n",
    "    sentences = text.str.split(r'[.!?]\\s+', regex=True)\n",
    "    \n",
    "    # creating column for the cleaned text and naming it 'cleaned'\n",
    "    dataset['cleaned'] = sentences\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to obtain sentiment scores\n",
    "def sentiment_score_calculator(datasets_lists):\n",
    "\n",
    "    # Initializing VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "    # looping through datasets \n",
    "    for file in datasets_lists:\n",
    "        \n",
    "        # reading in file \n",
    "        news_press_release = pd.read_csv(file)\n",
    "\n",
    "        # combining two text columns into one column to clean text in one go\n",
    "        news_press_release['raw'] = news_press_release.apply(lambda row: row['Headline'] if pd.isna(row['Brief']) \n",
    "                                                        else (row['Brief'] if pd.isna(row['Headline']) \n",
    "                                                            else row['Headline'] + \". \" + row['Brief']), axis=1)\n",
    "\n",
    "        # cleaning text cleaning function with VADER appliation\n",
    "        text_cleaning(news_press_release['raw'], news_press_release)\n",
    "\n",
    "\n",
    "        # creating empty lists\n",
    "        score_rating = []\n",
    "        score = []\n",
    "\n",
    "        # looping through each row in the column 'cleaned'\n",
    "        for row in news_press_release['cleaned']:\n",
    "            \n",
    "            row_score = 0\n",
    "\n",
    "            # looping through each sentence in current row and using vader to score each sentence\n",
    "            for sentence in row:\n",
    "                try:\n",
    "                    print(sentence)\n",
    "                    sentence_score = analyzer.polarity_scores(sentence)[\"compound\"] # vader polarity score analyzer\n",
    "                    print(sentence_score)\n",
    "                    print(\"Done with sentence_score in row, onto next sentence.\")\n",
    "\n",
    "                    # accumulating each sentence's score for current row\n",
    "                    row_score += sentence_score\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error processing sentence: {sentence}\\n{e}\")\n",
    "                    continue\n",
    "\n",
    "            # averaging row scores and appending to score\n",
    "            avg_row_score = row_score / len(row)\n",
    "            score.append(avg_row_score)\n",
    "\n",
    "            print(f\"row_score: {avg_row_score}\")\n",
    "            print(len(row))\n",
    "            print(\"end of row, onto next row.\")\n",
    "\n",
    "            # classifying averaged row scores into various score ratings\n",
    "            if avg_row_score > 0.05 and avg_row_score < 0.5:\n",
    "                score_rating.append(\"weakly_positive\")\n",
    "                print(\"Weakly Positive\")\n",
    "            elif avg_row_score > 0.5:\n",
    "                score_rating.append(\"strongly_positive\")\n",
    "                print(\"Strongly Positive\")\n",
    "            elif avg_row_score > -0.5 and avg_row_score < -0.05:\n",
    "                score_rating.append(\"weakly_negative\")\n",
    "                print(\"Weakly Negative\")\n",
    "            elif avg_row_score < -0.5:\n",
    "                score_rating.append(\"strongly_negative\")\n",
    "                print(\"Strongly Negative\")\n",
    "            else:\n",
    "                score_rating.append(\"neutral\")\n",
    "                print(\"Neutral\")\n",
    "\n",
    "        # creating columns for sentiment and score\n",
    "        news_press_release['sentiment'] = score_rating\n",
    "        news_press_release['sentiment_score'] = score\n",
    "\n",
    "        # writing the updated vader sentiment scores to each file \n",
    "        news_press_release.to_csv(file, index=False)\n",
    "        print(f\"Finished updating {file} with sentiment rating and sentiment score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_sentiments(datasets_lists):\n",
    "\n",
    "    # looping through each dataset and applying aggregated sentiments\n",
    "    for file in datasets_lists:\n",
    "\n",
    "        if not file or not os.path.exists(file):\n",
    "            print(f\"Skipping missing file: {file}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # reading in dataset\n",
    "            news_press_release_sentiment = pd.read_csv(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "        \n",
    "\n",
    "        # Checking if either 'published_date' or 'release_date' exists and group by the first one found\n",
    "        date_column = 'published_date' if 'published_date' in news_press_release_sentiment.columns else 'release_date'\n",
    "\n",
    "        # computing average score per unique publilshed_date\n",
    "        avg_scores = news_press_release_sentiment.groupby(date_column)['sentiment_score'].mean().reset_index()\n",
    "\n",
    "        # classifying weighted daily scores into classes\n",
    "\n",
    "        # Defining conditions for classification\n",
    "        conditions = [\n",
    "            avg_scores['sentiment_score'] > 0.5,\n",
    "            (avg_scores['sentiment_score'] > 0.05) & (avg_scores['sentiment_score'] <= 0.5),\n",
    "            (avg_scores['sentiment_score'] >= -0.05) & (avg_scores['sentiment_score'] <= 0.05),\n",
    "            (avg_scores['sentiment_score'] < -0.05) & (avg_scores['sentiment_score'] >= -0.5),\n",
    "            avg_scores['sentiment_score'] < -0.5\n",
    "        ]\n",
    "\n",
    "        # Defining corresponding classifications\n",
    "        classifications = [\n",
    "            \"strongly_positive\",\n",
    "            \"weakly_positive\",\n",
    "            \"neutral\",\n",
    "            \"weakly_negative\",\n",
    "            \"strongly_negative\"\n",
    "        ]\n",
    "\n",
    "        # Assigning classifications based on conditions\n",
    "        avg_scores['weighted_daily_sentiment'] = np.select(conditions, classifications, default=\"neutral\")\n",
    "        \n",
    "        # merging to assign classifications to each row\n",
    "        news_press_release_sentiment = news_press_release_sentiment.merge(avg_scores[[date_column, 'weighted_daily_sentiment']], on=date_column, how='left')\n",
    "\n",
    "        # writing to csv files\n",
    "        news_press_release_sentiment.to_csv(file, index=False)\n",
    "        print(f\"Finished updating {file} with weighted daily sentiment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentiments score for news and press release datasets for each company\n",
    "sentiment_score_calculator(stock_news_datasets)\n",
    "sentiment_score_calculator(news_press_releases_datasets)\n",
    "\n",
    "# Calculating aggregated sentiments(daily) for news, press release and social sentiments datasets for each company\n",
    "aggregated_sentiments(stock_news_datasets)\n",
    "aggregated_sentiments(news_press_releases_datasets)\n",
    "aggregated_sentiments(social_sentiments_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in company data and Creating Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting company names \n",
    "company_names = ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']\n",
    "\n",
    "# Dictionary to store datasets\n",
    "company_data = {}\n",
    "\n",
    "\n",
    "# reading in files\n",
    "for names in company_names:\n",
    "\n",
    "    if names == 'GOOG':\n",
    "\n",
    "        company_data[names] = {\n",
    "            \"stock_news\": pd.read_csv(f\"{names}_news_data.csv\"),\n",
    "            \"press_releases\": pd.read_csv(f\"{names}_press_release_data.csv\")\n",
    "        }\n",
    "\n",
    "    else:\n",
    "\n",
    "        try:\n",
    "            company_data[names] = {\n",
    "                \"stock_news\": pd.read_csv(f\"{names}_news_data.csv\"),\n",
    "                \"press_releases\": pd.read_csv(f\"{names}_press_release_data.csv\"),\n",
    "                \"social_sentiments\": pd.read_csv(f\"{names}_social_sentiment_data.csv\")\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {names}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "dataset_variables = list(company_data.keys())\n",
    "\n",
    "dataset_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates on stock_news data removed. Number of rows remaining: 2430\n",
      "Duplicates on press_release data removed. Number of rows remaining: 573\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 90\n",
      "Finished updating AAPL with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 3297\n",
      "Duplicates on press_release data removed. Number of rows remaining: 709\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 90\n",
      "Finished updating AMZN with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 2129\n",
      "Duplicates on press_release data removed. Number of rows remaining: 165\n",
      "Finished updating GOOG with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 2046\n",
      "Duplicates on press_release data removed. Number of rows remaining: 114\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 90\n",
      "Finished updating META with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 2057\n",
      "Duplicates on press_release data removed. Number of rows remaining: 509\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 90\n",
      "Finished updating MSFT with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 1963\n",
      "Duplicates on press_release data removed. Number of rows remaining: 325\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 91\n",
      "Finished updating NVDA with weighted daily sentiment.\n"
     ]
    }
   ],
   "source": [
    "# Picking weighted_daily_sentiment and rename it to twitter_social_sentiment, press_release_sentiment and stock_news_sentiment for each dataset\n",
    "for company in company_names:\n",
    "\n",
    "    if company == 'GOOG':\n",
    "\n",
    "        company_data[company]['stock_news'].rename(columns={'weighted_daily_sentiment' : 'stock_news_sentiment',\n",
    "                                                        'published_date' : 'date'}, inplace=True)\n",
    "\n",
    "        company_data[company]['press_releases'].rename(columns={'weighted_daily_sentiment' : 'press_release_sentiment',\n",
    "                                                            'release_date' : 'date'}, inplace=True)\n",
    "        \n",
    "        # Selecting date & sentiment columns\n",
    "        company_data[company]['stock_news'] = company_data[company]['stock_news'][['date', 'stock_news_sentiment']]\n",
    "        company_data[company]['press_releases'] = company_data[company]['press_releases'][['date', 'press_release_sentiment']]\n",
    "\n",
    "        # Dropping all duplicates\n",
    "        company_data[company]['stock_news'].drop_duplicates(inplace=True)\n",
    "        company_data[company]['press_releases'].drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "         # Displaying rows left for each dataset for the given company\n",
    "        print('Duplicates on stock_news data removed. Number of rows remaining:', company_data[company]['stock_news'].shape[0])\n",
    "        print('Duplicates on press_release data removed. Number of rows remaining:', company_data[company]['press_releases'].shape[0])\n",
    "\n",
    "        # merging two datasets into one for Google as sentiment data is non-existent\n",
    "        combined_news_data = company_data[company]['stock_news'].merge(company_data[company]['press_releases'], on='date', how='outer')\n",
    "\n",
    "        # Writing processed data to news file for each company\n",
    "        combined_news_data.to_csv(f\"{company}_complete_news_data.csv\", index=False)\n",
    "        print(f\"Finished updating {company} with weighted daily sentiment.\")\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            company_data[company]['stock_news'].rename(columns={'weighted_daily_sentiment' : 'stock_news_sentiment',\n",
    "                                                            'published_date' : 'date'}, inplace=True)\n",
    "\n",
    "            company_data[company]['press_releases'].rename(columns={'weighted_daily_sentiment' : 'press_release_sentiment',\n",
    "                                                                'release_date' : 'date'}, inplace=True)\n",
    "\n",
    "            company_data[company]['social_sentiments'].rename(columns={'weighted_daily_sentiment' : 'twitter_social_sentiment',\n",
    "                                                                    'published_date' : 'date'}, inplace=True)\n",
    "\n",
    "\n",
    "            # Selecting date & sentiment columns\n",
    "            company_data[company]['stock_news'] = company_data[company]['stock_news'][['date', 'stock_news_sentiment']]\n",
    "            company_data[company]['press_releases'] = company_data[company]['press_releases'][['date', 'press_release_sentiment']]\n",
    "            company_data[company]['social_sentiments'] = company_data[company]['social_sentiments'][['date', 'twitter_social_sentiment']]\n",
    "\n",
    "            # Dropping all duplicates\n",
    "            company_data[company]['stock_news'].drop_duplicates(inplace=True)\n",
    "            company_data[company]['press_releases'].drop_duplicates(inplace=True)\n",
    "            company_data[company]['social_sentiments'].drop_duplicates(inplace=True)\n",
    "\n",
    "            # Displaying rows left for each dataset for the given company\n",
    "            print('Duplicates on stock_news data removed. Number of rows remaining:', company_data[company]['stock_news'].shape[0])\n",
    "            print('Duplicates on press_release data removed. Number of rows remaining:', company_data[company]['press_releases'].shape[0])\n",
    "            print('Duplicates on social_sentiments data removed. Number of rows remaining:', company_data[company]['social_sentiments'].shape[0])\n",
    "\n",
    "\n",
    "            # merging three datasets into one for each company\n",
    "            combined_news_data = company_data[company]['stock_news'].merge(company_data[company]['press_releases'], on='date', how='outer').merge(company_data[company]['social_sentiments'], on='date', how='outer')\n",
    "\n",
    "            # Writing processed data to news file for each company\n",
    "            combined_news_data.to_csv(f\"{company}_complete_news_data.csv\", index=False)\n",
    "            print(f\"Finished updating {company} with weighted daily sentiment.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {company}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forex Data Pulling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for the desired currencies\n",
    "forex_currencies = ['EURUSD', 'GBPUSD', 'JPYUSD', 'CNHUSD', 'KRWUSD', 'CHFUSD', 'CADUSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing EURUSD_forex_data.csv with 1344 records.\n",
      "Finished writing GBPUSD_forex_data.csv with 1358 records.\n",
      "Finished writing JPYUSD_forex_data.csv with 1348 records.\n",
      "Finished writing CNHUSD_forex_data.csv with 1342 records.\n",
      "Finished writing KRWUSD_forex_data.csv with 1365 records.\n",
      "Finished writing CHFUSD_forex_data.csv with 1364 records.\n",
      "Finished writing CADUSD_forex_data.csv with 1378 records.\n"
     ]
    }
   ],
   "source": [
    "# Looping through each currency, pull all the data for the currency relative to USD\n",
    "for currency in forex_currencies:\n",
    "\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{currency}?\"\n",
    "\n",
    "    all_forex_data = []\n",
    "    formatted_data = []\n",
    "\n",
    "    params = {\n",
    "        \"symbol\" : currency,\n",
    "        \"apikey\": fmp_api_key \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url=url, params=params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching forex data for {currency: {e}}\")\n",
    "        continue\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "\n",
    "    if not data:\n",
    "        break\n",
    "\n",
    "    all_forex_data.extend(data.get(\"historical\", []))\n",
    "\n",
    "\n",
    "    csv_file = f\"{currency}_forex_data.csv\"\n",
    "\n",
    "    headers = [\"date\", f\"{currency}_open\", f\"{currency}_high\", f\"{currency}_low\", f\"{currency}_close\", f\"{currency}_adjClose\", f\"{currency}_volume\", \n",
    "               \"unadjustedVolume\", \"change\", f\"{currency}_changePercent\", \"vwap\", \"label\", \"changeOverTime\"]\n",
    "\n",
    "\n",
    "    # Appending data to list\n",
    "    if all_forex_data:\n",
    "        for record in all_forex_data:\n",
    "\n",
    "            date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "            formatted_data.append({\n",
    "                \"date\" : date_str,\n",
    "                f\"{currency}_open\" : record.get(\"open\", \"\"),\n",
    "                f\"{currency}_high\" : record.get(\"high\", \"\"),\n",
    "                f\"{currency}_low\" : record.get(\"low\", \"\"),\n",
    "                f\"{currency}_close\" : record.get(\"close\", \"\"),\n",
    "                f\"{currency}_adjClose\" : record.get(\"adjClose\", \"\"),\n",
    "                f\"{currency}_volume\" : record.get(\"volume\", \"\"),\n",
    "                \"unadjustedVolume\" : record.get(\"unadjustedVolume\", \"\"),\n",
    "                \"change\" : record.get(\"change\", \"\"),\n",
    "                f\"{currency}_changePercent\" : record.get(\"changePercent\", \"\"),\n",
    "                \"vwap\" : record.get(\"vwap\", \"\"),\n",
    "                \"label\" : record.get(\"label\", \"\"),\n",
    "                \"changeOverTime\" : record.get(\"changeOverTime\", \"\")\n",
    "            })\n",
    "\n",
    "    # Writing appended data to csv file\n",
    "    if formatted_data:\n",
    "        with open(csv_file, mode=\"w\", newline='', encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "\n",
    "        print(f\"Finished writing {csv_file} with {len(formatted_data)} records.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Failed to obtain forex data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PULLING COMMODITY DATA\n",
    "##### Chosen commodities; Gold, Silver, Lithium, Copper, Palladium. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "commodities = [\n",
    "    {\"commodity\": \"Gold\", \"symbol\": \"XAUUSD\"},\n",
    "    {\"commodity\": \"Silver\", \"symbol\": \"XAGUSD\"},\n",
    "    {\"commodity\": \"Lithium\", \"symbol\": \"LIT\"},\n",
    "    {\"commodity\": \"Copper\", \"symbol\": \"HGUSD\"},\n",
    "    {\"commodity\": \"Palladium\", \"symbol\": \"XPDUSD\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for Gold written to Gold_commodity_data.csv with 11623 records!\n",
      "Data for Silver written to Silver_commodity_data.csv with 10683 records!\n",
      "Data for Lithium written to Lithium_commodity_data.csv with 3674 records!\n",
      "Data for Copper written to Copper_commodity_data.csv with 9344 records!\n",
      "Data for Palladium written to Palladium_commodity_data.csv with 10085 records!\n",
      "All commodity data successfully written to their respective CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Define current date\n",
    "current_date = date.today()\n",
    "\n",
    "# Iterate through commodities\n",
    "for commodity in commodities:\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{commodity['symbol']}?apikey={fmp_api_key}\"\n",
    "\n",
    "\n",
    "    limit = 1000\n",
    "    max_pages = 250   \n",
    "    all_commodity_data = []\n",
    "\n",
    "    params = {\n",
    "                \"symbol\": commodity['symbol'],\n",
    "                'from': '1980-01-01',  \n",
    "                'to': current_date,\n",
    "                'limit': limit\n",
    "            }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {commodity['commodity']} on page {page}: {e}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    if not data:\n",
    "        break\n",
    "\n",
    "    all_commodity_data.extend(data.get(\"historical\", []))\n",
    "\n",
    "\n",
    "    # Create CSV file for commodity data\n",
    "    csv_file = f\"{commodity['commodity']}_commodity_data.csv\"\n",
    "    headers = [\"date\", f\"{commodity['commodity']}_price\", f\"{commodity['commodity']}_volume\", f\"{commodity['commodity']}_open\", f\"{commodity['commodity']}_close\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    if all_commodity_data:\n",
    "        for record in all_commodity_data:\n",
    "            # Formats for date parsing\n",
    "            formats = ['%Y-%m-%d %I:%M:%S %p', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "            if 'date' in record:\n",
    "                for fmt in formats:\n",
    "                    try:\n",
    "                        record['date'] = datetime.strptime(record['date'], fmt).strftime('%d-%m-%Y')\n",
    "                        break\n",
    "                    except ValueError as e:\n",
    "                        continue\n",
    "\n",
    "            formatted_data.append({\n",
    "                \"date\": record.get(\"date\", \"\"),\n",
    "                f\"{commodity['commodity']}_price\": record.get(\"close\", \"\"),  \n",
    "                f\"{commodity['commodity']}_volume\": record.get(\"volume\", \"\"),\n",
    "                f\"{commodity['commodity']}_open\": record.get(\"open\", \"\"),\n",
    "                f\"{commodity['commodity']}_close\": record.get(\"close\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "\n",
    "        print(f\"Data for {commodity['commodity']} written to {csv_file} with {len(formatted_data)} records!\")\n",
    "    else:\n",
    "        print(f\"No data found for {commodity['commodity']}.\")\n",
    "\n",
    "print(\"All commodity data successfully written to their respective CSV files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the Treasury Rates data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched data for 2024-12-02 to 2025-03-02\n",
      "Fetched data for 2024-09-03 to 2024-12-02\n",
      "Fetched data for 2024-06-05 to 2024-09-03\n",
      "Fetched data for 2024-03-07 to 2024-06-05\n",
      "Fetched data for 2023-12-08 to 2024-03-07\n",
      "Fetched data for 2023-09-09 to 2023-12-08\n",
      "Fetched data for 2023-06-11 to 2023-09-09\n",
      "Fetched data for 2023-03-13 to 2023-06-11\n",
      "Fetched data for 2022-12-13 to 2023-03-13\n",
      "Fetched data for 2022-09-14 to 2022-12-13\n",
      "Fetched data for 2022-06-16 to 2022-09-14\n",
      "Fetched data for 2022-03-18 to 2022-06-16\n",
      "Fetched data for 2021-12-18 to 2022-03-18\n",
      "Fetched data for 2021-09-19 to 2021-12-18\n",
      "Fetched data for 2021-06-21 to 2021-09-19\n",
      "Fetched data for 2021-03-23 to 2021-06-21\n",
      "Fetched data for 2020-12-23 to 2021-03-23\n",
      "Fetched data for 2020-09-24 to 2020-12-23\n",
      "Fetched data for 2020-06-26 to 2020-09-24\n",
      "Fetched data for 2020-03-28 to 2020-06-26\n",
      "Fetched data for 2019-12-29 to 2020-03-28\n",
      "Fetched data for 2019-09-30 to 2019-12-29\n",
      "Fetched data for 2019-07-02 to 2019-09-30\n",
      "Fetched data for 2019-04-03 to 2019-07-02\n",
      "Fetched data for 2019-01-03 to 2019-04-03\n",
      "Fetched data for 2018-10-05 to 2019-01-03\n",
      "Fetched data for 2018-07-07 to 2018-10-05\n",
      "Fetched data for 2018-04-08 to 2018-07-07\n",
      "Fetched data for 2018-01-08 to 2018-04-08\n",
      "Fetched data for 2017-10-10 to 2018-01-08\n",
      "Fetched data for 2017-07-12 to 2017-10-10\n",
      "Fetched data for 2017-04-13 to 2017-07-12\n",
      "Fetched data for 2017-01-13 to 2017-04-13\n",
      "Fetched data for 2016-10-15 to 2017-01-13\n",
      "Fetched data for 2016-07-17 to 2016-10-15\n",
      "Fetched data for 2016-04-18 to 2016-07-17\n",
      "Fetched data for 2016-01-19 to 2016-04-18\n",
      "Fetched data for 2015-10-21 to 2016-01-19\n",
      "Fetched data for 2015-07-23 to 2015-10-21\n",
      "Fetched data for 2015-04-24 to 2015-07-23\n",
      "Fetched data for 2015-01-24 to 2015-04-24\n",
      "Fetched data for 2014-10-26 to 2015-01-24\n",
      "Fetched data for 2014-07-28 to 2014-10-26\n",
      "Fetched data for 2014-04-29 to 2014-07-28\n",
      "Fetched data for 2014-01-29 to 2014-04-29\n",
      "Fetched data for 2013-10-31 to 2014-01-29\n",
      "Fetched data for 2013-08-02 to 2013-10-31\n",
      "Fetched data for 2013-05-04 to 2013-08-02\n",
      "Fetched data for 2013-02-03 to 2013-05-04\n",
      "Fetched data for 2012-11-05 to 2013-02-03\n",
      "Fetched data for 2012-08-07 to 2012-11-05\n",
      "Fetched data for 2012-05-09 to 2012-08-07\n",
      "Fetched data for 2012-02-09 to 2012-05-09\n",
      "Fetched data for 2011-11-11 to 2012-02-09\n",
      "Fetched data for 2011-08-13 to 2011-11-11\n",
      "Fetched data for 2011-05-15 to 2011-08-13\n",
      "Fetched data for 2011-02-14 to 2011-05-15\n",
      "Fetched data for 2010-11-16 to 2011-02-14\n",
      "Fetched data for 2010-08-18 to 2010-11-16\n",
      "Fetched data for 2010-05-20 to 2010-08-18\n",
      "Fetched data for 2010-02-19 to 2010-05-20\n",
      "Fetched data for 2009-11-21 to 2010-02-19\n",
      "Fetched data for 2009-08-23 to 2009-11-21\n",
      "Fetched data for 2009-05-25 to 2009-08-23\n",
      "Fetched data for 2009-02-24 to 2009-05-25\n",
      "Fetched data for 2008-11-26 to 2009-02-24\n",
      "Fetched data for 2008-08-28 to 2008-11-26\n",
      "Fetched data for 2008-05-30 to 2008-08-28\n",
      "Fetched data for 2008-03-01 to 2008-05-30\n",
      "Fetched data for 2007-12-02 to 2008-03-01\n",
      "Fetched data for 2007-09-03 to 2007-12-02\n",
      "Fetched data for 2007-06-05 to 2007-09-03\n",
      "Fetched data for 2007-03-07 to 2007-06-05\n",
      "Fetched data for 2006-12-07 to 2007-03-07\n",
      "Fetched data for 2006-09-08 to 2006-12-07\n",
      "Fetched data for 2006-06-10 to 2006-09-08\n",
      "Fetched data for 2006-03-12 to 2006-06-10\n",
      "Fetched data for 2005-12-12 to 2006-03-12\n",
      "Fetched data for 2005-09-13 to 2005-12-12\n",
      "Fetched data for 2005-06-15 to 2005-09-13\n",
      "Fetched data for 2005-03-17 to 2005-06-15\n",
      "Fetched data for 2004-12-17 to 2005-03-17\n",
      "Fetched data for 2004-09-18 to 2004-12-17\n",
      "Fetched data for 2004-06-20 to 2004-09-18\n",
      "Fetched data for 2004-03-22 to 2004-06-20\n",
      "Fetched data for 2003-12-23 to 2004-03-22\n",
      "Fetched data for 2003-09-24 to 2003-12-23\n",
      "Fetched data for 2003-06-26 to 2003-09-24\n",
      "Fetched data for 2003-03-28 to 2003-06-26\n",
      "Fetched data for 2002-12-28 to 2003-03-28\n",
      "Fetched data for 2002-09-29 to 2002-12-28\n",
      "Fetched data for 2002-07-01 to 2002-09-29\n",
      "Fetched data for 2002-04-02 to 2002-07-01\n",
      "Fetched data for 2002-01-02 to 2002-04-02\n",
      "Fetched data for 2001-10-04 to 2002-01-02\n",
      "Fetched data for 2001-07-06 to 2001-10-04\n",
      "Fetched data for 2001-04-07 to 2001-07-06\n",
      "Fetched data for 2001-01-07 to 2001-04-07\n",
      "Fetched data for 2000-10-09 to 2001-01-07\n",
      "Fetched data for 2000-07-11 to 2000-10-09\n",
      "Fetched data for 2000-04-12 to 2000-07-11\n",
      "Fetched data for 2000-01-13 to 2000-04-12\n",
      "Fetched data for 1999-10-15 to 2000-01-13\n",
      "Fetched data for 1999-07-17 to 1999-10-15\n",
      "Fetched data for 1999-04-18 to 1999-07-17\n",
      "Fetched data for 1999-01-18 to 1999-04-18\n",
      "Fetched data for 1998-10-20 to 1999-01-18\n",
      "Fetched data for 1998-07-22 to 1998-10-20\n",
      "Fetched data for 1998-04-23 to 1998-07-22\n",
      "Fetched data for 1998-01-23 to 1998-04-23\n",
      "Fetched data for 1997-10-25 to 1998-01-23\n",
      "Fetched data for 1997-07-27 to 1997-10-25\n",
      "Fetched data for 1997-04-28 to 1997-07-27\n",
      "Fetched data for 1997-01-28 to 1997-04-28\n",
      "Fetched data for 1996-10-30 to 1997-01-28\n",
      "Fetched data for 1996-08-01 to 1996-10-30\n",
      "Fetched data for 1996-05-03 to 1996-08-01\n",
      "Fetched data for 1996-02-03 to 1996-05-03\n",
      "Fetched data for 1995-11-05 to 1996-02-03\n",
      "Fetched data for 1995-08-07 to 1995-11-05\n",
      "Fetched data for 1995-05-09 to 1995-08-07\n",
      "Fetched data for 1995-02-08 to 1995-05-09\n",
      "Fetched data for 1994-11-10 to 1995-02-08\n",
      "Fetched data for 1994-08-12 to 1994-11-10\n",
      "Fetched data for 1994-05-14 to 1994-08-12\n",
      "Fetched data for 1994-02-13 to 1994-05-14\n",
      "Fetched data for 1993-11-15 to 1994-02-13\n",
      "Fetched data for 1993-08-17 to 1993-11-15\n",
      "Fetched data for 1993-05-19 to 1993-08-17\n",
      "Fetched data for 1993-02-18 to 1993-05-19\n",
      "Fetched data for 1992-11-20 to 1993-02-18\n",
      "Fetched data for 1992-08-22 to 1992-11-20\n",
      "Fetched data for 1992-05-24 to 1992-08-22\n",
      "Fetched data for 1992-02-24 to 1992-05-24\n",
      "Fetched data for 1991-11-26 to 1992-02-24\n",
      "Fetched data for 1991-08-28 to 1991-11-26\n",
      "Fetched data for 1991-05-30 to 1991-08-28\n",
      "Fetched data for 1991-03-01 to 1991-05-30\n",
      "Fetched data for 1990-12-01 to 1991-03-01\n",
      "Fetched data for 1990-09-02 to 1990-12-01\n",
      "Fetched data for 1990-06-04 to 1990-09-02\n",
      "Fetched data for 1990-03-06 to 1990-06-04\n",
      "Fetched data for 1989-12-06 to 1990-03-06\n",
      "Treasury rates data written to treasury_rates_data.csv with 8895 records!\n"
     ]
    }
   ],
   "source": [
    "# Define API key and URL\n",
    "# The API key should be loaded before this, as you've mentioned it is already loaded elsewhere\n",
    "base_url = \"https://financialmodelingprep.com/api/v4/treasury\"\n",
    "\n",
    "# Defined date variables\n",
    "end_date = \"1980-01-01\"  \n",
    "current_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Function to subtract months from a date\n",
    "def subtract_months(start_date, months):\n",
    "    new_date = datetime.strptime(start_date, \"%Y-%m-%d\") - timedelta(days=months*30)\n",
    "    return new_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Defined CSV file name\n",
    "csv_file = \"treasury_rates_data.csv\"\n",
    "headers = [\"date\", \"month1\", \"month2\", \"month3\", \"month6\", \"year1\", \"year2\", \"year3\", \"year5\", \"year7\", \"year10\", \"year20\", \"year30\"]\n",
    "formatted_data = []\n",
    "\n",
    "# Initialized the starting date (current date)\n",
    "current_from_date = current_date\n",
    "\n",
    "# Loop to fetch data in 3-month chunks\n",
    "while current_from_date > end_date:\n",
    "    # Calculated the \"from\" and \"to\" dates for the API call (3 months at a time)\n",
    "    current_to_date = current_from_date\n",
    "    current_from_date = subtract_months(current_from_date, 3)\n",
    "    \n",
    "    # Make sure the \"from_date\" does not go earlier than the end date\n",
    "    if current_from_date < end_date:\n",
    "        current_from_date = end_date\n",
    "\n",
    "    # Define parameters for API request\n",
    "    params = {\n",
    "        \"from\": current_from_date,  \n",
    "        \"to\": current_to_date,\n",
    "        \"apikey\": fmp_api_key   \n",
    "    }\n",
    "\n",
    "    # Fetch treasury rates data\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Ensure we catch any HTTP errors\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching treasury rates for {current_from_date} to {current_to_date}: {e}\")\n",
    "        continue  \n",
    "\n",
    "    # Process and format data\n",
    "    if data:\n",
    "        for record in data:\n",
    "            formatted_data.append({\n",
    "                \"date\": record.get(\"date\", \"\"),\n",
    "                \"month1\": record.get(\"month1\", \"\"),\n",
    "                \"month2\": record.get(\"month2\", \"\"),\n",
    "                \"month3\": record.get(\"month3\", \"\"),\n",
    "                \"month6\": record.get(\"month6\", \"\"),\n",
    "                \"year1\": record.get(\"year1\", \"\"),\n",
    "                \"year2\": record.get(\"year2\", \"\"),\n",
    "                \"year3\": record.get(\"year3\", \"\"),\n",
    "                \"year5\": record.get(\"year5\", \"\"),\n",
    "                \"year7\": record.get(\"year7\", \"\"),\n",
    "                \"year10\": record.get(\"year10\", \"\"),\n",
    "                \"year20\": record.get(\"year20\", \"\"),\n",
    "                \"year30\": record.get(\"year30\", \"\"),\n",
    "            })\n",
    "        print(f\"Fetched data for {current_from_date} to {current_to_date}\")\n",
    "\n",
    "# Write data to CSV after all batches\n",
    "if formatted_data:\n",
    "    with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(formatted_data)\n",
    "\n",
    "    print(f\"Treasury rates data written to {csv_file} with {len(formatted_data)} records!\")\n",
    "else:\n",
    "    print(\"No treasury data found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
