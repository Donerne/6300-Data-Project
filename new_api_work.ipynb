{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMP News API Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No API key set for fmp_api_key in .env file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m fmp_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmp_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fmp_api_key:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo API key set for fmp_api_key in .env file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI key loaded successfully from .env file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: No API key set for fmp_api_key in .env file"
     ]
    }
   ],
   "source": [
    "# loading in api file\n",
    "load_dotenv(r\"api.env\")\n",
    "\n",
    "# Obtaining key for API file\n",
    "fmp_api_key = os.getenv(\"fmp_api_key\")\n",
    "if not fmp_api_key:\n",
    "    raise ValueError(\"No API key set for fmp_api_key in .env file\")\n",
    "\n",
    "print(\"API key loaded successfully from .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up dictionaries in lists to facilitate looping through\n",
    "stock_news = [\n",
    "    {\"symbol\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"symbol\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"symbol\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"symbol\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"symbol\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"symbol\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "press_releases = [\n",
    "    {\"company\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"company\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"company\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"company\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"company\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"company\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "historical_social_sentiment = [\n",
    "    {\"symbol\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"symbol\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"symbol\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"symbol\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"symbol\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"symbol\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# setting up lists to loop through for sentiment score and weighted scores analysis\n",
    "stock_news_datasets = [\n",
    "    \"AMZN_news_data.csv\",\n",
    "    \"AAPL_news_data.csv\",\n",
    "    \"GOOG_news_data.csv\",\n",
    "    \"MSFT_news_data.csv\",\n",
    "    \"META_news_data.csv\",\n",
    "    \"NVDA_news_data.csv\"\n",
    "]\n",
    "\n",
    "news_press_releases_datasets = [\n",
    "    \"AMZN_press_release_data.csv\",\n",
    "    \"AAPL_press_release_data.csv\",\n",
    "    \"GOOG_press_release_data.csv\",\n",
    "    \"MSFT_press_release_data.csv\",\n",
    "    \"META_press_release_data.csv\",\n",
    "    \"NVDA_press_release_data.csv\"\n",
    "]\n",
    "\n",
    "social_sentiments_datasets = [\n",
    "    \"AMZN_social_sentiment_data.csv\",\n",
    "    \"AAPL_social_sentiment_data.csv\",\n",
    "    \"GOOG_social_sentiment_data.csv\",\n",
    "    \"MSFT_social_sentiment_data.csv\",\n",
    "    \"META_social_sentiment_data.csv\",\n",
    "    \"NVDA_social_sentiment_data.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock News Data Pull "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for AMZN written to AMZN_news_data.csv with 29337 articles!\n",
      "Data for AAPL written to AAPL_news_data.csv with 26482 articles!\n",
      "Data for GOOG written to GOOG_news_data.csv with 17780 articles!\n",
      "Data for MSFT written to MSFT_news_data.csv with 16737 articles!\n",
      "Data for META written to META_news_data.csv with 18438 articles!\n",
      "Data for NVDA written to NVDA_news_data.csv with 19013 articles!\n",
      "All stock data successfully written to their respective CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Obtaining today's date\n",
    "current_date =date.today()\n",
    "\n",
    "# Obtaining stock news data for each tech company\n",
    "for news in stock_news:\n",
    "\n",
    "    # setting up url path & parameter values\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/stock_news?\"\n",
    "    page = 0\n",
    "    limit = 2000\n",
    "\n",
    "    # creating empty list to capture news data\n",
    "    all_news = []\n",
    "\n",
    "    # looping through pages of data to obtain stock news data\n",
    "    while True:\n",
    "        \n",
    "        # applying parameter values to required params\n",
    "        params = {\n",
    "        \"apikey\" : fmp_api_key,\n",
    "        \"tickers\" : news['symbol'],\n",
    "        \"page\": page,\n",
    "        'from': news['ipo_date'],\n",
    "        'to': current_date,\n",
    "        'limit': limit\n",
    "        }\n",
    "\n",
    "        # requesting data\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {news['symbol']} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        # setting successfully requested data to variable data\n",
    "        data = response.json()\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # appending additional data to list\n",
    "        all_news.extend(data)\n",
    "\n",
    "        # moving to next page\n",
    "        page += 1\n",
    "\n",
    "    csv_file = f\"{news['symbol']}_news_data.csv\"\n",
    "    headers = [\"published_date\", \"Headline\", \"Brief\", \"URL\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    # writing all pulled news data to their respective companies in csv files\n",
    "    if all_news:\n",
    "        for record in all_news:\n",
    "\n",
    "            date_str = record.get(\"publishedDate\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "            formatted_data.append({\n",
    "            \"published_date\": date_str,\n",
    "            \"Headline\": record.get(\"title\", \"\"),\n",
    "            \"Brief\": record.get(\"text\", \"\"),\n",
    "            \"URL\": record.get(\"url\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "        \n",
    "        print(f\"Data for {news['symbol']} written to {csv_file} with {len(formatted_data)} articles!\")\n",
    "    else:\n",
    "        print(f\"No data found for {news['symbol']}.\")\n",
    "\n",
    "print(\"All stock data successfully written to their respective CSV files.\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Press Release Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining stock press release data for each tech company\n",
    "for press_release in press_releases:\n",
    "\n",
    "    # setting up url path & parameter values\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/press-releases/{press_release['company']}?\"\n",
    "\n",
    "    # setting page to zero\n",
    "    page = 0\n",
    "\n",
    "    # creating empty list to capture all press release data\n",
    "    all_press_releases = []\n",
    "\n",
    "    max_pages = 300\n",
    "\n",
    "    # looping through pages of data to obtain stock press release data\n",
    "    while page < max_pages:\n",
    "\n",
    "        # applying parameter values to required params\n",
    "        params = {\n",
    "        \"apikey\" : fmp_api_key,\n",
    "        \"page\": page\n",
    "        }\n",
    "\n",
    "        # requesting data\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {press_release['company']} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # setting successfully requested data to variable data\n",
    "        data = response.json()\n",
    "        \n",
    "        print(f\"Fetching data for {press_release['company']} - Page {page}...\")\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "\n",
    "        # appending additional data to list\n",
    "        all_press_releases.extend(data)\n",
    "\n",
    "        # moving to next page\n",
    "        page += 1\n",
    "\n",
    "    csv_file = f\"{press_release['company']}_press_release_data.csv\"\n",
    "    headers = [\"release_date\", \"Headline\", \"Brief\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    # writing all pulled press release data to their respective companies in csv files\n",
    "    if all_press_releases:\n",
    "        for record in all_press_releases:\n",
    "\n",
    "            date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "            formatted_data.append({\n",
    "            \"release_date\": date_str,\n",
    "            \"Headline\": record.get(\"title\", \"\"),\n",
    "            \"Brief\": record.get(\"text\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "        \n",
    "        print(f\"Data for {press_release['company']} written to {csv_file} with {len(formatted_data)} press releases!\")\n",
    "    else:\n",
    "        print(f\"No data found for {press_release['company']}.\")\n",
    "\n",
    "print(\"All stock data successfully written to their respective CSV files.\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiments Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_date = date.today()\n",
    " \n",
    "# Loop through each stock symbol to fetch sentiment data\n",
    "for social_sentiment in historical_social_sentiment:\n",
    "    symbol = social_sentiment[\"symbol\"]\n",
    "    ipo_date = social_sentiment[\"ipo_date\"]\n",
    " \n",
    "    # Correct API URL\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/historical/social-sentiment?symbol={symbol}\"\n",
    " \n",
    "    page = 0\n",
    "    all_historical_social_sentiment = []\n",
    " \n",
    "    while True:\n",
    "        social_params = {\n",
    "            \"apikey\": fmp_api_key,\n",
    "            \"page\": page\n",
    "        }\n",
    " \n",
    "        try:\n",
    "            response = requests.get(url, params=social_params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    " \n",
    "            # Print API response for debugging\n",
    "            print(f\"\\nðŸ” Response for {symbol}, Page {page}: {len(data)} records\")\n",
    " \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {symbol} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "        time.sleep(1)\n",
    " \n",
    "        # Stop if no data is returned\n",
    "        if not data:\n",
    "            print(f\"No sentiment data found for {symbol}.\")\n",
    "            break\n",
    " \n",
    "        all_historical_social_sentiment.extend(data)\n",
    "        page += 1  # Move to the next page\n",
    " \n",
    "    # CSV file name (each company has its own file)\n",
    "    csv_file = f\"{symbol}_social_sentiment_data.csv\"\n",
    " \n",
    "    # Updated headers to match API response\n",
    "    headers = [\n",
    "        \"published_date\", \"twitter_posts\", \"twitter_likes\", \"twitter_sentiment\",\n",
    "        \"stock_twitter_posts\", \"stock_twitter_likes\", \"sentiment_score\"\n",
    "    ]\n",
    " \n",
    "    formatted_data = []\n",
    " \n",
    "    if all_historical_social_sentiment:\n",
    "        for record in all_historical_social_sentiment:\n",
    "            # Extract only the date part\n",
    "            date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    " \n",
    "            formatted_data.append({\n",
    "                \"published_date\": date_str,\n",
    "                \"twitter_posts\": record.get(\"twitterPosts\", \"\"),\n",
    "                \"twitter_likes\": record.get(\"twitterLikes\", \"\"),\n",
    "                \"twitter_sentiment\": record.get(\"twitterSentiment\", \"\"),\n",
    "                \"stock_twitter_posts\": record.get(\"stocktwitsPosts\", \"\"),\n",
    "                \"stock_twitter_likes\": record.get(\"stocktwitsLikes\", \"\"),  # Fixed duplicate key\n",
    "                \"sentiment_score\": record.get(\"stocktwitsSentiment\", \"\")\n",
    "            })\n",
    " \n",
    "        # Write data to a CSV file for this company\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    " \n",
    "        print(f\"Sentiment data for {symbol} written to {csv_file} with {len(formatted_data)} records!\")\n",
    "    else:\n",
    "        print(f\"No sentiment data found for {symbol}.\")\n",
    " \n",
    "print(\"\\nAll sentiment data successfully written to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text PreProcessing & Function Creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating a function to clean text for VADER sentiment analysis\n",
    "def text_cleaning(text, dataset):\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace(r\"http\\S+|www\\S+\", \"\", regex=True) # removing URLS\n",
    "    text = text.str.replace(r\"[^a-zA-Z0-9$%.,!?'\\s-]\", \"\", regex=True) # keeping relevant characters\n",
    "    text = text.str.replace(r\"\\s+\", \" \", regex=True).str.strip() # removing extra spaces\n",
    "    text = text.str.replace(r\"\\b(\\d+)%\", r\"\\1 percent\", regex=True) # converting percentages\n",
    "    text = text.str.replace(r\"\\b(\\d+)M\\b\", r\"\\1 million\", regex=True)  # Convert M to million\n",
    "    text = text.str.replace(r\"\\b(\\d+)B\\b\", r\"\\1 billion\", regex=True)  # Convert B to billion\n",
    "\n",
    "    # splitting texts manually based on \".!?\"\n",
    "    sentences = text.str.split(r'[.!?]\\s+', regex=True)\n",
    "    \n",
    "    # creating column for the cleaned text and naming it 'cleaned'\n",
    "    dataset['cleaned'] = sentences\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to obtain sentiment scores\n",
    "def sentiment_score_calculator(datasets_lists):\n",
    "\n",
    "    # Initializing VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "    # looping through datasets \n",
    "    for file in datasets_lists:\n",
    "        \n",
    "        # reading in file \n",
    "        news_press_release = pd.read_csv(file)\n",
    "\n",
    "        # combining two text columns into one column to clean text in one go\n",
    "        news_press_release['raw'] = news_press_release.apply(lambda row: row['Headline'] if pd.isna(row['Brief']) \n",
    "                                                        else (row['Brief'] if pd.isna(row['Headline']) \n",
    "                                                            else row['Headline'] + \". \" + row['Brief']), axis=1)\n",
    "\n",
    "        # cleaning text cleaning function with VADER appliation\n",
    "        text_cleaning(news_press_release['raw'], news_press_release)\n",
    "\n",
    "\n",
    "        # creating empty lists\n",
    "        score_rating = []\n",
    "        score = []\n",
    "\n",
    "        # looping through each row in the column 'cleaned'\n",
    "        for row in news_press_release['cleaned']:\n",
    "            \n",
    "            row_score = 0\n",
    "\n",
    "            # looping through each sentence in current row and using vader to score each sentence\n",
    "            for sentence in row:\n",
    "                try:\n",
    "                    print(sentence)\n",
    "                    sentence_score = analyzer.polarity_scores(sentence)[\"compound\"] # vader polarity score analyzer\n",
    "                    print(sentence_score)\n",
    "                    print(\"Done with sentence_score in row, onto next sentence.\")\n",
    "\n",
    "                    # accumulating each sentence's score for current row\n",
    "                    row_score += sentence_score\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error processing sentence: {sentence}\\n{e}\")\n",
    "                    continue\n",
    "\n",
    "            # averaging row scores and appending to score\n",
    "            avg_row_score = row_score / len(row)\n",
    "            score.append(avg_row_score)\n",
    "\n",
    "            print(f\"row_score: {avg_row_score}\")\n",
    "            print(len(row))\n",
    "            print(\"end of row, onto next row.\")\n",
    "\n",
    "            # classifying averaged row scores into various score ratings\n",
    "            if avg_row_score > 0.05 and avg_row_score < 0.5:\n",
    "                score_rating.append(\"weakly_positive\")\n",
    "                print(\"Weakly Positive\")\n",
    "            elif avg_row_score > 0.5:\n",
    "                score_rating.append(\"strongly_positive\")\n",
    "                print(\"Strongly Positive\")\n",
    "            elif avg_row_score > -0.5 and avg_row_score < -0.05:\n",
    "                score_rating.append(\"weakly_negative\")\n",
    "                print(\"Weakly Negative\")\n",
    "            elif avg_row_score < -0.5:\n",
    "                score_rating.append(\"strongly_negative\")\n",
    "                print(\"Strongly Negative\")\n",
    "            else:\n",
    "                score_rating.append(\"neutral\")\n",
    "                print(\"Neutral\")\n",
    "\n",
    "        # creating columns for sentiment and score\n",
    "        news_press_release['sentiment'] = score_rating\n",
    "        news_press_release['sentiment_score'] = score\n",
    "\n",
    "        # writing the updated vader sentiment scores to each file \n",
    "        news_press_release.to_csv(file, index=False)\n",
    "        print(f\"Finished updating {file} with sentiment rating and sentiment score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_sentiments(datasets_lists):\n",
    "\n",
    "    # looping through each dataset and applying aggregated sentiments\n",
    "    for file in datasets_lists:\n",
    "\n",
    "        if not file or not os.path.exists(file):\n",
    "            print(f\"Skipping missing file: {file}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # reading in dataset\n",
    "            news_press_release_sentiment = pd.read_csv(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "        \n",
    "\n",
    "        # Checking if either 'published_date' or 'release_date' exists and group by the first one found\n",
    "        date_column = 'published_date' if 'published_date' in news_press_release_sentiment.columns else 'release_date'\n",
    "\n",
    "        # computing average score per unique publilshed_date\n",
    "        avg_scores = news_press_release_sentiment.groupby(date_column)['sentiment_score'].mean().reset_index()\n",
    "\n",
    "        # classifying weighted daily scores into classes\n",
    "\n",
    "        # Defining conditions for classification\n",
    "        conditions = [\n",
    "            avg_scores['sentiment_score'] > 0.5,\n",
    "            (avg_scores['sentiment_score'] > 0.05) & (avg_scores['sentiment_score'] <= 0.5),\n",
    "            (avg_scores['sentiment_score'] >= -0.05) & (avg_scores['sentiment_score'] <= 0.05),\n",
    "            (avg_scores['sentiment_score'] < -0.05) & (avg_scores['sentiment_score'] >= -0.5),\n",
    "            avg_scores['sentiment_score'] < -0.5\n",
    "        ]\n",
    "\n",
    "        # Defining corresponding classifications\n",
    "        classifications = [\n",
    "            \"strongly_positive\",\n",
    "            \"weakly_positive\",\n",
    "            \"neutral\",\n",
    "            \"weakly_negative\",\n",
    "            \"strongly_negative\"\n",
    "        ]\n",
    "\n",
    "        # Assigning classifications based on conditions\n",
    "        avg_scores['weighted_daily_sentiment'] = np.select(conditions, classifications, default=\"neutral\")\n",
    "        \n",
    "        # merging to assign classifications to each row\n",
    "        news_press_release_sentiment = news_press_release_sentiment.merge(avg_scores[[date_column, 'weighted_daily_sentiment']], on=date_column, how='left')\n",
    "\n",
    "        # writing to csv files\n",
    "        news_press_release_sentiment.to_csv(file, index=False)\n",
    "        print(f\"Finished updating {file} with weighted daily sentiment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentiments score for news and press release datasets for each company\n",
    "sentiment_score_calculator(stock_news_datasets)\n",
    "sentiment_score_calculator(news_press_releases_datasets)\n",
    "\n",
    "# Calculating aggregated sentiments(daily) for news, press release and social sentiments datasets for each company\n",
    "aggregated_sentiments(stock_news_datasets)\n",
    "aggregated_sentiments(news_press_releases_datasets)\n",
    "aggregated_sentiments(social_sentiments_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in company data and Creating Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting company names \n",
    "company_names = ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']\n",
    "\n",
    "# Dictionary to store datasets\n",
    "company_data = {}\n",
    "\n",
    "\n",
    "# reading in files\n",
    "for names in company_names:\n",
    "\n",
    "    if names == 'GOOG':\n",
    "\n",
    "        company_data[names] = {\n",
    "            \"stock_news\": pd.read_csv(f\"{names}_news_data.csv\"),\n",
    "            \"press_releases\": pd.read_csv(f\"{names}_press_release_data.csv\")\n",
    "        }\n",
    "\n",
    "    else:\n",
    "\n",
    "        try:\n",
    "            company_data[names] = {\n",
    "                \"stock_news\": pd.read_csv(f\"{names}_news_data.csv\"),\n",
    "                \"press_releases\": pd.read_csv(f\"{names}_press_release_data.csv\"),\n",
    "                \"social_sentiments\": pd.read_csv(f\"{names}_social_sentiment_data.csv\")\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {names}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "dataset_variables = list(company_data.keys())\n",
    "\n",
    "dataset_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates on stock_news data removed. Number of rows remaining: 2423\n",
      "Duplicates on press_release data removed. Number of rows remaining: 570\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 93\n",
      "Finished updating AAPL with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 3290\n",
      "Duplicates on press_release data removed. Number of rows remaining: 708\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 93\n",
      "Finished updating AMZN with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 2127\n",
      "Duplicates on press_release data removed. Number of rows remaining: 167\n",
      "Finished updating GOOG with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 2039\n",
      "Duplicates on press_release data removed. Number of rows remaining: 111\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 93\n",
      "Finished updating META with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 2050\n",
      "Duplicates on press_release data removed. Number of rows remaining: 504\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 93\n",
      "Finished updating MSFT with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 1956\n",
      "Duplicates on press_release data removed. Number of rows remaining: 321\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 93\n",
      "Finished updating NVDA with weighted daily sentiment.\n"
     ]
    }
   ],
   "source": [
    "# Picking weighted_daily_sentiment and rename it to twitter_social_sentiment, press_release_sentiment and stock_news_sentiment for each dataset\n",
    "for company in company_names:\n",
    "\n",
    "    if company == 'GOOG':\n",
    "\n",
    "        company_data[company]['stock_news'].rename(columns={'weighted_daily_sentiment' : 'stock_news_sentiment',\n",
    "                                                        'published_date' : 'date'}, inplace=True)\n",
    "\n",
    "        company_data[company]['press_releases'].rename(columns={'weighted_daily_sentiment' : 'press_release_sentiment',\n",
    "                                                            'release_date' : 'date'}, inplace=True)\n",
    "        \n",
    "        # Selecting date & sentiment columns\n",
    "        company_data[company]['stock_news'] = company_data[company]['stock_news'][['date', 'stock_news_sentiment']]\n",
    "        company_data[company]['press_releases'] = company_data[company]['press_releases'][['date', 'press_release_sentiment']]\n",
    "\n",
    "        # Dropping all duplicates\n",
    "        company_data[company]['stock_news'].drop_duplicates(inplace=True)\n",
    "        company_data[company]['press_releases'].drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "         # Displaying rows left for each dataset for the given company\n",
    "        print('Duplicates on stock_news data removed. Number of rows remaining:', company_data[company]['stock_news'].shape[0])\n",
    "        print('Duplicates on press_release data removed. Number of rows remaining:', company_data[company]['press_releases'].shape[0])\n",
    "\n",
    "        # merging two datasets into one for Google as sentiment data is non-existent\n",
    "        combined_news_data = company_data[company]['stock_news'].merge(company_data[company]['press_releases'], on='date', how='outer')\n",
    "\n",
    "        # Writing processed data to news file for each company\n",
    "        combined_news_data.to_csv(f\"{company}_complete_news_data.csv\", index=False)\n",
    "        print(f\"Finished updating {company} with weighted daily sentiment.\")\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            company_data[company]['stock_news'].rename(columns={'weighted_daily_sentiment' : 'stock_news_sentiment',\n",
    "                                                            'published_date' : 'date'}, inplace=True)\n",
    "\n",
    "            company_data[company]['press_releases'].rename(columns={'weighted_daily_sentiment' : 'press_release_sentiment',\n",
    "                                                                'release_date' : 'date'}, inplace=True)\n",
    "\n",
    "            company_data[company]['social_sentiments'].rename(columns={'weighted_daily_sentiment' : 'twitter_social_sentiment',\n",
    "                                                                    'published_date' : 'date'}, inplace=True)\n",
    "\n",
    "\n",
    "            # Selecting date & sentiment columns\n",
    "            company_data[company]['stock_news'] = company_data[company]['stock_news'][['date', 'stock_news_sentiment']]\n",
    "            company_data[company]['press_releases'] = company_data[company]['press_releases'][['date', 'press_release_sentiment']]\n",
    "            company_data[company]['social_sentiments'] = company_data[company]['social_sentiments'][['date', 'twitter_social_sentiment']]\n",
    "\n",
    "            # Dropping all duplicates\n",
    "            company_data[company]['stock_news'].drop_duplicates(inplace=True)\n",
    "            company_data[company]['press_releases'].drop_duplicates(inplace=True)\n",
    "            company_data[company]['social_sentiments'].drop_duplicates(inplace=True)\n",
    "\n",
    "            # Displaying rows left for each dataset for the given company\n",
    "            print('Duplicates on stock_news data removed. Number of rows remaining:', company_data[company]['stock_news'].shape[0])\n",
    "            print('Duplicates on press_release data removed. Number of rows remaining:', company_data[company]['press_releases'].shape[0])\n",
    "            print('Duplicates on social_sentiments data removed. Number of rows remaining:', company_data[company]['social_sentiments'].shape[0])\n",
    "\n",
    "\n",
    "            # merging three datasets into one for each company\n",
    "            combined_news_data = company_data[company]['stock_news'].merge(company_data[company]['press_releases'], on='date', how='outer').merge(company_data[company]['social_sentiments'], on='date', how='outer')\n",
    "\n",
    "            # Writing processed data to news file for each company\n",
    "            combined_news_data.to_csv(f\"{company}_complete_news_data.csv\", index=False)\n",
    "            print(f\"Finished updating {company} with weighted daily sentiment.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {company}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PULLING COMMODITY DATA\n",
    "##### Chosen commodities; Gold, Silver, Lithium, Copper, Palladium. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commodities = [\n",
    "    {\"commodity\": \"Gold\", \"symbol\": \"XAUUSD\"},\n",
    "    {\"commodity\": \"Silver\", \"symbol\": \"XAGUSD\"},\n",
    "    {\"commodity\": \"Lithium\", \"symbol\": \"LIT\"},\n",
    "    {\"commodity\": \"Copper\", \"symbol\": \"HGUSD\"},\n",
    "    {\"commodity\": \"Palladium\", \"symbol\": \"XPDUSD\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define current date\n",
    "current_date = date.today()\n",
    "\n",
    "# Iterate through commodities\n",
    "for commodity in commodities:\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{commodity['symbol']}?apikey={fmp_api_key}\"\n",
    "\n",
    "    page = 0\n",
    "    limit = 1000\n",
    "    max_pages = 250   \n",
    "    all_commodity_data = []\n",
    "\n",
    "    params = {\n",
    "                \"symbol\": commodity['symbol'],\n",
    "               # \"page\": page,\n",
    "                'from': '1980-01-01',  \n",
    "                'to': current_date,\n",
    "                'limit': limit\n",
    "            }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {commodity['commodity']} on page {page}: {e}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    if not data:\n",
    "        break\n",
    "\n",
    "    all_commodity_data.extend(data.get(\"historical\", []))\n",
    "\n",
    "\n",
    "    # Create CSV file for commodity data\n",
    "    csv_file = f\"{commodity['commodity']}_commodity_data.csv\"\n",
    "    headers = [\"date\", \"price\", \"volume\", \"open\", \"close\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    if all_commodity_data:\n",
    "        for record in all_commodity_data:\n",
    "            # Formats for date parsing\n",
    "            formats = ['%Y-%m-%d %I:%M:%S %p', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "            if 'date' in record:\n",
    "                for fmt in formats:\n",
    "                    try:\n",
    "                        record['date'] = datetime.strptime(record['date'], fmt).strftime('%d-%m-%Y')\n",
    "                        break\n",
    "                    except ValueError as e:\n",
    "                        continue\n",
    "\n",
    "            formatted_data.append({\n",
    "                \"date\": record.get(\"date\", \"\"),\n",
    "                \"price\": record.get(\"close\", \"\"),  \n",
    "                \"volume\": record.get(\"volume\", \"\"),\n",
    "                \"open\": record.get(\"open\", \"\"),\n",
    "                \"close\": record.get(\"close\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "\n",
    "        print(f\"Data for {commodity['commodity']} written to {csv_file} with {len(formatted_data)} records!\")\n",
    "    else:\n",
    "        print(f\"No data found for {commodity['commodity']}.\")\n",
    "\n",
    "print(\"All commodity data successfully written to their respective CSV files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the Treasury Rates data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API key and URL\n",
    "# The API key should be loaded before this, as you've mentioned it is already loaded elsewhere\n",
    "base_url = \"https://financialmodelingprep.com/api/v4/treasury\"\n",
    "\n",
    "# Defined date variables\n",
    "end_date = \"1980-01-01\"  \n",
    "current_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Function to subtract months from a date\n",
    "def subtract_months(start_date, months):\n",
    "    new_date = datetime.strptime(start_date, \"%Y-%m-%d\") - timedelta(days=months*30)\n",
    "    return new_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Defined CSV file name\n",
    "csv_file = \"treasury_rates_data.csv\"\n",
    "headers = [\"date\", \"month1\", \"month2\", \"month3\", \"month6\", \"year1\", \"year2\", \"year3\", \"year5\", \"year7\", \"year10\", \"year20\", \"year30\"]\n",
    "formatted_data = []\n",
    "\n",
    "# Initialized the starting date (current date)\n",
    "current_from_date = current_date\n",
    "\n",
    "# Loop to fetch data in 3-month chunks\n",
    "while current_from_date > end_date:\n",
    "    # Calculated the \"from\" and \"to\" dates for the API call (3 months at a time)\n",
    "    current_to_date = current_from_date\n",
    "    current_from_date = subtract_months(current_from_date, 3)\n",
    "    \n",
    "    # Make sure the \"from_date\" does not go earlier than the end date\n",
    "    if current_from_date < end_date:\n",
    "        current_from_date = end_date\n",
    "\n",
    "    # Define parameters for API request\n",
    "    params = {\n",
    "        \"from\": current_from_date,  \n",
    "        \"to\": current_to_date,\n",
    "        \"apikey\": fmp_api_key   \n",
    "    }\n",
    "\n",
    "    # Fetch treasury rates data\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Ensure we catch any HTTP errors\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching treasury rates for {current_from_date} to {current_to_date}: {e}\")\n",
    "        continue  \n",
    "\n",
    "    # Process and format data\n",
    "    if data:\n",
    "        for record in data:\n",
    "            formatted_data.append({\n",
    "                \"date\": record.get(\"date\", \"\"),\n",
    "                \"month1\": record.get(\"month1\", \"\"),\n",
    "                \"month2\": record.get(\"month2\", \"\"),\n",
    "                \"month3\": record.get(\"month3\", \"\"),\n",
    "                \"month6\": record.get(\"month6\", \"\"),\n",
    "                \"year1\": record.get(\"year1\", \"\"),\n",
    "                \"year2\": record.get(\"year2\", \"\"),\n",
    "                \"year3\": record.get(\"year3\", \"\"),\n",
    "                \"year5\": record.get(\"year5\", \"\"),\n",
    "                \"year7\": record.get(\"year7\", \"\"),\n",
    "                \"year10\": record.get(\"year10\", \"\"),\n",
    "                \"year20\": record.get(\"year20\", \"\"),\n",
    "                \"year30\": record.get(\"year30\", \"\"),\n",
    "            })\n",
    "        print(f\"Fetched data for {current_from_date} to {current_to_date}\")\n",
    "\n",
    "# Write data to CSV after all batches\n",
    "if formatted_data:\n",
    "    with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(formatted_data)\n",
    "\n",
    "    print(f\"Treasury rates data written to {csv_file} with {len(formatted_data)} records!\")\n",
    "else:\n",
    "    print(\"No treasury data found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
