{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMP News API Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully from .env file.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(r\"api.env\")\n",
    "\n",
    "fmp_api_key = os.getenv(\"fmp_api_key\")\n",
    "if not fmp_api_key:\n",
    "    raise ValueError(\"No API key set for fmp_api_key in .env file\")\n",
    "\n",
    "print(\"API key loaded successfully from .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_news = [\n",
    "    {\"symbol\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"symbol\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"symbol\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"symbol\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"symbol\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"symbol\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "press_releases = [\n",
    "    {\"company\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"company\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"company\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"company\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"company\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"company\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "historical_social_sentiment = [\n",
    "    {\"symbol\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"symbol\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"symbol\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"symbol\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"symbol\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"symbol\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for AMZN written to AMZN_news_data.csv with 29074 articles!\n",
      "Data for AAPL written to AAPL_news_data.csv with 26284 articles!\n",
      "Data for GOOG written to GOOG_news_data.csv with 17631 articles!\n",
      "Data for MSFT written to MSFT_news_data.csv with 16622 articles!\n",
      "Data for META written to META_news_data.csv with 18255 articles!\n",
      "Data for NVDA written to NVDA_news_data.csv with 18739 articles!\n",
      "All stock data successfully written to their respective CSV files.\n"
     ]
    }
   ],
   "source": [
    "current_date =date.today()\n",
    "\n",
    "for news in stock_news:\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/stock_news?\"\n",
    "\n",
    "    page = 0\n",
    "    limit = 1000\n",
    "    all_news = []\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "        \"apikey\" : fmp_api_key,\n",
    "        \"tickers\" : news['symbol'],\n",
    "        \"page\": page,\n",
    "        'from': news['ipo_date'],\n",
    "        'to': current_date,\n",
    "        'limit': limit\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {news['symbol']} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "\n",
    "        all_news.extend(data)\n",
    "\n",
    "        # if len(data) < limit:\n",
    "        #     break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    csv_file = f\"{news['symbol']}_news_data.csv\"\n",
    "    headers = [\"published_date\", \"Headline\", \"Brief\", \"URL\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    if all_news:\n",
    "        for record in all_news:\n",
    "\n",
    "            formats = ['%Y-%m-%d %I:%M:%S %p', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "            if 'publishedDate' in record:\n",
    "                for fmt in formats:\n",
    "                    try:\n",
    "                        record['publishedDate'] = datetime.strptime(record['publishedDate'], fmt).strftime('%d-%m-%Y')\n",
    "                        break\n",
    "                    except ValueError as e:\n",
    "                        continue\n",
    "                        # print(f\"Error processing date {record['date']}: {e}\")\n",
    "\n",
    "            formatted_data.append({\n",
    "            \"published_date\": record.get(\"publishedDate\", \"\"),\n",
    "            \"Headline\": record.get(\"title\", \"\"),\n",
    "            \"Brief\": record.get(\"text\", \"\"),\n",
    "            \"URL\": record.get(\"url\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "        \n",
    "        print(f\"Data for {news['symbol']} written to {csv_file} with {len(formatted_data)} articles!\")\n",
    "    else:\n",
    "        print(f\"No data found for {news['symbol']}.\")\n",
    "\n",
    "print(\"All stock data successfully written to their respective CSV files.\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for AMZN - Page 0...\n",
      "Fetching data for AMZN - Page 1...\n",
      "Fetching data for AMZN - Page 2...\n",
      "Fetching data for AMZN - Page 3...\n",
      "Fetching data for AMZN - Page 4...\n",
      "Fetching data for AMZN - Page 5...\n",
      "Fetching data for AMZN - Page 6...\n",
      "Fetching data for AMZN - Page 7...\n",
      "Fetching data for AMZN - Page 8...\n",
      "Fetching data for AMZN - Page 9...\n",
      "Fetching data for AMZN - Page 10...\n",
      "Fetching data for AMZN - Page 11...\n",
      "Fetching data for AMZN - Page 12...\n",
      "Fetching data for AMZN - Page 13...\n",
      "Fetching data for AMZN - Page 14...\n",
      "Fetching data for AMZN - Page 15...\n",
      "Fetching data for AMZN - Page 16...\n",
      "Fetching data for AMZN - Page 17...\n",
      "Fetching data for AMZN - Page 18...\n",
      "Fetching data for AMZN - Page 19...\n",
      "Fetching data for AMZN - Page 20...\n",
      "Fetching data for AMZN - Page 21...\n",
      "Fetching data for AMZN - Page 22...\n",
      "Fetching data for AMZN - Page 23...\n",
      "Fetching data for AMZN - Page 24...\n",
      "Fetching data for AMZN - Page 25...\n",
      "Fetching data for AMZN - Page 26...\n",
      "Fetching data for AMZN - Page 27...\n",
      "Fetching data for AMZN - Page 28...\n",
      "Fetching data for AMZN - Page 29...\n",
      "Fetching data for AMZN - Page 30...\n",
      "Fetching data for AMZN - Page 31...\n",
      "Fetching data for AMZN - Page 32...\n",
      "Fetching data for AMZN - Page 33...\n",
      "Fetching data for AMZN - Page 34...\n",
      "Fetching data for AMZN - Page 35...\n",
      "Fetching data for AMZN - Page 36...\n",
      "Fetching data for AMZN - Page 37...\n",
      "Fetching data for AMZN - Page 38...\n",
      "Fetching data for AMZN - Page 39...\n",
      "Fetching data for AMZN - Page 40...\n",
      "Fetching data for AMZN - Page 41...\n",
      "Fetching data for AMZN - Page 42...\n",
      "Fetching data for AMZN - Page 43...\n",
      "Fetching data for AMZN - Page 44...\n",
      "Fetching data for AMZN - Page 45...\n",
      "Fetching data for AMZN - Page 46...\n",
      "Fetching data for AMZN - Page 47...\n",
      "Fetching data for AMZN - Page 48...\n",
      "Fetching data for AMZN - Page 49...\n",
      "Fetching data for AMZN - Page 50...\n",
      "Fetching data for AMZN - Page 51...\n",
      "Fetching data for AMZN - Page 52...\n",
      "Fetching data for AMZN - Page 53...\n",
      "Fetching data for AMZN - Page 54...\n",
      "Fetching data for AMZN - Page 55...\n",
      "Fetching data for AMZN - Page 56...\n",
      "Fetching data for AMZN - Page 57...\n",
      "Fetching data for AMZN - Page 58...\n",
      "Fetching data for AMZN - Page 59...\n",
      "Fetching data for AMZN - Page 60...\n",
      "Fetching data for AMZN - Page 61...\n",
      "Fetching data for AMZN - Page 62...\n",
      "Fetching data for AMZN - Page 63...\n",
      "Fetching data for AMZN - Page 64...\n",
      "Fetching data for AMZN - Page 65...\n",
      "Fetching data for AMZN - Page 66...\n",
      "Fetching data for AMZN - Page 67...\n",
      "Fetching data for AMZN - Page 68...\n",
      "Fetching data for AMZN - Page 69...\n",
      "Fetching data for AMZN - Page 70...\n",
      "Fetching data for AMZN - Page 71...\n",
      "Fetching data for AMZN - Page 72...\n",
      "Fetching data for AMZN - Page 73...\n",
      "Fetching data for AMZN - Page 74...\n",
      "Fetching data for AMZN - Page 75...\n",
      "Fetching data for AMZN - Page 76...\n",
      "Fetching data for AMZN - Page 77...\n",
      "Fetching data for AMZN - Page 78...\n",
      "Fetching data for AMZN - Page 79...\n",
      "Fetching data for AMZN - Page 80...\n",
      "Fetching data for AMZN - Page 81...\n",
      "Fetching data for AMZN - Page 82...\n",
      "Fetching data for AMZN - Page 83...\n",
      "Fetching data for AMZN - Page 84...\n",
      "Fetching data for AMZN - Page 85...\n",
      "Fetching data for AMZN - Page 86...\n",
      "Fetching data for AMZN - Page 87...\n",
      "Fetching data for AMZN - Page 88...\n",
      "Fetching data for AMZN - Page 89...\n",
      "Fetching data for AMZN - Page 90...\n",
      "Fetching data for AMZN - Page 91...\n",
      "Fetching data for AMZN - Page 92...\n",
      "Fetching data for AMZN - Page 93...\n",
      "Fetching data for AMZN - Page 94...\n",
      "Fetching data for AMZN - Page 95...\n",
      "Fetching data for AMZN - Page 96...\n",
      "Fetching data for AMZN - Page 97...\n",
      "Fetching data for AMZN - Page 98...\n",
      "Fetching data for AMZN - Page 99...\n",
      "Fetching data for AMZN - Page 100...\n",
      "Fetching data for AMZN - Page 101...\n",
      "Fetching data for AMZN - Page 102...\n",
      "Fetching data for AMZN - Page 103...\n",
      "Fetching data for AMZN - Page 104...\n",
      "Fetching data for AMZN - Page 105...\n",
      "Fetching data for AMZN - Page 106...\n",
      "Fetching data for AMZN - Page 107...\n",
      "Fetching data for AMZN - Page 108...\n",
      "Fetching data for AMZN - Page 109...\n",
      "Fetching data for AMZN - Page 110...\n",
      "Fetching data for AMZN - Page 111...\n",
      "Fetching data for AMZN - Page 112...\n",
      "Fetching data for AMZN - Page 113...\n",
      "Fetching data for AMZN - Page 114...\n",
      "Fetching data for AMZN - Page 115...\n",
      "Fetching data for AMZN - Page 116...\n",
      "Fetching data for AMZN - Page 117...\n",
      "Fetching data for AMZN - Page 118...\n",
      "Fetching data for AMZN - Page 119...\n",
      "Fetching data for AMZN - Page 120...\n",
      "Fetching data for AMZN - Page 121...\n",
      "Fetching data for AMZN - Page 122...\n",
      "Fetching data for AMZN - Page 123...\n",
      "Fetching data for AMZN - Page 124...\n",
      "Fetching data for AMZN - Page 125...\n",
      "Fetching data for AMZN - Page 126...\n",
      "Fetching data for AMZN - Page 127...\n",
      "Fetching data for AMZN - Page 128...\n",
      "Fetching data for AMZN - Page 129...\n",
      "Fetching data for AMZN - Page 130...\n",
      "Fetching data for AMZN - Page 131...\n",
      "Fetching data for AMZN - Page 132...\n",
      "Fetching data for AMZN - Page 133...\n",
      "Fetching data for AMZN - Page 134...\n",
      "Fetching data for AMZN - Page 135...\n",
      "Fetching data for AMZN - Page 136...\n",
      "Fetching data for AMZN - Page 137...\n",
      "Fetching data for AMZN - Page 138...\n",
      "Fetching data for AMZN - Page 139...\n",
      "Fetching data for AMZN - Page 140...\n",
      "Fetching data for AMZN - Page 141...\n",
      "Fetching data for AMZN - Page 142...\n",
      "Fetching data for AMZN - Page 143...\n",
      "Fetching data for AMZN - Page 144...\n",
      "Fetching data for AMZN - Page 145...\n",
      "Fetching data for AMZN - Page 146...\n",
      "Fetching data for AMZN - Page 147...\n",
      "Fetching data for AMZN - Page 148...\n",
      "Fetching data for AMZN - Page 149...\n",
      "Fetching data for AMZN - Page 150...\n",
      "Fetching data for AMZN - Page 151...\n",
      "Fetching data for AMZN - Page 152...\n",
      "Fetching data for AMZN - Page 153...\n",
      "Fetching data for AMZN - Page 154...\n",
      "Fetching data for AMZN - Page 155...\n",
      "Fetching data for AMZN - Page 156...\n",
      "Fetching data for AMZN - Page 157...\n",
      "Fetching data for AMZN - Page 158...\n",
      "Fetching data for AMZN - Page 159...\n",
      "Fetching data for AMZN - Page 160...\n",
      "Fetching data for AMZN - Page 161...\n",
      "Fetching data for AMZN - Page 162...\n",
      "Fetching data for AMZN - Page 163...\n",
      "Fetching data for AMZN - Page 164...\n",
      "Fetching data for AMZN - Page 165...\n",
      "Fetching data for AMZN - Page 166...\n",
      "Fetching data for AMZN - Page 167...\n",
      "Fetching data for AMZN - Page 168...\n",
      "Fetching data for AMZN - Page 169...\n",
      "Fetching data for AMZN - Page 170...\n",
      "Fetching data for AMZN - Page 171...\n",
      "Fetching data for AMZN - Page 172...\n",
      "Fetching data for AMZN - Page 173...\n",
      "Fetching data for AMZN - Page 174...\n",
      "Fetching data for AMZN - Page 175...\n",
      "Fetching data for AMZN - Page 176...\n",
      "Fetching data for AMZN - Page 177...\n",
      "Fetching data for AMZN - Page 178...\n",
      "Fetching data for AMZN - Page 179...\n",
      "Fetching data for AMZN - Page 180...\n",
      "Fetching data for AMZN - Page 181...\n",
      "Fetching data for AMZN - Page 182...\n",
      "Fetching data for AMZN - Page 183...\n",
      "Fetching data for AMZN - Page 184...\n",
      "Fetching data for AMZN - Page 185...\n",
      "Fetching data for AMZN - Page 186...\n",
      "Fetching data for AMZN - Page 187...\n",
      "Fetching data for AMZN - Page 188...\n",
      "Fetching data for AMZN - Page 189...\n",
      "Fetching data for AMZN - Page 190...\n",
      "Fetching data for AMZN - Page 191...\n",
      "Fetching data for AMZN - Page 192...\n",
      "Fetching data for AMZN - Page 193...\n",
      "Fetching data for AMZN - Page 194...\n",
      "Fetching data for AMZN - Page 195...\n",
      "Fetching data for AMZN - Page 196...\n",
      "Fetching data for AMZN - Page 197...\n",
      "Fetching data for AMZN - Page 198...\n",
      "Fetching data for AMZN - Page 199...\n",
      "Data for AMZN written to AMZN_press_release_data.csv with 19608 press releases!\n",
      "Fetching data for AAPL - Page 0...\n",
      "Fetching data for AAPL - Page 1...\n",
      "Fetching data for AAPL - Page 2...\n",
      "Fetching data for AAPL - Page 3...\n",
      "Fetching data for AAPL - Page 4...\n",
      "Fetching data for AAPL - Page 5...\n",
      "Fetching data for AAPL - Page 6...\n",
      "Fetching data for AAPL - Page 7...\n",
      "Fetching data for AAPL - Page 8...\n",
      "Fetching data for AAPL - Page 9...\n",
      "Fetching data for AAPL - Page 10...\n",
      "Fetching data for AAPL - Page 11...\n",
      "Data for AAPL written to AAPL_press_release_data.csv with 879 press releases!\n",
      "Fetching data for GOOG - Page 0...\n",
      "Fetching data for GOOG - Page 1...\n",
      "Fetching data for GOOG - Page 2...\n",
      "Fetching data for GOOG - Page 3...\n",
      "Data for GOOG written to GOOG_press_release_data.csv with 192 press releases!\n",
      "Fetching data for MSFT - Page 0...\n",
      "Fetching data for MSFT - Page 1...\n",
      "Fetching data for MSFT - Page 2...\n",
      "Fetching data for MSFT - Page 3...\n",
      "Fetching data for MSFT - Page 4...\n",
      "Fetching data for MSFT - Page 5...\n",
      "Fetching data for MSFT - Page 6...\n",
      "Fetching data for MSFT - Page 7...\n",
      "Fetching data for MSFT - Page 8...\n",
      "Fetching data for MSFT - Page 9...\n",
      "Data for MSFT written to MSFT_press_release_data.csv with 822 press releases!\n",
      "Fetching data for META - Page 0...\n",
      "Fetching data for META - Page 1...\n",
      "Fetching data for META - Page 2...\n",
      "Data for META written to META_press_release_data.csv with 131 press releases!\n",
      "Fetching data for NVDA - Page 0...\n",
      "Fetching data for NVDA - Page 1...\n",
      "Fetching data for NVDA - Page 2...\n",
      "Fetching data for NVDA - Page 3...\n",
      "Fetching data for NVDA - Page 4...\n",
      "Fetching data for NVDA - Page 5...\n",
      "Fetching data for NVDA - Page 6...\n",
      "Data for NVDA written to NVDA_press_release_data.csv with 596 press releases!\n",
      "All stock data successfully written to their respective CSV files.\n"
     ]
    }
   ],
   "source": [
    "for press_release in press_releases:\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/press-releases/{press_release['company']}?\"\n",
    "\n",
    "    page = 0\n",
    "    all_press_releases = []\n",
    "\n",
    "    max_pages = 200\n",
    "\n",
    "    while page < max_pages:\n",
    "        params = {\n",
    "        \"apikey\" : fmp_api_key,\n",
    "        \"page\": page\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {press_release['company']} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        data = response.json()\n",
    "        print(f\"Fetching data for {press_release['company']} - Page {page}...\")\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "\n",
    "        all_press_releases.extend(data)\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    csv_file = f\"{press_release['company']}_press_release_data.csv\"\n",
    "    headers = [\"release_date\", \"Headline\", \"Brief\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    if all_press_releases:\n",
    "        for record in all_press_releases:\n",
    "\n",
    "            formats = ['%Y-%m-%d %I:%M:%S %p', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "            if 'date' in record:\n",
    "                for fmt in formats:\n",
    "                    try:\n",
    "                        record['date'] = datetime.strptime(record['date'], fmt).strftime('%d-%m-%Y')\n",
    "                        break\n",
    "                    except ValueError as e:\n",
    "                        continue\n",
    "\n",
    "            formatted_data.append({\n",
    "            \"release_date\": record.get(\"date\", \"\"),\n",
    "            \"Headline\": record.get(\"title\", \"\"),\n",
    "            \"Brief\": record.get(\"text\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "        \n",
    "        print(f\"Data for {press_release['company']} written to {csv_file} with {len(formatted_data)} press releases!\")\n",
    "    else:\n",
    "        print(f\"No data found for {press_release['company']}.\")\n",
    "\n",
    "print(\"All stock data successfully written to their respective CSV files.\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing CSV file\n",
    "csv_filename = \"news_data.csv\"\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to classify sentiment\n",
    "def get_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)[\"compound\"]\n",
    "    if score > 0.05:\n",
    "        return \"Positive\"\n",
    "    elif score < -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply sentiment analysis to each row\n",
    "df[\"Sentiment\"] = df.apply(lambda row: get_sentiment(f\"{row['Headline']} {row['Brief']}\"), axis=1)\n",
    "\n",
    "# Save back to the same CSV file (overwrite with sentiment added)\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Sentiment appended to {csv_filename} successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
