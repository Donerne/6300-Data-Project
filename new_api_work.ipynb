{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMP News API Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully from .env file.\n"
     ]
    }
   ],
   "source": [
    "# loading in api file\n",
    "load_dotenv(r\"api.env\")\n",
    "\n",
    "# Obtaining key for API file\n",
    "fmp_api_key = os.getenv(\"fmp_api_key\")\n",
    "if not fmp_api_key:\n",
    "    raise ValueError(\"No API key set for fmp_api_key in .env file\")\n",
    "\n",
    "print(\"API key loaded successfully from .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up dictionaries in lists to facilitate looping through\n",
    "stock_news = [\n",
    "    {\"symbol\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"symbol\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"symbol\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"symbol\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"symbol\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"symbol\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "press_releases = [\n",
    "    {\"company\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"company\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"company\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"company\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"company\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"company\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "historical_social_sentiment = [\n",
    "    {\"symbol\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"symbol\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"symbol\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"symbol\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"symbol\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"symbol\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "\n",
    "# setting up lists to loop through for sentiment score and weighted scores analysis\n",
    "stock_news_datasets = [\n",
    "    \"raw_news_data/AMZN_news_data.csv\",\n",
    "    \"raw_news_data/AAPL_news_data.csv\",\n",
    "    \"raw_news_data/GOOG_news_data.csv\",\n",
    "    \"raw_news_data/MSFT_news_data.csv\",\n",
    "    \"raw_news_data/META_news_data.csv\",\n",
    "    \"raw_news_data/NVDA_news_data.csv\"\n",
    "]\n",
    "\n",
    "news_press_releases_datasets = [\n",
    "    \"press_release_data/AMZN_press_release_data.csv\",\n",
    "    \"press_release_data/AAPL_press_release_data.csv\",\n",
    "    \"press_release_data/GOOG_press_release_data.csv\",\n",
    "    \"press_release_data/MSFT_press_release_data.csv\",\n",
    "    \"press_release_data/META_press_release_data.csv\",\n",
    "    \"press_release_data/NVDA_press_release_data.csv\"\n",
    "]\n",
    "\n",
    "social_sentiments_datasets = [\n",
    "    \"social_sentiment_data/AMZN_social_sentiment_data.csv\",\n",
    "    \"social_sentiment_data/AAPL_social_sentiment_data.csv\",\n",
    "    \"social_sentiment_data/GOOG_social_sentiment_data.csv\",\n",
    "    \"social_sentiment_data/MSFT_social_sentiment_data.csv\",\n",
    "    \"social_sentiment_data/META_social_sentiment_data.csv\",\n",
    "    \"social_sentiment_data/NVDA_social_sentiment_data.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock News Data Pull "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Page 29\n",
      "Data for AMZN written to raw_news_data/AMZN_news_data.csv with 29431 articles!\n",
      "Last Page 26\n",
      "Data for AAPL written to raw_news_data/AAPL_news_data.csv with 26589 articles!\n",
      "Last Page 17\n",
      "Data for GOOG written to raw_news_data/GOOG_news_data.csv with 17270 articles!\n",
      "Last Page 16\n",
      "Data for MSFT written to raw_news_data/MSFT_news_data.csv with 16832 articles!\n",
      "Last Page 18\n",
      "Data for META written to raw_news_data/META_news_data.csv with 18504 articles!\n",
      "Last Page 19\n",
      "Data for NVDA written to raw_news_data/NVDA_news_data.csv with 19415 articles!\n",
      "All stock data successfully written to their respective CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Obtaining today's date\n",
    "current_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Obtaining stock news data for each tech company\n",
    "for news in stock_news:\n",
    "\n",
    "    # setting up url path & parameter values\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/stock_news?\"\n",
    "    page = 0\n",
    "    limit = 5000\n",
    "\n",
    "    # creating empty list to capture news data\n",
    "    all_news = []\n",
    "\n",
    "    # looping through pages of data to obtain stock news data\n",
    "    while True:\n",
    "        \n",
    "        # applying parameter values to required params\n",
    "        params = {\n",
    "        \"apikey\" : fmp_api_key,\n",
    "        \"tickers\" : news['symbol'],\n",
    "        \"page\": page,\n",
    "        'from': news['ipo_date'],\n",
    "        'to': current_date,\n",
    "        'limit': limit\n",
    "        }\n",
    "\n",
    "        # requesting data\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {news['symbol']} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        # setting successfully requested data to variable data\n",
    "        data = response.json()\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # appending additional data to list\n",
    "        all_news.extend(data)\n",
    "\n",
    "        # moving to next page\n",
    "        page += 1\n",
    "    \n",
    "    print(f\"Last Page {page - 1}\")\n",
    "\n",
    "    csv_file = f\"raw_news_data/{news['symbol']}_news_data.csv\"\n",
    "    headers = [\"published_date\", \"Headline\", \"Brief\", \"URL\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    # writing all pulled news data to their respective companies in csv files\n",
    "    if all_news:\n",
    "        for record in all_news:\n",
    "\n",
    "            date_str = record.get(\"publishedDate\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "            # Convert from \"YYYY-MM-DD\" to \"DD-MM-YYYY\"\n",
    "            date_str = datetime.strptime(date_str, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "            formatted_data.append({\n",
    "            \"published_date\": date_str,\n",
    "            \"Headline\": record.get(\"title\", \"\"),\n",
    "            \"Brief\": record.get(\"text\", \"\"),\n",
    "            \"URL\": record.get(\"url\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "        \n",
    "        print(f\"Data for {news['symbol']} written to {csv_file} with {len(formatted_data)} articles!\")\n",
    "    else:\n",
    "        print(f\"No data found for {news['symbol']}.\")\n",
    "\n",
    "print(\"All stock data successfully written to their respective CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Press Release Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for AMZN - Page 0...\n",
      "Fetching data for AMZN - Page 1...\n",
      "Fetching data for AMZN - Page 2...\n",
      "Fetching data for AMZN - Page 3...\n",
      "Fetching data for AMZN - Page 4...\n",
      "Fetching data for AMZN - Page 5...\n",
      "Fetching data for AMZN - Page 6...\n",
      "Fetching data for AMZN - Page 7...\n",
      "Fetching data for AMZN - Page 8...\n",
      "Fetching data for AMZN - Page 9...\n",
      "Fetching data for AMZN - Page 10...\n",
      "Fetching data for AMZN - Page 11...\n",
      "Fetching data for AMZN - Page 12...\n",
      "Fetching data for AMZN - Page 13...\n",
      "Fetching data for AMZN - Page 14...\n",
      "Fetching data for AMZN - Page 15...\n",
      "Fetching data for AMZN - Page 16...\n",
      "Fetching data for AMZN - Page 17...\n",
      "Fetching data for AMZN - Page 18...\n",
      "Fetching data for AMZN - Page 19...\n",
      "Fetching data for AMZN - Page 20...\n",
      "Fetching data for AMZN - Page 21...\n",
      "Fetching data for AMZN - Page 22...\n",
      "Fetching data for AMZN - Page 23...\n",
      "Fetching data for AMZN - Page 24...\n",
      "Fetching data for AMZN - Page 25...\n",
      "Fetching data for AMZN - Page 26...\n",
      "Fetching data for AMZN - Page 27...\n",
      "Fetching data for AMZN - Page 28...\n",
      "Fetching data for AMZN - Page 29...\n",
      "Fetching data for AMZN - Page 30...\n",
      "Fetching data for AMZN - Page 31...\n",
      "Fetching data for AMZN - Page 32...\n",
      "Fetching data for AMZN - Page 33...\n",
      "Fetching data for AMZN - Page 34...\n",
      "Fetching data for AMZN - Page 35...\n",
      "Fetching data for AMZN - Page 36...\n",
      "Fetching data for AMZN - Page 37...\n",
      "Fetching data for AMZN - Page 38...\n",
      "Fetching data for AMZN - Page 39...\n",
      "Fetching data for AMZN - Page 40...\n",
      "Fetching data for AMZN - Page 41...\n",
      "Fetching data for AMZN - Page 42...\n",
      "Fetching data for AMZN - Page 43...\n",
      "Fetching data for AMZN - Page 44...\n",
      "Fetching data for AMZN - Page 45...\n",
      "Fetching data for AMZN - Page 46...\n",
      "Fetching data for AMZN - Page 47...\n",
      "Fetching data for AMZN - Page 48...\n",
      "Fetching data for AMZN - Page 49...\n",
      "Fetching data for AMZN - Page 50...\n",
      "Fetching data for AMZN - Page 51...\n",
      "Fetching data for AMZN - Page 52...\n",
      "Fetching data for AMZN - Page 53...\n",
      "Fetching data for AMZN - Page 54...\n",
      "Fetching data for AMZN - Page 55...\n",
      "Fetching data for AMZN - Page 56...\n",
      "Fetching data for AMZN - Page 57...\n",
      "Fetching data for AMZN - Page 58...\n",
      "Fetching data for AMZN - Page 59...\n",
      "Fetching data for AMZN - Page 60...\n",
      "Fetching data for AMZN - Page 61...\n",
      "Fetching data for AMZN - Page 62...\n",
      "Fetching data for AMZN - Page 63...\n",
      "Fetching data for AMZN - Page 64...\n",
      "Fetching data for AMZN - Page 65...\n",
      "Fetching data for AMZN - Page 66...\n",
      "Fetching data for AMZN - Page 67...\n",
      "Fetching data for AMZN - Page 68...\n",
      "Fetching data for AMZN - Page 69...\n",
      "Fetching data for AMZN - Page 70...\n",
      "Fetching data for AMZN - Page 71...\n",
      "Fetching data for AMZN - Page 72...\n",
      "Fetching data for AMZN - Page 73...\n",
      "Fetching data for AMZN - Page 74...\n",
      "Fetching data for AMZN - Page 75...\n",
      "Fetching data for AMZN - Page 76...\n",
      "Fetching data for AMZN - Page 77...\n",
      "Fetching data for AMZN - Page 78...\n",
      "Fetching data for AMZN - Page 79...\n",
      "Fetching data for AMZN - Page 80...\n",
      "Fetching data for AMZN - Page 81...\n",
      "Fetching data for AMZN - Page 82...\n",
      "Fetching data for AMZN - Page 83...\n",
      "Fetching data for AMZN - Page 84...\n",
      "Fetching data for AMZN - Page 85...\n",
      "Fetching data for AMZN - Page 86...\n",
      "Fetching data for AMZN - Page 87...\n",
      "Fetching data for AMZN - Page 88...\n",
      "Fetching data for AMZN - Page 89...\n",
      "Fetching data for AMZN - Page 90...\n",
      "Fetching data for AMZN - Page 91...\n",
      "Fetching data for AMZN - Page 92...\n",
      "Fetching data for AMZN - Page 93...\n",
      "Fetching data for AMZN - Page 94...\n",
      "Fetching data for AMZN - Page 95...\n",
      "Fetching data for AMZN - Page 96...\n",
      "Fetching data for AMZN - Page 97...\n",
      "Fetching data for AMZN - Page 98...\n",
      "Fetching data for AMZN - Page 99...\n",
      "Fetching data for AMZN - Page 100...\n",
      "Fetching data for AMZN - Page 101...\n",
      "Fetching data for AMZN - Page 102...\n",
      "Fetching data for AMZN - Page 103...\n",
      "Fetching data for AMZN - Page 104...\n",
      "Fetching data for AMZN - Page 105...\n",
      "Fetching data for AMZN - Page 106...\n",
      "Fetching data for AMZN - Page 107...\n",
      "Fetching data for AMZN - Page 108...\n",
      "Fetching data for AMZN - Page 109...\n",
      "Fetching data for AMZN - Page 110...\n",
      "Fetching data for AMZN - Page 111...\n",
      "Fetching data for AMZN - Page 112...\n",
      "Fetching data for AMZN - Page 113...\n",
      "Fetching data for AMZN - Page 114...\n",
      "Fetching data for AMZN - Page 115...\n",
      "Fetching data for AMZN - Page 116...\n",
      "Fetching data for AMZN - Page 117...\n",
      "Fetching data for AMZN - Page 118...\n",
      "Fetching data for AMZN - Page 119...\n",
      "Fetching data for AMZN - Page 120...\n",
      "Fetching data for AMZN - Page 121...\n",
      "Fetching data for AMZN - Page 122...\n",
      "Fetching data for AMZN - Page 123...\n",
      "Fetching data for AMZN - Page 124...\n",
      "Fetching data for AMZN - Page 125...\n",
      "Fetching data for AMZN - Page 126...\n",
      "Fetching data for AMZN - Page 127...\n",
      "Fetching data for AMZN - Page 128...\n",
      "Fetching data for AMZN - Page 129...\n",
      "Fetching data for AMZN - Page 130...\n",
      "Fetching data for AMZN - Page 131...\n",
      "Fetching data for AMZN - Page 132...\n",
      "Fetching data for AMZN - Page 133...\n",
      "Fetching data for AMZN - Page 134...\n",
      "Fetching data for AMZN - Page 135...\n",
      "Fetching data for AMZN - Page 136...\n",
      "Fetching data for AMZN - Page 137...\n",
      "Fetching data for AMZN - Page 138...\n",
      "Fetching data for AMZN - Page 139...\n",
      "Fetching data for AMZN - Page 140...\n",
      "Fetching data for AMZN - Page 141...\n",
      "Fetching data for AMZN - Page 142...\n",
      "Fetching data for AMZN - Page 143...\n",
      "Fetching data for AMZN - Page 144...\n",
      "Fetching data for AMZN - Page 145...\n",
      "Fetching data for AMZN - Page 146...\n",
      "Fetching data for AMZN - Page 147...\n",
      "Fetching data for AMZN - Page 148...\n",
      "Fetching data for AMZN - Page 149...\n",
      "Fetching data for AMZN - Page 150...\n",
      "Fetching data for AMZN - Page 151...\n",
      "Fetching data for AMZN - Page 152...\n",
      "Fetching data for AMZN - Page 153...\n",
      "Fetching data for AMZN - Page 154...\n",
      "Fetching data for AMZN - Page 155...\n",
      "Fetching data for AMZN - Page 156...\n",
      "Fetching data for AMZN - Page 157...\n",
      "Fetching data for AMZN - Page 158...\n",
      "Fetching data for AMZN - Page 159...\n",
      "Fetching data for AMZN - Page 160...\n",
      "Fetching data for AMZN - Page 161...\n",
      "Fetching data for AMZN - Page 162...\n",
      "Fetching data for AMZN - Page 163...\n",
      "Fetching data for AMZN - Page 164...\n",
      "Fetching data for AMZN - Page 165...\n",
      "Fetching data for AMZN - Page 166...\n",
      "Fetching data for AMZN - Page 167...\n",
      "Fetching data for AMZN - Page 168...\n",
      "Fetching data for AMZN - Page 169...\n",
      "Fetching data for AMZN - Page 170...\n",
      "Fetching data for AMZN - Page 171...\n",
      "Fetching data for AMZN - Page 172...\n",
      "Fetching data for AMZN - Page 173...\n",
      "Fetching data for AMZN - Page 174...\n",
      "Fetching data for AMZN - Page 175...\n",
      "Fetching data for AMZN - Page 176...\n",
      "Fetching data for AMZN - Page 177...\n",
      "Fetching data for AMZN - Page 178...\n",
      "Fetching data for AMZN - Page 179...\n",
      "Fetching data for AMZN - Page 180...\n",
      "Fetching data for AMZN - Page 181...\n",
      "Fetching data for AMZN - Page 182...\n",
      "Fetching data for AMZN - Page 183...\n",
      "Fetching data for AMZN - Page 184...\n",
      "Fetching data for AMZN - Page 185...\n",
      "Fetching data for AMZN - Page 186...\n",
      "Fetching data for AMZN - Page 187...\n",
      "Fetching data for AMZN - Page 188...\n",
      "Fetching data for AMZN - Page 189...\n",
      "Fetching data for AMZN - Page 190...\n",
      "Fetching data for AMZN - Page 191...\n",
      "Fetching data for AMZN - Page 192...\n",
      "Fetching data for AMZN - Page 193...\n",
      "Fetching data for AMZN - Page 194...\n",
      "Fetching data for AMZN - Page 195...\n",
      "Fetching data for AMZN - Page 196...\n",
      "Fetching data for AMZN - Page 197...\n",
      "Fetching data for AMZN - Page 198...\n",
      "Fetching data for AMZN - Page 199...\n",
      "Fetching data for AMZN - Page 200...\n",
      "Fetching data for AMZN - Page 201...\n",
      "Fetching data for AMZN - Page 202...\n",
      "Fetching data for AMZN - Page 203...\n",
      "Fetching data for AMZN - Page 204...\n",
      "Fetching data for AMZN - Page 205...\n",
      "Fetching data for AMZN - Page 206...\n",
      "Fetching data for AMZN - Page 207...\n",
      "Fetching data for AMZN - Page 208...\n",
      "Fetching data for AMZN - Page 209...\n",
      "Fetching data for AMZN - Page 210...\n",
      "Fetching data for AMZN - Page 211...\n",
      "Fetching data for AMZN - Page 212...\n",
      "Fetching data for AMZN - Page 213...\n",
      "Fetching data for AMZN - Page 214...\n",
      "Fetching data for AMZN - Page 215...\n",
      "Fetching data for AMZN - Page 216...\n",
      "Fetching data for AMZN - Page 217...\n",
      "Fetching data for AMZN - Page 218...\n",
      "Fetching data for AMZN - Page 219...\n",
      "Fetching data for AMZN - Page 220...\n",
      "Fetching data for AMZN - Page 221...\n",
      "Fetching data for AMZN - Page 222...\n",
      "Fetching data for AMZN - Page 223...\n",
      "Fetching data for AMZN - Page 224...\n",
      "Fetching data for AMZN - Page 225...\n",
      "Fetching data for AMZN - Page 226...\n",
      "Fetching data for AMZN - Page 227...\n",
      "Fetching data for AMZN - Page 228...\n",
      "Fetching data for AMZN - Page 229...\n",
      "Fetching data for AMZN - Page 230...\n",
      "Fetching data for AMZN - Page 231...\n",
      "Fetching data for AMZN - Page 232...\n",
      "Fetching data for AMZN - Page 233...\n",
      "Fetching data for AMZN - Page 234...\n",
      "Fetching data for AMZN - Page 235...\n",
      "Fetching data for AMZN - Page 236...\n",
      "Fetching data for AMZN - Page 237...\n",
      "Fetching data for AMZN - Page 238...\n",
      "Fetching data for AMZN - Page 239...\n",
      "Fetching data for AMZN - Page 240...\n",
      "Fetching data for AMZN - Page 241...\n",
      "Fetching data for AMZN - Page 242...\n",
      "Fetching data for AMZN - Page 243...\n",
      "Fetching data for AMZN - Page 244...\n",
      "Fetching data for AMZN - Page 245...\n",
      "Fetching data for AMZN - Page 246...\n",
      "Fetching data for AMZN - Page 247...\n",
      "Fetching data for AMZN - Page 248...\n",
      "Fetching data for AMZN - Page 249...\n",
      "Fetching data for AMZN - Page 250...\n",
      "Fetching data for AMZN - Page 251...\n",
      "Fetching data for AMZN - Page 252...\n",
      "Fetching data for AMZN - Page 253...\n",
      "Fetching data for AMZN - Page 254...\n",
      "Fetching data for AMZN - Page 255...\n",
      "Fetching data for AMZN - Page 256...\n",
      "Fetching data for AMZN - Page 257...\n",
      "Fetching data for AMZN - Page 258...\n",
      "Fetching data for AMZN - Page 259...\n",
      "Fetching data for AMZN - Page 260...\n",
      "Fetching data for AMZN - Page 261...\n",
      "Fetching data for AMZN - Page 262...\n",
      "Fetching data for AMZN - Page 263...\n",
      "Fetching data for AMZN - Page 264...\n",
      "Fetching data for AMZN - Page 265...\n",
      "Fetching data for AMZN - Page 266...\n",
      "Fetching data for AMZN - Page 267...\n",
      "Fetching data for AMZN - Page 268...\n",
      "Fetching data for AMZN - Page 269...\n",
      "Fetching data for AMZN - Page 270...\n",
      "Fetching data for AMZN - Page 271...\n",
      "Fetching data for AMZN - Page 272...\n",
      "Fetching data for AMZN - Page 273...\n",
      "Fetching data for AMZN - Page 274...\n",
      "Fetching data for AMZN - Page 275...\n",
      "Fetching data for AMZN - Page 276...\n",
      "Fetching data for AMZN - Page 277...\n",
      "Fetching data for AMZN - Page 278...\n",
      "Fetching data for AMZN - Page 279...\n",
      "Fetching data for AMZN - Page 280...\n",
      "Fetching data for AMZN - Page 281...\n",
      "Fetching data for AMZN - Page 282...\n",
      "Fetching data for AMZN - Page 283...\n",
      "Fetching data for AMZN - Page 284...\n",
      "Fetching data for AMZN - Page 285...\n",
      "Fetching data for AMZN - Page 286...\n",
      "Fetching data for AMZN - Page 287...\n",
      "Fetching data for AMZN - Page 288...\n",
      "Fetching data for AMZN - Page 289...\n",
      "Fetching data for AMZN - Page 290...\n",
      "Fetching data for AMZN - Page 291...\n",
      "Fetching data for AMZN - Page 292...\n",
      "Fetching data for AMZN - Page 293...\n",
      "Fetching data for AMZN - Page 294...\n",
      "Fetching data for AMZN - Page 295...\n",
      "Fetching data for AMZN - Page 296...\n",
      "Fetching data for AMZN - Page 297...\n",
      "Fetching data for AMZN - Page 298...\n",
      "Fetching data for AMZN - Page 299...\n",
      "Data for AMZN written to press_release_data/AMZN_press_release_data.csv with 29609 press releases!\n",
      "Fetching data for AAPL - Page 0...\n",
      "Fetching data for AAPL - Page 1...\n",
      "Fetching data for AAPL - Page 2...\n",
      "Fetching data for AAPL - Page 3...\n",
      "Fetching data for AAPL - Page 4...\n",
      "Fetching data for AAPL - Page 5...\n",
      "Fetching data for AAPL - Page 6...\n",
      "Fetching data for AAPL - Page 7...\n",
      "Fetching data for AAPL - Page 8...\n",
      "Fetching data for AAPL - Page 9...\n",
      "Fetching data for AAPL - Page 10...\n",
      "Fetching data for AAPL - Page 11...\n",
      "Data for AAPL written to press_release_data/AAPL_press_release_data.csv with 888 press releases!\n",
      "Fetching data for GOOG - Page 0...\n",
      "Fetching data for GOOG - Page 1...\n",
      "Fetching data for GOOG - Page 2...\n",
      "Fetching data for GOOG - Page 3...\n",
      "Data for GOOG written to press_release_data/GOOG_press_release_data.csv with 189 press releases!\n",
      "Fetching data for MSFT - Page 0...\n",
      "Fetching data for MSFT - Page 1...\n",
      "Fetching data for MSFT - Page 2...\n",
      "Fetching data for MSFT - Page 3...\n",
      "Fetching data for MSFT - Page 4...\n",
      "Fetching data for MSFT - Page 5...\n",
      "Fetching data for MSFT - Page 6...\n",
      "Fetching data for MSFT - Page 7...\n",
      "Fetching data for MSFT - Page 8...\n",
      "Fetching data for MSFT - Page 9...\n",
      "Data for MSFT written to press_release_data/MSFT_press_release_data.csv with 852 press releases!\n",
      "Fetching data for META - Page 0...\n",
      "Fetching data for META - Page 1...\n",
      "Fetching data for META - Page 2...\n",
      "Data for META written to press_release_data/META_press_release_data.csv with 137 press releases!\n",
      "Fetching data for NVDA - Page 0...\n",
      "Fetching data for NVDA - Page 1...\n",
      "Fetching data for NVDA - Page 2...\n",
      "Fetching data for NVDA - Page 3...\n",
      "Fetching data for NVDA - Page 4...\n",
      "Fetching data for NVDA - Page 5...\n",
      "Fetching data for NVDA - Page 6...\n",
      "Fetching data for NVDA - Page 7...\n",
      "Data for NVDA written to press_release_data/NVDA_press_release_data.csv with 606 press releases!\n",
      "All stock data successfully written to their respective CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Obtaining stock press release data for each tech company\n",
    "for press_release in press_releases:\n",
    "\n",
    "    # setting up url path & parameter values\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/press-releases/{press_release['company']}?\"\n",
    "\n",
    "    # setting page to zero\n",
    "    page = 0\n",
    "\n",
    "    # creating empty list to capture all press release data\n",
    "    all_press_releases = []\n",
    "\n",
    "    max_pages = 300\n",
    "\n",
    "    # looping through pages of data to obtain stock press release data\n",
    "    while page < max_pages:\n",
    "\n",
    "        # applying parameter values to required params\n",
    "        params = {\n",
    "        \"apikey\" : fmp_api_key,\n",
    "        \"page\": page\n",
    "        }\n",
    "\n",
    "        # requesting data\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {press_release['company']} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # setting successfully requested data to variable data\n",
    "        data = response.json()\n",
    "        \n",
    "        print(f\"Fetching data for {press_release['company']} - Page {page}...\")\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "\n",
    "        # appending additional data to list\n",
    "        all_press_releases.extend(data)\n",
    "\n",
    "        # moving to next page\n",
    "        page += 1\n",
    "\n",
    "    csv_file = f\"press_release_data/{press_release['company']}_press_release_data.csv\"\n",
    "    headers = [\"release_date\", \"Headline\", \"Brief\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    # writing all pulled press release data to their respective companies in csv files\n",
    "    if all_press_releases:\n",
    "        for record in all_press_releases:\n",
    "\n",
    "            date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "            # Convert from \"YYYY-MM-DD\" to \"DD-MM-YYYY\"\n",
    "            date_str = datetime.strptime(date_str, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "            formatted_data.append({\n",
    "            \"release_date\": date_str,\n",
    "            \"Headline\": record.get(\"title\", \"\"),\n",
    "            \"Brief\": record.get(\"text\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "        \n",
    "        print(f\"Data for {press_release['company']} written to {csv_file} with {len(formatted_data)} press releases!\")\n",
    "    else:\n",
    "        print(f\"No data found for {press_release['company']}.\")\n",
    "\n",
    "print(\"All stock data successfully written to their respective CSV files.\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiments Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Response for AMZN, Page 0: 70 records\n",
      "\n",
      "🔍 Response for AMZN, Page 1: 65 records\n",
      "\n",
      "🔍 Response for AMZN, Page 2: 64 records\n",
      "\n",
      "🔍 Response for AMZN, Page 3: 66 records\n",
      "\n",
      "🔍 Response for AMZN, Page 4: 64 records\n",
      "\n",
      "🔍 Response for AMZN, Page 5: 74 records\n",
      "\n",
      "🔍 Response for AMZN, Page 6: 71 records\n",
      "\n",
      "🔍 Response for AMZN, Page 7: 66 records\n",
      "\n",
      "🔍 Response for AMZN, Page 8: 67 records\n",
      "\n",
      "🔍 Response for AMZN, Page 9: 62 records\n",
      "\n",
      "🔍 Response for AMZN, Page 10: 64 records\n",
      "\n",
      "🔍 Response for AMZN, Page 11: 65 records\n",
      "\n",
      "🔍 Response for AMZN, Page 12: 66 records\n",
      "\n",
      "🔍 Response for AMZN, Page 13: 71 records\n",
      "\n",
      "🔍 Response for AMZN, Page 14: 66 records\n",
      "\n",
      "🔍 Response for AMZN, Page 15: 71 records\n",
      "\n",
      "🔍 Response for AMZN, Page 16: 67 records\n",
      "\n",
      "🔍 Response for AMZN, Page 17: 66 records\n",
      "\n",
      "🔍 Response for AMZN, Page 18: 73 records\n",
      "\n",
      "🔍 Response for AMZN, Page 19: 70 records\n",
      "\n",
      "🔍 Response for AMZN, Page 20: 81 records\n",
      "\n",
      "🔍 Response for AMZN, Page 21: 68 records\n",
      "\n",
      "🔍 Response for AMZN, Page 22: 79 records\n",
      "\n",
      "🔍 Response for AMZN, Page 23: 73 records\n",
      "\n",
      "🔍 Response for AMZN, Page 24: 77 records\n",
      "\n",
      "🔍 Response for AMZN, Page 25: 72 records\n",
      "\n",
      "🔍 Response for AMZN, Page 26: 166 records\n",
      "\n",
      "🔍 Response for AMZN, Page 27: 66 records\n",
      "\n",
      "🔍 Response for AMZN, Page 28: 66 records\n",
      "\n",
      "🔍 Response for AMZN, Page 29: 68 records\n",
      "\n",
      "🔍 Response for AMZN, Page 30: 21 records\n",
      "\n",
      "🔍 Response for AMZN, Page 31: 0 records\n",
      "No sentiment data found for AMZN.\n",
      "Sentiment data for AMZN written to social_sentiment_data/AMZN_social_sentiment_data.csv with 2185 records!\n",
      "\n",
      "🔍 Response for AAPL, Page 0: 70 records\n",
      "\n",
      "🔍 Response for AAPL, Page 1: 65 records\n",
      "\n",
      "🔍 Response for AAPL, Page 2: 63 records\n",
      "\n",
      "🔍 Response for AAPL, Page 3: 69 records\n",
      "\n",
      "🔍 Response for AAPL, Page 4: 65 records\n",
      "\n",
      "🔍 Response for AAPL, Page 5: 71 records\n",
      "\n",
      "🔍 Response for AAPL, Page 6: 65 records\n",
      "\n",
      "🔍 Response for AAPL, Page 7: 64 records\n",
      "\n",
      "🔍 Response for AAPL, Page 8: 66 records\n",
      "\n",
      "🔍 Response for AAPL, Page 9: 63 records\n",
      "\n",
      "🔍 Response for AAPL, Page 10: 61 records\n",
      "\n",
      "🔍 Response for AAPL, Page 11: 61 records\n",
      "\n",
      "🔍 Response for AAPL, Page 12: 61 records\n",
      "\n",
      "🔍 Response for AAPL, Page 13: 63 records\n",
      "\n",
      "🔍 Response for AAPL, Page 14: 64 records\n",
      "\n",
      "🔍 Response for AAPL, Page 15: 61 records\n",
      "\n",
      "🔍 Response for AAPL, Page 16: 72 records\n",
      "\n",
      "🔍 Response for AAPL, Page 17: 63 records\n",
      "\n",
      "🔍 Response for AAPL, Page 18: 63 records\n",
      "\n",
      "🔍 Response for AAPL, Page 19: 70 records\n",
      "\n",
      "🔍 Response for AAPL, Page 20: 65 records\n",
      "\n",
      "🔍 Response for AAPL, Page 21: 80 records\n",
      "\n",
      "🔍 Response for AAPL, Page 22: 70 records\n",
      "\n",
      "🔍 Response for AAPL, Page 23: 64 records\n",
      "\n",
      "🔍 Response for AAPL, Page 24: 68 records\n",
      "\n",
      "🔍 Response for AAPL, Page 25: 69 records\n",
      "\n",
      "🔍 Response for AAPL, Page 26: 67 records\n",
      "\n",
      "🔍 Response for AAPL, Page 27: 168 records\n",
      "\n",
      "🔍 Response for AAPL, Page 28: 64 records\n",
      "\n",
      "🔍 Response for AAPL, Page 29: 64 records\n",
      "\n",
      "🔍 Response for AAPL, Page 30: 71 records\n",
      "\n",
      "🔍 Response for AAPL, Page 31: 35 records\n",
      "\n",
      "🔍 Response for AAPL, Page 32: 0 records\n",
      "No sentiment data found for AAPL.\n",
      "Sentiment data for AAPL written to social_sentiment_data/AAPL_social_sentiment_data.csv with 2185 records!\n",
      "\n",
      "🔍 Response for GOOG, Page 0: 0 records\n",
      "No sentiment data found for GOOG.\n",
      "No sentiment data found for GOOG.\n",
      "\n",
      "🔍 Response for MSFT, Page 0: 81 records\n",
      "\n",
      "🔍 Response for MSFT, Page 1: 69 records\n",
      "\n",
      "🔍 Response for MSFT, Page 2: 67 records\n",
      "\n",
      "🔍 Response for MSFT, Page 3: 70 records\n",
      "\n",
      "🔍 Response for MSFT, Page 4: 69 records\n",
      "\n",
      "🔍 Response for MSFT, Page 5: 81 records\n",
      "\n",
      "🔍 Response for MSFT, Page 6: 67 records\n",
      "\n",
      "🔍 Response for MSFT, Page 7: 77 records\n",
      "\n",
      "🔍 Response for MSFT, Page 8: 65 records\n",
      "\n",
      "🔍 Response for MSFT, Page 9: 62 records\n",
      "\n",
      "🔍 Response for MSFT, Page 10: 67 records\n",
      "\n",
      "🔍 Response for MSFT, Page 11: 61 records\n",
      "\n",
      "🔍 Response for MSFT, Page 12: 64 records\n",
      "\n",
      "🔍 Response for MSFT, Page 13: 68 records\n",
      "\n",
      "🔍 Response for MSFT, Page 14: 66 records\n",
      "\n",
      "🔍 Response for MSFT, Page 15: 86 records\n",
      "\n",
      "🔍 Response for MSFT, Page 16: 70 records\n",
      "\n",
      "🔍 Response for MSFT, Page 17: 77 records\n",
      "\n",
      "🔍 Response for MSFT, Page 18: 76 records\n",
      "\n",
      "🔍 Response for MSFT, Page 19: 83 records\n",
      "\n",
      "🔍 Response for MSFT, Page 20: 85 records\n",
      "\n",
      "🔍 Response for MSFT, Page 21: 92 records\n",
      "\n",
      "🔍 Response for MSFT, Page 22: 86 records\n",
      "\n",
      "🔍 Response for MSFT, Page 23: 80 records\n",
      "\n",
      "🔍 Response for MSFT, Page 24: 168 records\n",
      "\n",
      "🔍 Response for MSFT, Page 25: 69 records\n",
      "\n",
      "🔍 Response for MSFT, Page 26: 64 records\n",
      "\n",
      "🔍 Response for MSFT, Page 27: 76 records\n",
      "\n",
      "🔍 Response for MSFT, Page 28: 32 records\n",
      "\n",
      "🔍 Response for MSFT, Page 29: 0 records\n",
      "No sentiment data found for MSFT.\n",
      "Sentiment data for MSFT written to social_sentiment_data/MSFT_social_sentiment_data.csv with 2178 records!\n",
      "\n",
      "🔍 Response for META, Page 0: 83 records\n",
      "\n",
      "🔍 Response for META, Page 1: 67 records\n",
      "\n",
      "🔍 Response for META, Page 2: 68 records\n",
      "\n",
      "🔍 Response for META, Page 3: 66 records\n",
      "\n",
      "🔍 Response for META, Page 4: 64 records\n",
      "\n",
      "🔍 Response for META, Page 5: 75 records\n",
      "\n",
      "🔍 Response for META, Page 6: 63 records\n",
      "\n",
      "🔍 Response for META, Page 7: 68 records\n",
      "\n",
      "🔍 Response for META, Page 8: 67 records\n",
      "\n",
      "🔍 Response for META, Page 9: 64 records\n",
      "\n",
      "🔍 Response for META, Page 10: 68 records\n",
      "\n",
      "🔍 Response for META, Page 11: 62 records\n",
      "\n",
      "🔍 Response for META, Page 12: 61 records\n",
      "\n",
      "🔍 Response for META, Page 13: 66 records\n",
      "\n",
      "🔍 Response for META, Page 14: 66 records\n",
      "\n",
      "🔍 Response for META, Page 15: 62 records\n",
      "\n",
      "🔍 Response for META, Page 16: 67 records\n",
      "\n",
      "🔍 Response for META, Page 17: 63 records\n",
      "\n",
      "🔍 Response for META, Page 18: 72 records\n",
      "\n",
      "🔍 Response for META, Page 19: 69 records\n",
      "\n",
      "🔍 Response for META, Page 20: 81 records\n",
      "\n",
      "🔍 Response for META, Page 21: 90 records\n",
      "\n",
      "🔍 Response for META, Page 22: 96 records\n",
      "\n",
      "🔍 Response for META, Page 23: 82 records\n",
      "\n",
      "🔍 Response for META, Page 24: 95 records\n",
      "\n",
      "🔍 Response for META, Page 25: 170 records\n",
      "\n",
      "🔍 Response for META, Page 26: 73 records\n",
      "\n",
      "🔍 Response for META, Page 27: 85 records\n",
      "\n",
      "🔍 Response for META, Page 28: 69 records\n",
      "\n",
      "🔍 Response for META, Page 29: 0 records\n",
      "No sentiment data found for META.\n",
      "Sentiment data for META written to social_sentiment_data/META_social_sentiment_data.csv with 2182 records!\n",
      "\n",
      "🔍 Response for NVDA, Page 0: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 1: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 2: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 3: 62 records\n",
      "\n",
      "🔍 Response for NVDA, Page 4: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 5: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 6: 63 records\n",
      "\n",
      "🔍 Response for NVDA, Page 7: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 8: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 9: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 10: 60 records\n",
      "\n",
      "🔍 Response for NVDA, Page 11: 60 records\n",
      "\n",
      "🔍 Response for NVDA, Page 12: 60 records\n",
      "\n",
      "🔍 Response for NVDA, Page 13: 60 records\n",
      "\n",
      "🔍 Response for NVDA, Page 14: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 15: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 16: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 17: 63 records\n",
      "\n",
      "🔍 Response for NVDA, Page 18: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 19: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 20: 62 records\n",
      "\n",
      "🔍 Response for NVDA, Page 21: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 22: 93 records\n",
      "\n",
      "🔍 Response for NVDA, Page 23: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 24: 62 records\n",
      "\n",
      "🔍 Response for NVDA, Page 25: 62 records\n",
      "\n",
      "🔍 Response for NVDA, Page 26: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 27: 62 records\n",
      "\n",
      "🔍 Response for NVDA, Page 28: 62 records\n",
      "\n",
      "🔍 Response for NVDA, Page 29: 154 records\n",
      "\n",
      "🔍 Response for NVDA, Page 30: 62 records\n",
      "\n",
      "🔍 Response for NVDA, Page 31: 61 records\n",
      "\n",
      "🔍 Response for NVDA, Page 32: 62 records\n",
      "\n",
      "🔍 Response for NVDA, Page 33: 42 records\n",
      "\n",
      "🔍 Response for NVDA, Page 34: 0 records\n",
      "No sentiment data found for NVDA.\n",
      "Sentiment data for NVDA written to social_sentiment_data/NVDA_social_sentiment_data.csv with 2188 records!\n",
      "\n",
      "All sentiment data successfully written to CSV files.\n"
     ]
    }
   ],
   "source": [
    "# # current_date = date.today()\n",
    " \n",
    "# # Loop through each stock symbol to fetch sentiment data\n",
    "# for social_sentiment in historical_social_sentiment:\n",
    "#     symbol = social_sentiment[\"symbol\"]\n",
    "#     ipo_date = social_sentiment[\"ipo_date\"]\n",
    " \n",
    "#     # Correct API URL\n",
    "#     url = f\"https://financialmodelingprep.com/api/v4/historical/social-sentiment?symbol={symbol}\"\n",
    " \n",
    "#     page = 0\n",
    "#     all_historical_social_sentiment = []\n",
    " \n",
    "#     while True:\n",
    "#         social_params = {\n",
    "#             \"apikey\": fmp_api_key,\n",
    "#             \"page\": page\n",
    "#         }\n",
    " \n",
    "#         try:\n",
    "#             response = requests.get(url, params=social_params)\n",
    "#             response.raise_for_status()\n",
    "#             data = response.json()\n",
    " \n",
    "#             # Print API response for debugging\n",
    "#             print(f\"\\nResponse for {symbol}, Page {page}: {len(data)} records\")\n",
    " \n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             print(f\"Error fetching data for {symbol} on page {page}: {e}\")\n",
    "#             break\n",
    "\n",
    "\n",
    "#         time.sleep(1)\n",
    " \n",
    "#         # Stop if no data is returned\n",
    "#         if not data:\n",
    "#             print(f\"No sentiment data found for {symbol}.\")\n",
    "#             break\n",
    " \n",
    "#         all_historical_social_sentiment.extend(data)\n",
    "#         page += 1  # Move to the next page\n",
    " \n",
    "#     # CSV file name (each company has its own file)\n",
    "#     csv_file = f\"social_sentiment_data/{symbol}_social_sentiment_data.csv\"\n",
    " \n",
    "#     # Updated headers to match API response\n",
    "#     headers = [\n",
    "#         \"published_date\", \"twitter_posts\", \"twitter_likes\", \"twitter_sentiment\",\n",
    "#         \"stock_twitter_posts\", \"stock_twitter_likes\", \"sentiment_score\"\n",
    "#     ]\n",
    " \n",
    "#     formatted_data = []\n",
    " \n",
    "#     if all_historical_social_sentiment:\n",
    "#         for record in all_historical_social_sentiment:\n",
    "#             # Extract only the date part\n",
    "#             date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "#             # Convert from \"YYYY-MM-DD\" to \"DD-MM-YYYY\"\n",
    "#             date_str = datetime.strptime(date_str, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    " \n",
    "#             formatted_data.append({\n",
    "#                 \"published_date\": date_str,\n",
    "#                 \"twitter_posts\": record.get(\"twitterPosts\", \"\"),\n",
    "#                 \"twitter_likes\": record.get(\"twitterLikes\", \"\"),\n",
    "#                 \"twitter_sentiment\": record.get(\"twitterSentiment\", \"\"),\n",
    "#                 \"stock_twitter_posts\": record.get(\"stocktwitsPosts\", \"\"),\n",
    "#                 \"stock_twitter_likes\": record.get(\"stocktwitsLikes\", \"\"),  # Fixed duplicate key\n",
    "#                 \"sentiment_score\": record.get(\"stocktwitsSentiment\", \"\")\n",
    "#             })\n",
    " \n",
    "#         # Write data to a CSV file for this company\n",
    "#         with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#             writer = csv.DictWriter(file, fieldnames=headers)\n",
    "#             writer.writeheader()\n",
    "#             writer.writerows(formatted_data)\n",
    " \n",
    "#         print(f\"Sentiment data for {symbol} written to {csv_file} with {len(formatted_data)} records!\")\n",
    "#     else:\n",
    "#         print(f\"No sentiment data found for {symbol}.\")\n",
    " \n",
    "# print(\"\\nAll sentiment data successfully written to CSV files.\")\n",
    "\n",
    "# Function to subtract months from a date\n",
    "def subtract_months(start_date, months):\n",
    "    new_date = datetime.strptime(start_date, \"%Y-%m-%d\") - timedelta(days=months * 30)\n",
    "    return new_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Loop through each stock symbol to fetch sentiment data\n",
    "for social_sentiment in historical_social_sentiment:\n",
    "    symbol = social_sentiment[\"symbol\"]\n",
    "    ipo_date = social_sentiment[\"ipo_date\"]\n",
    "\n",
    "    # Start from today and loop back in 3-month chunks\n",
    "    current_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # CSV file name (each company has its own file)\n",
    "    csv_file = f\"{symbol}_social_sentiment_data.csv\"\n",
    "\n",
    "    # Define headers\n",
    "    headers = [\n",
    "        \"published_date\", \"twitter_posts\", \"twitter_likes\", \"twitter_sentiment\",\n",
    "        \"stock_twitter_posts\", \"stock_twitter_likes\", \"sentiment_score\"\n",
    "    ]\n",
    "\n",
    "    # Initialize CSV file and write headers\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "\n",
    "    # Process data in 3-month chunks until reaching the IPO date\n",
    "    while current_date > ipo_date:\n",
    "        from_date = subtract_months(current_date, 3)\n",
    "        if from_date < ipo_date:\n",
    "            from_date = ipo_date  # Ensure we don't go before the IPO date\n",
    "\n",
    "        print(f\"\\nFetching data for {symbol} from {from_date} to {current_date}...\")\n",
    "\n",
    "        page = 0\n",
    "        all_historical_social_sentiment = []\n",
    "\n",
    "        while True:\n",
    "            # API Request\n",
    "            url = f\"https://financialmodelingprep.com/api/v4/historical/social-sentiment?symbol={symbol}\"\n",
    "            params = {\n",
    "                \"apikey\": fmp_api_key,\n",
    "                \"page\": page\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "\n",
    "                # Debugging: Print API response size\n",
    "                print(f\"Page {page} - {len(data)} records fetched\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching data for {symbol} from {from_date} to {current_date}, Page {page}: {e}\")\n",
    "                break\n",
    "\n",
    "            time.sleep(1)  # Prevent rate-limiting issues\n",
    "\n",
    "            # Stop if no data is returned\n",
    "            if not data:\n",
    "                print(f\"No sentiment data found for {symbol} from {from_date} to {current_date}.\")\n",
    "                break\n",
    "\n",
    "            all_historical_social_sentiment.extend(data)\n",
    "            page += 1  # Move to the next page\n",
    "\n",
    "        # Format and write data to CSV\n",
    "        formatted_data = []\n",
    "        if all_historical_social_sentiment:\n",
    "            for record in all_historical_social_sentiment:\n",
    "                date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "                formatted_data.append({\n",
    "                    \"published_date\": date_str,\n",
    "                    \"twitter_posts\": record.get(\"twitterPosts\", \"\"),\n",
    "                    \"twitter_likes\": record.get(\"twitterLikes\", \"\"),\n",
    "                    \"twitter_sentiment\": record.get(\"twitterSentiment\", \"\"),\n",
    "                    \"stock_twitter_posts\": record.get(\"stocktwitsPosts\", \"\"),\n",
    "                    \"stock_twitter_likes\": record.get(\"stocktwitsLikes\", \"\"),\n",
    "                    \"sentiment_score\": record.get(\"stocktwitsSentiment\", \"\")\n",
    "                })\n",
    "\n",
    "            # Append data to CSV\n",
    "            with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=headers)\n",
    "                writer.writerows(formatted_data)\n",
    "\n",
    "            print(f\"{len(formatted_data)} records written for {symbol} from {from_date} to {current_date}\")\n",
    "\n",
    "        # Move to the next 3-month period (backwards)\n",
    "        current_date = from_date\n",
    "\n",
    "print(\"\\nAll sentiment data successfully written to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text PreProcessing & Function Creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating a function to clean text for VADER sentiment analysis\n",
    "def text_cleaning(text, dataset):\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace(r\"http\\S+|www\\S+\", \"\", regex=True) # removing URLS\n",
    "    text = text.str.replace(r\"[^a-zA-Z0-9$%.,!?'\\s-]\", \"\", regex=True) # keeping relevant characters\n",
    "    text = text.str.replace(r\"\\s+\", \" \", regex=True).str.strip() # removing extra spaces\n",
    "    text = text.str.replace(r\"\\b(\\d+)%\", r\"\\1 percent\", regex=True) # converting percentages\n",
    "    text = text.str.replace(r\"\\b(\\d+)M\\b\", r\"\\1 million\", regex=True)  # Convert M to million\n",
    "    text = text.str.replace(r\"\\b(\\d+)B\\b\", r\"\\1 billion\", regex=True)  # Convert B to billion\n",
    "\n",
    "    # splitting texts manually based on \".!?\"\n",
    "    sentences = text.str.split(r'[.!?]\\s+', regex=True)\n",
    "    \n",
    "    # creating column for the cleaned text and naming it 'cleaned'\n",
    "    dataset['cleaned'] = sentences\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to obtain sentiment scores\n",
    "def sentiment_score_calculator(datasets_lists):\n",
    "\n",
    "    # Initializing VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "    # looping through datasets \n",
    "    for file in datasets_lists:\n",
    "        \n",
    "        # reading in file \n",
    "        news_press_release = pd.read_csv(file)\n",
    "\n",
    "        # combining two text columns into one column to clean text in one go\n",
    "        news_press_release['raw'] = news_press_release.apply(lambda row: row['Headline'] if pd.isna(row['Brief']) \n",
    "                                                        else (row['Brief'] if pd.isna(row['Headline']) \n",
    "                                                            else row['Headline'] + \". \" + row['Brief']), axis=1)\n",
    "\n",
    "        # cleaning text cleaning function with VADER appliation\n",
    "        text_cleaning(news_press_release['raw'], news_press_release)\n",
    "\n",
    "\n",
    "        # creating empty lists\n",
    "        score_rating = []\n",
    "        score = []\n",
    "\n",
    "        # looping through each row in the column 'cleaned'\n",
    "        for row in news_press_release['cleaned']:\n",
    "            \n",
    "            row_score = 0\n",
    "\n",
    "            # looping through each sentence in current row and using vader to score each sentence\n",
    "            for sentence in row:\n",
    "                try:\n",
    "                    print(sentence)\n",
    "                    sentence_score = analyzer.polarity_scores(sentence)[\"compound\"] # vader polarity score analyzer\n",
    "                    print(sentence_score)\n",
    "                    print(\"Done with sentence_score in row, onto next sentence.\")\n",
    "\n",
    "                    # accumulating each sentence's score for current row\n",
    "                    row_score += sentence_score\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error processing sentence: {sentence}\\n{e}\")\n",
    "                    continue\n",
    "\n",
    "            # averaging row scores and appending to score\n",
    "            avg_row_score = row_score / len(row)\n",
    "            score.append(avg_row_score)\n",
    "\n",
    "            print(f\"row_score: {avg_row_score}\")\n",
    "            print(len(row))\n",
    "            print(\"end of row, onto next row.\")\n",
    "\n",
    "            # classifying averaged row scores into various score ratings\n",
    "            if avg_row_score > 0.05 and avg_row_score < 0.5:\n",
    "                score_rating.append(\"weakly_positive\")\n",
    "                print(\"Weakly Positive\")\n",
    "            elif avg_row_score > 0.5:\n",
    "                score_rating.append(\"strongly_positive\")\n",
    "                print(\"Strongly Positive\")\n",
    "            elif avg_row_score > -0.5 and avg_row_score < -0.05:\n",
    "                score_rating.append(\"weakly_negative\")\n",
    "                print(\"Weakly Negative\")\n",
    "            elif avg_row_score < -0.5:\n",
    "                score_rating.append(\"strongly_negative\")\n",
    "                print(\"Strongly Negative\")\n",
    "            else:\n",
    "                score_rating.append(\"neutral\")\n",
    "                print(\"Neutral\")\n",
    "\n",
    "        # creating columns for sentiment and score\n",
    "        news_press_release['sentiment'] = score_rating\n",
    "        news_press_release['sentiment_score'] = score\n",
    "\n",
    "        # writing the updated vader sentiment scores to each file \n",
    "        news_press_release.to_csv(file, index=False)\n",
    "        print(f\"Finished updating {file} with sentiment rating and sentiment score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_sentiments(datasets_lists):\n",
    "\n",
    "    # looping through each dataset and applying aggregated sentiments\n",
    "    for file in datasets_lists:\n",
    "\n",
    "        if not file or not os.path.exists(file):\n",
    "            print(f\"Skipping missing file: {file}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # reading in dataset\n",
    "            news_press_release_sentiment = pd.read_csv(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "        \n",
    "\n",
    "        # Checking if either 'published_date' or 'release_date' exists and group by the first one found\n",
    "        date_column = 'published_date' if 'published_date' in news_press_release_sentiment.columns else 'release_date'\n",
    "\n",
    "        # computing average score per unique publilshed_date\n",
    "        avg_scores = news_press_release_sentiment.groupby(date_column)['sentiment_score'].mean().reset_index()\n",
    "\n",
    "        # classifying weighted daily scores into classes\n",
    "\n",
    "        # Defining conditions for classification\n",
    "        conditions = [\n",
    "            avg_scores['sentiment_score'] > 0.5,\n",
    "            (avg_scores['sentiment_score'] > 0.05) & (avg_scores['sentiment_score'] <= 0.5),\n",
    "            (avg_scores['sentiment_score'] >= -0.05) & (avg_scores['sentiment_score'] <= 0.05),\n",
    "            (avg_scores['sentiment_score'] < -0.05) & (avg_scores['sentiment_score'] >= -0.5),\n",
    "            avg_scores['sentiment_score'] < -0.5\n",
    "        ]\n",
    "\n",
    "        # Defining corresponding classifications\n",
    "        classifications = [\n",
    "            \"strongly_positive\",\n",
    "            \"weakly_positive\",\n",
    "            \"neutral\",\n",
    "            \"weakly_negative\",\n",
    "            \"strongly_negative\"\n",
    "        ]\n",
    "\n",
    "        # Assigning classifications based on conditions\n",
    "        avg_scores['weighted_daily_sentiment'] = np.select(conditions, classifications, default=\"neutral\")\n",
    "        \n",
    "        # merging to assign classifications to each row\n",
    "        news_press_release_sentiment = news_press_release_sentiment.merge(avg_scores[[date_column, 'weighted_daily_sentiment']], on=date_column, how='left')\n",
    "\n",
    "        # writing to csv files\n",
    "        news_press_release_sentiment.to_csv(file, index=False)\n",
    "        print(f\"Finished updating {file} with weighted daily sentiment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentiments score for news and press release datasets for each company\n",
    "sentiment_score_calculator(stock_news_datasets)\n",
    "sentiment_score_calculator(news_press_releases_datasets)\n",
    "\n",
    "# Calculating aggregated sentiments(daily) for news, press release and social sentiments datasets for each company\n",
    "aggregated_sentiments(stock_news_datasets)\n",
    "aggregated_sentiments(news_press_releases_datasets)\n",
    "aggregated_sentiments(social_sentiments_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in company data and Creating Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting company names \n",
    "company_names = ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']\n",
    "\n",
    "# Dictionary to store datasets\n",
    "company_data = {}\n",
    "\n",
    "\n",
    "# reading in files\n",
    "for names in company_names:\n",
    "\n",
    "    if names == 'GOOG':\n",
    "\n",
    "        company_data[names] = {\n",
    "            \"stock_news\": pd.read_csv(f\"raw_news_data/{names}_news_data.csv\"),\n",
    "            \"press_releases\": pd.read_csv(f\"press_release_data/{names}_press_release_data.csv\")\n",
    "        }\n",
    "\n",
    "    else:\n",
    "\n",
    "        try:\n",
    "            company_data[names] = {\n",
    "                \"stock_news\": pd.read_csv(f\"raw_news_data/{names}_news_data.csv\"),\n",
    "                \"press_releases\": pd.read_csv(f\"press_release_data/{names}_press_release_data.csv\"),\n",
    "                \"social_sentiments\": pd.read_csv(f\"social_sentiment_data/{names}_social_sentiment_data.csv\")\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {names}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "dataset_variables = list(company_data.keys())\n",
    "\n",
    "dataset_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates on stock_news data removed. Number of rows remaining: 2432\n",
      "Duplicates on press_release data removed. Number of rows remaining: 573\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 91\n",
      "Finished updating AAPL with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 3299\n",
      "Duplicates on press_release data removed. Number of rows remaining: 710\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 91\n",
      "Finished updating AMZN with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 2131\n",
      "Duplicates on press_release data removed. Number of rows remaining: 165\n",
      "Finished updating GOOG with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 2048\n",
      "Duplicates on press_release data removed. Number of rows remaining: 114\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 91\n",
      "Finished updating META with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 2059\n",
      "Duplicates on press_release data removed. Number of rows remaining: 510\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 90\n",
      "Finished updating MSFT with weighted daily sentiment.\n",
      "Duplicates on stock_news data removed. Number of rows remaining: 1965\n",
      "Duplicates on press_release data removed. Number of rows remaining: 326\n",
      "Duplicates on social_sentiments data removed. Number of rows remaining: 91\n",
      "Finished updating NVDA with weighted daily sentiment.\n"
     ]
    }
   ],
   "source": [
    "# Picking weighted_daily_sentiment and rename it to twitter_social_sentiment, press_release_sentiment and stock_news_sentiment for each dataset\n",
    "for company in company_names:\n",
    "\n",
    "    if company == 'GOOG':\n",
    "\n",
    "        company_data[company]['stock_news'].rename(columns={'weighted_daily_sentiment' : f'{company}_stock_news_sentiment',\n",
    "                                                        'published_date' : 'date'}, inplace=True)\n",
    "\n",
    "        company_data[company]['press_releases'].rename(columns={'weighted_daily_sentiment' :  f'{company}_press_release_sentiment',\n",
    "                                                            'release_date' : 'date'}, inplace=True)\n",
    "        \n",
    "        # Selecting date & sentiment columns\n",
    "        company_data[company]['stock_news'] = company_data[company]['stock_news'][['date',  f'{company}_stock_news_sentiment']]\n",
    "        company_data[company]['press_releases'] = company_data[company]['press_releases'][['date',  f'{company}_press_release_sentiment']]\n",
    "\n",
    "        # Dropping all duplicates\n",
    "        company_data[company]['stock_news'].drop_duplicates(inplace=True)\n",
    "        company_data[company]['press_releases'].drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "         # Displaying rows left for each dataset for the given company\n",
    "        print('Duplicates on stock_news data removed. Number of rows remaining:', company_data[company]['stock_news'].shape[0])\n",
    "        print('Duplicates on press_release data removed. Number of rows remaining:', company_data[company]['press_releases'].shape[0])\n",
    "\n",
    "        # merging two datasets into one for Google as sentiment data is non-existent\n",
    "        combined_news_data = company_data[company]['stock_news'].merge(company_data[company]['press_releases'], on='date', how='outer')\n",
    "\n",
    "        # Writing processed data to news file for each company\n",
    "        combined_news_data.to_csv(f\"complete_news_data/{company}_complete_news_data.csv\", index=False)\n",
    "        print(f\"Finished updating {company} with weighted daily sentiment.\")\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            company_data[company]['stock_news'].rename(columns={'weighted_daily_sentiment' :  f'{company}_stock_news_sentiment',\n",
    "                                                            'published_date' : 'date'}, inplace=True)\n",
    "\n",
    "            company_data[company]['press_releases'].rename(columns={'weighted_daily_sentiment' :  f'{company}_press_release_sentiment',\n",
    "                                                                'release_date' : 'date'}, inplace=True)\n",
    "\n",
    "            company_data[company]['social_sentiments'].rename(columns={'weighted_daily_sentiment' :  f'{company}_twitter_social_sentiment',\n",
    "                                                                    'published_date' : 'date'}, inplace=True)\n",
    "\n",
    "\n",
    "            # Selecting date & sentiment columns\n",
    "            company_data[company]['stock_news'] = company_data[company]['stock_news'][['date',  f'{company}_stock_news_sentiment']]\n",
    "            company_data[company]['press_releases'] = company_data[company]['press_releases'][['date',  f'{company}_press_release_sentiment']]\n",
    "            company_data[company]['social_sentiments'] = company_data[company]['social_sentiments'][['date',  f'{company}_twitter_social_sentiment']]\n",
    "\n",
    "            # Dropping all duplicates\n",
    "            company_data[company]['stock_news'].drop_duplicates(inplace=True)\n",
    "            company_data[company]['press_releases'].drop_duplicates(inplace=True)\n",
    "            company_data[company]['social_sentiments'].drop_duplicates(inplace=True)\n",
    "\n",
    "            # Displaying rows left for each dataset for the given company\n",
    "            print('Duplicates on stock_news data removed. Number of rows remaining:', company_data[company]['stock_news'].shape[0])\n",
    "            print('Duplicates on press_release data removed. Number of rows remaining:', company_data[company]['press_releases'].shape[0])\n",
    "            print('Duplicates on social_sentiments data removed. Number of rows remaining:', company_data[company]['social_sentiments'].shape[0])\n",
    "\n",
    "\n",
    "            # merging three datasets into one for each company\n",
    "            combined_news_data = company_data[company]['stock_news'].merge(company_data[company]['press_releases'], on='date', how='outer').merge(company_data[company]['social_sentiments'], on='date', how='outer')\n",
    "\n",
    "            # Writing processed data to news file for each company\n",
    "            combined_news_data.to_csv(f\"complete_news_data/{company}_complete_news_data.csv\", index=False)\n",
    "            print(f\"Finished updating {company} with weighted daily sentiment.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {company}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forex Data Pulling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for the desired currencies\n",
    "forex_currencies = ['EURUSD', 'GBPUSD', 'JPYUSD', 'CNHUSD', 'KRWUSD', 'CHFUSD', 'CADUSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing forex_data/EURUSD_forex_data.csv with 1344 records.\n",
      "Finished writing forex_data/GBPUSD_forex_data.csv with 1358 records.\n",
      "Finished writing forex_data/JPYUSD_forex_data.csv with 1348 records.\n",
      "Finished writing forex_data/CNHUSD_forex_data.csv with 1342 records.\n",
      "Finished writing forex_data/KRWUSD_forex_data.csv with 1365 records.\n",
      "Finished writing forex_data/CHFUSD_forex_data.csv with 1364 records.\n",
      "Finished writing forex_data/CADUSD_forex_data.csv with 1378 records.\n"
     ]
    }
   ],
   "source": [
    "# Looping through each currency, pull all the data for the currency relative to USD\n",
    "for currency in forex_currencies:\n",
    "\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{currency}?\"\n",
    "\n",
    "    all_forex_data = []\n",
    "    formatted_data = []\n",
    "\n",
    "    params = {\n",
    "        \"symbol\" : currency,\n",
    "        \"apikey\": fmp_api_key \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url=url, params=params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching forex data for {currency: {e}}\")\n",
    "        continue\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "\n",
    "    if not data:\n",
    "        break\n",
    "\n",
    "    all_forex_data.extend(data.get(\"historical\", []))\n",
    "\n",
    "\n",
    "    csv_file = f\"forex_data/{currency}_forex_data.csv\"\n",
    "\n",
    "    headers = [\"date\", f\"{currency}_open\", f\"{currency}_high\", f\"{currency}_low\", f\"{currency}_close\", f\"{currency}_adjClose\", f\"{currency}_traded_volume\", \n",
    "               f\"{currency}_unadjusted_traded_Volume\", f\"{currency}_change\", f\"{currency}_changePercent\", f\"{currency}_vwap\", f\"{currency}_label\", f\"{currency}_changeOverTime\"]\n",
    "\n",
    "\n",
    "    # Appending data to list\n",
    "    if all_forex_data:\n",
    "        for record in all_forex_data:\n",
    "\n",
    "            date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "            # Convert from \"YYYY-MM-DD\" to \"DD-MM-YYYY\"\n",
    "            date_str = datetime.strptime(date_str, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "            formatted_data.append({\n",
    "                \"date\" : date_str,\n",
    "                f\"{currency}_open\" : record.get(\"open\", \"\"),\n",
    "                f\"{currency}_high\" : record.get(\"high\", \"\"),\n",
    "                f\"{currency}_low\" : record.get(\"low\", \"\"),\n",
    "                f\"{currency}_close\" : record.get(\"close\", \"\"),\n",
    "                f\"{currency}_adjClose\" : record.get(\"adjClose\", \"\"),\n",
    "                f\"{currency}_traded_volume\" : record.get(\"volume\", \"\"),\n",
    "                f\"{currency}_unadjusted_traded_Volume\" : record.get(\"unadjustedVolume\", \"\"),\n",
    "                f\"{currency}_change\" : record.get(\"change\", \"\"),\n",
    "                f\"{currency}_changePercent\" : record.get(\"changePercent\", \"\"),\n",
    "                f\"{currency}_vwap\" : record.get(\"vwap\", \"\"),\n",
    "                f\"{currency}_label\" : record.get(\"label\", \"\"),\n",
    "                f\"{currency}_changeOverTime\" : record.get(\"changeOverTime\", \"\")\n",
    "            })\n",
    "\n",
    "    # Writing appended data to csv file\n",
    "    if formatted_data:\n",
    "        with open(csv_file, mode=\"w\", newline='', encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "\n",
    "        print(f\"Finished writing {csv_file} with {len(formatted_data)} records.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Failed to obtain forex data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PULLING COMMODITY DATA\n",
    "##### Chosen commodities; Gold, Silver, Lithium, Copper, Palladium. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "commodities = [\n",
    "    {\"commodity\": \"Gold\", \"symbol\": \"XAUUSD\"},\n",
    "    {\"commodity\": \"Silver\", \"symbol\": \"XAGUSD\"},\n",
    "    {\"commodity\": \"Lithium\", \"symbol\": \"LIT\"},\n",
    "    {\"commodity\": \"Copper\", \"symbol\": \"HGUSD\"},\n",
    "    {\"commodity\": \"Palladium\", \"symbol\": \"XPDUSD\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for Gold written to commodity_data/Gold_commodity_data.csv with 11625 records!\n",
      "Data for Silver written to commodity_data/Silver_commodity_data.csv with 10685 records!\n",
      "Data for Lithium written to commodity_data/Lithium_commodity_data.csv with 3675 records!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for Copper written to commodity_data/Copper_commodity_data.csv with 9347 records!\n",
      "Data for Palladium written to commodity_data/Palladium_commodity_data.csv with 10088 records!\n",
      "All commodity data successfully written to their respective CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Define current date\n",
    "current_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Iterate through commodities\n",
    "for commodity in commodities:\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{commodity['symbol']}?apikey={fmp_api_key}\"\n",
    "\n",
    "\n",
    "    limit = 1000\n",
    "    max_pages = 250   \n",
    "    all_commodity_data = []\n",
    "\n",
    "    params = {\n",
    "                \"symbol\": commodity['symbol'],\n",
    "                'from': '1980-01-01',  \n",
    "                'to': current_date,\n",
    "                'limit': limit\n",
    "            }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {commodity['commodity']} on page {page}: {e}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    if not data:\n",
    "        break\n",
    "\n",
    "    all_commodity_data.extend(data.get(\"historical\", []))\n",
    "\n",
    "\n",
    "    # Create CSV file for commodity data\n",
    "    csv_file = f\"commodity_data/{commodity['commodity']}_commodity_data.csv\"\n",
    "    headers = [\"date\", f\"{commodity['commodity']}_price\", f\"{commodity['commodity']}_volume\", f\"{commodity['commodity']}_open\", f\"{commodity['commodity']}_close\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    if all_commodity_data:\n",
    "        for record in all_commodity_data:\n",
    "            # # Formats for date parsing\n",
    "            # formats = ['%Y-%m-%d %I:%M:%S %p', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "            if 'date' in record:\n",
    "                 date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "                 date_str = datetime.strptime(date_str, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "                # for fmt in formats:\n",
    "                #     try:\n",
    "                #         record['date'] = datetime.strptime(record['date'], fmt).strftime('%d-%m-%Y')\n",
    "                #         # Convert from \"YYYY-MM-DD\" to \"DD-MM-YYYY\"\n",
    "                        \n",
    "                #         break\n",
    "                #     except ValueError as e:\n",
    "                #         continue\n",
    "\n",
    "            formatted_data.append({\n",
    "                \"date\": date_str,\n",
    "                f\"{commodity['commodity']}_price\": record.get(\"close\", \"\"),  \n",
    "                f\"{commodity['commodity']}_volume\": record.get(\"volume\", \"\"),\n",
    "                f\"{commodity['commodity']}_open\": record.get(\"open\", \"\"),\n",
    "                f\"{commodity['commodity']}_close\": record.get(\"close\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "\n",
    "        print(f\"Data for {commodity['commodity']} written to {csv_file} with {len(formatted_data)} records!\")\n",
    "    else:\n",
    "        print(f\"No data found for {commodity['commodity']}.\")\n",
    "\n",
    "print(\"All commodity data successfully written to their respective CSV files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the Treasury Rates data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched data for 2024-12-04 to 2025-03-04\n",
      "Fetched data for 2024-09-05 to 2024-12-04\n",
      "Fetched data for 2024-06-07 to 2024-09-05\n",
      "Fetched data for 2024-03-09 to 2024-06-07\n",
      "Fetched data for 2023-12-10 to 2024-03-09\n",
      "Fetched data for 2023-09-11 to 2023-12-10\n",
      "Fetched data for 2023-06-13 to 2023-09-11\n",
      "Fetched data for 2023-03-15 to 2023-06-13\n",
      "Fetched data for 2022-12-15 to 2023-03-15\n",
      "Fetched data for 2022-09-16 to 2022-12-15\n",
      "Fetched data for 2022-06-18 to 2022-09-16\n",
      "Fetched data for 2022-03-20 to 2022-06-18\n",
      "Fetched data for 2021-12-20 to 2022-03-20\n",
      "Fetched data for 2021-09-21 to 2021-12-20\n",
      "Fetched data for 2021-06-23 to 2021-09-21\n",
      "Fetched data for 2021-03-25 to 2021-06-23\n",
      "Fetched data for 2020-12-25 to 2021-03-25\n",
      "Fetched data for 2020-09-26 to 2020-12-25\n",
      "Fetched data for 2020-06-28 to 2020-09-26\n",
      "Fetched data for 2020-03-30 to 2020-06-28\n",
      "Fetched data for 2019-12-31 to 2020-03-30\n",
      "Fetched data for 2019-10-02 to 2019-12-31\n",
      "Fetched data for 2019-07-04 to 2019-10-02\n",
      "Fetched data for 2019-04-05 to 2019-07-04\n",
      "Fetched data for 2019-01-05 to 2019-04-05\n",
      "Fetched data for 2018-10-07 to 2019-01-05\n",
      "Fetched data for 2018-07-09 to 2018-10-07\n",
      "Fetched data for 2018-04-10 to 2018-07-09\n",
      "Fetched data for 2018-01-10 to 2018-04-10\n",
      "Fetched data for 2017-10-12 to 2018-01-10\n",
      "Fetched data for 2017-07-14 to 2017-10-12\n",
      "Fetched data for 2017-04-15 to 2017-07-14\n",
      "Fetched data for 2017-01-15 to 2017-04-15\n",
      "Fetched data for 2016-10-17 to 2017-01-15\n",
      "Fetched data for 2016-07-19 to 2016-10-17\n",
      "Fetched data for 2016-04-20 to 2016-07-19\n",
      "Fetched data for 2016-01-21 to 2016-04-20\n",
      "Fetched data for 2015-10-23 to 2016-01-21\n",
      "Fetched data for 2015-07-25 to 2015-10-23\n",
      "Fetched data for 2015-04-26 to 2015-07-25\n",
      "Fetched data for 2015-01-26 to 2015-04-26\n",
      "Fetched data for 2014-10-28 to 2015-01-26\n",
      "Fetched data for 2014-07-30 to 2014-10-28\n",
      "Fetched data for 2014-05-01 to 2014-07-30\n",
      "Fetched data for 2014-01-31 to 2014-05-01\n",
      "Fetched data for 2013-11-02 to 2014-01-31\n",
      "Fetched data for 2013-08-04 to 2013-11-02\n",
      "Fetched data for 2013-05-06 to 2013-08-04\n",
      "Fetched data for 2013-02-05 to 2013-05-06\n",
      "Fetched data for 2012-11-07 to 2013-02-05\n",
      "Fetched data for 2012-08-09 to 2012-11-07\n",
      "Fetched data for 2012-05-11 to 2012-08-09\n",
      "Fetched data for 2012-02-11 to 2012-05-11\n",
      "Fetched data for 2011-11-13 to 2012-02-11\n",
      "Fetched data for 2011-08-15 to 2011-11-13\n",
      "Fetched data for 2011-05-17 to 2011-08-15\n",
      "Fetched data for 2011-02-16 to 2011-05-17\n",
      "Fetched data for 2010-11-18 to 2011-02-16\n",
      "Fetched data for 2010-08-20 to 2010-11-18\n",
      "Fetched data for 2010-05-22 to 2010-08-20\n",
      "Fetched data for 2010-02-21 to 2010-05-22\n",
      "Fetched data for 2009-11-23 to 2010-02-21\n",
      "Fetched data for 2009-08-25 to 2009-11-23\n",
      "Fetched data for 2009-05-27 to 2009-08-25\n",
      "Fetched data for 2009-02-26 to 2009-05-27\n",
      "Fetched data for 2008-11-28 to 2009-02-26\n",
      "Fetched data for 2008-08-30 to 2008-11-28\n",
      "Fetched data for 2008-06-01 to 2008-08-30\n",
      "Fetched data for 2008-03-03 to 2008-06-01\n",
      "Fetched data for 2007-12-04 to 2008-03-03\n",
      "Fetched data for 2007-09-05 to 2007-12-04\n",
      "Fetched data for 2007-06-07 to 2007-09-05\n",
      "Fetched data for 2007-03-09 to 2007-06-07\n",
      "Fetched data for 2006-12-09 to 2007-03-09\n",
      "Fetched data for 2006-09-10 to 2006-12-09\n",
      "Fetched data for 2006-06-12 to 2006-09-10\n",
      "Fetched data for 2006-03-14 to 2006-06-12\n",
      "Fetched data for 2005-12-14 to 2006-03-14\n",
      "Fetched data for 2005-09-15 to 2005-12-14\n",
      "Fetched data for 2005-06-17 to 2005-09-15\n",
      "Fetched data for 2005-03-19 to 2005-06-17\n",
      "Fetched data for 2004-12-19 to 2005-03-19\n",
      "Fetched data for 2004-09-20 to 2004-12-19\n",
      "Fetched data for 2004-06-22 to 2004-09-20\n",
      "Fetched data for 2004-03-24 to 2004-06-22\n",
      "Fetched data for 2003-12-25 to 2004-03-24\n",
      "Fetched data for 2003-09-26 to 2003-12-25\n",
      "Fetched data for 2003-06-28 to 2003-09-26\n",
      "Fetched data for 2003-03-30 to 2003-06-28\n",
      "Fetched data for 2002-12-30 to 2003-03-30\n",
      "Fetched data for 2002-10-01 to 2002-12-30\n",
      "Fetched data for 2002-07-03 to 2002-10-01\n",
      "Fetched data for 2002-04-04 to 2002-07-03\n",
      "Fetched data for 2002-01-04 to 2002-04-04\n",
      "Fetched data for 2001-10-06 to 2002-01-04\n",
      "Fetched data for 2001-07-08 to 2001-10-06\n",
      "Fetched data for 2001-04-09 to 2001-07-08\n",
      "Fetched data for 2001-01-09 to 2001-04-09\n",
      "Fetched data for 2000-10-11 to 2001-01-09\n",
      "Fetched data for 2000-07-13 to 2000-10-11\n",
      "Fetched data for 2000-04-14 to 2000-07-13\n",
      "Fetched data for 2000-01-15 to 2000-04-14\n",
      "Fetched data for 1999-10-17 to 2000-01-15\n",
      "Fetched data for 1999-07-19 to 1999-10-17\n",
      "Fetched data for 1999-04-20 to 1999-07-19\n",
      "Fetched data for 1999-01-20 to 1999-04-20\n",
      "Fetched data for 1998-10-22 to 1999-01-20\n",
      "Fetched data for 1998-07-24 to 1998-10-22\n",
      "Fetched data for 1998-04-25 to 1998-07-24\n",
      "Fetched data for 1998-01-25 to 1998-04-25\n",
      "Fetched data for 1997-10-27 to 1998-01-25\n",
      "Fetched data for 1997-07-29 to 1997-10-27\n",
      "Fetched data for 1997-04-30 to 1997-07-29\n",
      "Fetched data for 1997-01-30 to 1997-04-30\n",
      "Fetched data for 1996-11-01 to 1997-01-30\n",
      "Fetched data for 1996-08-03 to 1996-11-01\n",
      "Fetched data for 1996-05-05 to 1996-08-03\n",
      "Fetched data for 1996-02-05 to 1996-05-05\n",
      "Fetched data for 1995-11-07 to 1996-02-05\n",
      "Fetched data for 1995-08-09 to 1995-11-07\n",
      "Fetched data for 1995-05-11 to 1995-08-09\n",
      "Fetched data for 1995-02-10 to 1995-05-11\n",
      "Fetched data for 1994-11-12 to 1995-02-10\n",
      "Fetched data for 1994-08-14 to 1994-11-12\n",
      "Fetched data for 1994-05-16 to 1994-08-14\n",
      "Fetched data for 1994-02-15 to 1994-05-16\n",
      "Fetched data for 1993-11-17 to 1994-02-15\n",
      "Fetched data for 1993-08-19 to 1993-11-17\n",
      "Fetched data for 1993-05-21 to 1993-08-19\n",
      "Fetched data for 1993-02-20 to 1993-05-21\n",
      "Fetched data for 1992-11-22 to 1993-02-20\n",
      "Fetched data for 1992-08-24 to 1992-11-22\n",
      "Fetched data for 1992-05-26 to 1992-08-24\n",
      "Fetched data for 1992-02-26 to 1992-05-26\n",
      "Fetched data for 1991-11-28 to 1992-02-26\n",
      "Fetched data for 1991-08-30 to 1991-11-28\n",
      "Fetched data for 1991-06-01 to 1991-08-30\n",
      "Fetched data for 1991-03-03 to 1991-06-01\n",
      "Fetched data for 1990-12-03 to 1991-03-03\n",
      "Fetched data for 1990-09-04 to 1990-12-03\n",
      "Fetched data for 1990-06-06 to 1990-09-04\n",
      "Fetched data for 1990-03-08 to 1990-06-06\n",
      "Fetched data for 1989-12-08 to 1990-03-08\n",
      "Treasury rates data written to treasury_rates_data.csv with 8897 records!\n"
     ]
    }
   ],
   "source": [
    "# Define API key and URL\n",
    "# The API key should be loaded before this, as you've mentioned it is already loaded elsewhere\n",
    "base_url = \"https://financialmodelingprep.com/api/v4/treasury\"\n",
    "\n",
    "# Defined date variables\n",
    "end_date = \"1980-01-01\"  \n",
    "current_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Function to subtract months from a date\n",
    "def subtract_months(start_date, months):\n",
    "    new_date = datetime.strptime(start_date, \"%Y-%m-%d\") - timedelta(days=months*30)\n",
    "    return new_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Defined CSV file name\n",
    "csv_file = \"treasury_rates_data.csv\"\n",
    "headers = [\"date\", \"month1\", \"month2\", \"month3\", \"month6\", \"year1\", \"year2\", \"year3\", \"year5\", \"year7\", \"year10\", \"year20\", \"year30\"]\n",
    "formatted_data = []\n",
    "\n",
    "# Initialized the starting date (current date)\n",
    "current_from_date = current_date\n",
    "\n",
    "# Loop to fetch data in 3-month chunks\n",
    "while current_from_date > end_date:\n",
    "    # Calculated the \"from\" and \"to\" dates for the API call (3 months at a time)\n",
    "    current_to_date = current_from_date\n",
    "    current_from_date = subtract_months(current_from_date, 3)\n",
    "    \n",
    "    # Make sure the \"from_date\" does not go earlier than the end date\n",
    "    if current_from_date < end_date:\n",
    "        current_from_date = end_date\n",
    "\n",
    "    # Define parameters for API request\n",
    "    params = {\n",
    "        \"from\": current_from_date,  \n",
    "        \"to\": current_to_date,\n",
    "        \"apikey\": fmp_api_key   \n",
    "    }\n",
    "\n",
    "    # Fetch treasury rates data\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Ensure we catch any HTTP errors\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching treasury rates for {current_from_date} to {current_to_date}: {e}\")\n",
    "        continue  \n",
    "\n",
    "    # Process and format data\n",
    "    if data:\n",
    "        for record in data:\n",
    "            date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "            date_str = datetime.strptime(date_str, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "            formatted_data.append({\n",
    "                \"date\": date_str,\n",
    "                \"month1\": record.get(\"month1\", \"\"),\n",
    "                \"month2\": record.get(\"month2\", \"\"),\n",
    "                \"month3\": record.get(\"month3\", \"\"),\n",
    "                \"month6\": record.get(\"month6\", \"\"),\n",
    "                \"year1\": record.get(\"year1\", \"\"),\n",
    "                \"year2\": record.get(\"year2\", \"\"),\n",
    "                \"year3\": record.get(\"year3\", \"\"),\n",
    "                \"year5\": record.get(\"year5\", \"\"),\n",
    "                \"year7\": record.get(\"year7\", \"\"),\n",
    "                \"year10\": record.get(\"year10\", \"\"),\n",
    "                \"year20\": record.get(\"year20\", \"\"),\n",
    "                \"year30\": record.get(\"year30\", \"\"),\n",
    "            })\n",
    "        print(f\"Fetched data for {current_from_date} to {current_to_date}\")\n",
    "\n",
    "# Write data to CSV after all batches\n",
    "if formatted_data:\n",
    "    with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(formatted_data)\n",
    "\n",
    "    print(f\"Treasury rates data written to {csv_file} with {len(formatted_data)} records!\")\n",
    "else:\n",
    "    print(\"No treasury data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inflation Rates Under Economics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated formatted data in list.\n",
      "Inflation rates data written to inflation_rates_data.csv with 5545 records!\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "url = f\"https://financialmodelingprep.com/api/v4/economic?\"\n",
    "\n",
    "# Defined CSV file name\n",
    "csv_file = \"inflation_rates_data.csv\"\n",
    "\n",
    "headers = [\"date\", \"inflationRate\"]\n",
    "\n",
    "formatted_data = []\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"apikey\": fmp_api_key,\n",
    "    \"name\" : 'inflationRate',\n",
    "    \"from\" : \"1980-12-12\",\n",
    "    \"to\" : current_date\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch Inflation rates data\n",
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()  # Ensure we catch any HTTP errors\n",
    "    data = response.json()\n",
    "    # print(data)\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching inflation rates for '1980-12-12' to {current_date}: {e}\") \n",
    "\n",
    "\n",
    "if data:\n",
    "    for record in data:\n",
    "        \n",
    "        date_str = record.get(\"date\", \"\").split(\" \")[0]  # Extract \"YYYY-MM-DD\" only\n",
    "\n",
    "        # Convert from \"YYYY-MM-DD\" to \"DD-MM-YYYY\"\n",
    "        date_str = datetime.strptime(date_str, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "        formatted_data.append({\n",
    "            \"date\" : date_str,\n",
    "            \"inflationRate\" : record.get(\"value\", \"\") \n",
    "        })\n",
    "    print(\"Updated formatted data in list.\")\n",
    "\n",
    "# Write data to CSV\n",
    "if formatted_data:\n",
    "    with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(formatted_data)\n",
    "\n",
    "    print(f\"Inflation rates data written to {csv_file} with {len(formatted_data)} records!\")\n",
    "else:\n",
    "    print(\"No inflation data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector PE Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for 2025-03-04...\n",
      "Data for 2025-03-04 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-03-03...\n",
      "Data for 2025-03-03 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-03-02...\n",
      "No Technology sector data for 2025-03-02\n",
      "Fetching data for 2025-03-01...\n",
      "No Technology sector data for 2025-03-01\n",
      "Fetching data for 2025-02-28...\n",
      "Data for 2025-02-28 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-27...\n",
      "Data for 2025-02-27 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-26...\n",
      "Data for 2025-02-26 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-25...\n",
      "Data for 2025-02-25 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-24...\n",
      "Data for 2025-02-24 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-23...\n",
      "No Technology sector data for 2025-02-23\n",
      "Fetching data for 2025-02-22...\n",
      "No Technology sector data for 2025-02-22\n",
      "Fetching data for 2025-02-21...\n",
      "Data for 2025-02-21 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-20...\n",
      "Data for 2025-02-20 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-19...\n",
      "Data for 2025-02-19 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-18...\n",
      "Data for 2025-02-18 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-17...\n",
      "Data for 2025-02-17 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-16...\n",
      "No Technology sector data for 2025-02-16\n",
      "Fetching data for 2025-02-15...\n",
      "No Technology sector data for 2025-02-15\n",
      "Fetching data for 2025-02-14...\n",
      "Data for 2025-02-14 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-13...\n",
      "Data for 2025-02-13 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-12...\n",
      "Data for 2025-02-12 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-11...\n",
      "Data for 2025-02-11 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-10...\n",
      "Data for 2025-02-10 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-09...\n",
      "No Technology sector data for 2025-02-09\n",
      "Fetching data for 2025-02-08...\n",
      "No Technology sector data for 2025-02-08\n",
      "Fetching data for 2025-02-07...\n",
      "Data for 2025-02-07 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-06...\n",
      "Data for 2025-02-06 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-05...\n",
      "Data for 2025-02-05 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-04...\n",
      "Data for 2025-02-04 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-03...\n",
      "Data for 2025-02-03 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-02-02...\n",
      "No Technology sector data for 2025-02-02\n",
      "Fetching data for 2025-02-01...\n",
      "No Technology sector data for 2025-02-01\n",
      "Fetching data for 2025-01-31...\n",
      "Data for 2025-01-31 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-30...\n",
      "Data for 2025-01-30 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-29...\n",
      "Data for 2025-01-29 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-28...\n",
      "Data for 2025-01-28 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-27...\n",
      "Data for 2025-01-27 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-26...\n",
      "No Technology sector data for 2025-01-26\n",
      "Fetching data for 2025-01-25...\n",
      "No Technology sector data for 2025-01-25\n",
      "Fetching data for 2025-01-24...\n",
      "Data for 2025-01-24 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-23...\n",
      "Data for 2025-01-23 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-22...\n",
      "Data for 2025-01-22 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-21...\n",
      "Data for 2025-01-21 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-20...\n",
      "Data for 2025-01-20 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-19...\n",
      "No Technology sector data for 2025-01-19\n",
      "Fetching data for 2025-01-18...\n",
      "No Technology sector data for 2025-01-18\n",
      "Fetching data for 2025-01-17...\n",
      "Data for 2025-01-17 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-16...\n",
      "Data for 2025-01-16 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-15...\n",
      "Data for 2025-01-15 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-14...\n",
      "Data for 2025-01-14 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-13...\n",
      "Data for 2025-01-13 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-12...\n",
      "No Technology sector data for 2025-01-12\n",
      "Fetching data for 2025-01-11...\n",
      "No Technology sector data for 2025-01-11\n",
      "Fetching data for 2025-01-10...\n",
      "Data for 2025-01-10 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-09...\n",
      "Data for 2025-01-09 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-08...\n",
      "Data for 2025-01-08 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-07...\n",
      "Data for 2025-01-07 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-06...\n",
      "Data for 2025-01-06 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-05...\n",
      "No Technology sector data for 2025-01-05\n",
      "Fetching data for 2025-01-04...\n",
      "No Technology sector data for 2025-01-04\n",
      "Fetching data for 2025-01-03...\n",
      "Data for 2025-01-03 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-02...\n",
      "Data for 2025-01-02 written to sector_pe_ratio.csv\n",
      "Fetching data for 2025-01-01...\n",
      "Data for 2025-01-01 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-31...\n",
      "Data for 2024-12-31 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-30...\n",
      "Data for 2024-12-30 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-29...\n",
      "No Technology sector data for 2024-12-29\n",
      "Fetching data for 2024-12-28...\n",
      "No Technology sector data for 2024-12-28\n",
      "Fetching data for 2024-12-27...\n",
      "Data for 2024-12-27 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-26...\n",
      "Data for 2024-12-26 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-25...\n",
      "Data for 2024-12-25 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-24...\n",
      "Data for 2024-12-24 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-23...\n",
      "Data for 2024-12-23 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-22...\n",
      "No Technology sector data for 2024-12-22\n",
      "Fetching data for 2024-12-21...\n",
      "No Technology sector data for 2024-12-21\n",
      "Fetching data for 2024-12-20...\n",
      "Data for 2024-12-20 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-19...\n",
      "Data for 2024-12-19 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-18...\n",
      "Data for 2024-12-18 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-17...\n",
      "Data for 2024-12-17 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-16...\n",
      "Data for 2024-12-16 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-15...\n",
      "No Technology sector data for 2024-12-15\n",
      "Fetching data for 2024-12-14...\n",
      "No Technology sector data for 2024-12-14\n",
      "Fetching data for 2024-12-13...\n",
      "Data for 2024-12-13 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-12...\n",
      "Data for 2024-12-12 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-11...\n",
      "Data for 2024-12-11 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-10...\n",
      "Data for 2024-12-10 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-09...\n",
      "Data for 2024-12-09 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-08...\n",
      "No Technology sector data for 2024-12-08\n",
      "Fetching data for 2024-12-07...\n",
      "No Technology sector data for 2024-12-07\n",
      "Fetching data for 2024-12-06...\n",
      "Data for 2024-12-06 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-05...\n",
      "Data for 2024-12-05 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-04...\n",
      "Data for 2024-12-04 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-03...\n",
      "Data for 2024-12-03 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-02...\n",
      "Data for 2024-12-02 written to sector_pe_ratio.csv\n",
      "Fetching data for 2024-12-01...\n",
      "No Technology sector data for 2024-12-01\n",
      "Fetching data for 2024-11-30...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m\n\u001b[0;32m     26\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m: date_str,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexchange\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNYSE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapikey\u001b[39m\u001b[38;5;124m\"\u001b[39m: fmp_api_key\n\u001b[0;32m     30\u001b[0m }\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     35\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:1430\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1430\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_url = \"https://financialmodelingprep.com/api/v4/sector_price_earning_ratio\"\n",
    "\n",
    "# Define CSV file name\n",
    "csv_filename = \"sector_pe_ratio.csv\"\n",
    "\n",
    "# Define CSV headers\n",
    "headers = [\"date\", \"sector\", \"exchange\", \"pe\"]\n",
    "\n",
    "# Set start and end dates\n",
    "start_date = datetime(1980, 12, 12)\n",
    "current_date = datetime.today()\n",
    "\n",
    "# Create and write headers to the CSV file\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)\n",
    "\n",
    "# Loop backwards day by day\n",
    "while current_date >= start_date:\n",
    "    # Define the date parameter\n",
    "    date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    print(f\"Fetching data for {date_str}...\")\n",
    "\n",
    "    # API Request\n",
    "    params = {\n",
    "        \"date\": date_str,\n",
    "        \"exchange\": \"NYSE\",\n",
    "        \"apikey\": fmp_api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Filter only Technology sector data\n",
    "        tech_sector_data = [record for record in data if record.get(\"sector\") == \"Technology\"]\n",
    "\n",
    "        # If data is found, append to CSV file\n",
    "        if tech_sector_data:\n",
    "            with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                for record in tech_sector_data:\n",
    "                    row = [record.get(\"date\", \"\"), record.get(\"sector\", \"\"), record.get(\"exchange\", \"\"), record.get(\"pe\", \"\")]\n",
    "                    writer.writerow(row)\n",
    "\n",
    "            print(f\"Data for {date_str} written to {csv_filename}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"No Technology sector data for {date_str}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {date_str}: {e}\")\n",
    "\n",
    "    # Move back one day\n",
    "    current_date -= timedelta(days=1)\n",
    "\n",
    "print(f\"\\nAll historical data successfully written to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched data for 2024-12-04 to 2025-03-04\n",
      "Fetched data for 2024-09-05 to 2024-12-04\n",
      "Fetched data for 2024-06-07 to 2024-09-05\n",
      "Fetched data for 2024-03-09 to 2024-06-07\n",
      "Fetched data for 2023-12-10 to 2024-03-09\n",
      "Fetched data for 2023-09-11 to 2023-12-10\n",
      "Fetched data for 2023-06-13 to 2023-09-11\n",
      "Fetched data for 2023-03-15 to 2023-06-13\n",
      "Fetched data for 2022-12-15 to 2023-03-15\n",
      "Fetched data for 2022-09-16 to 2022-12-15\n",
      "Fetched data for 2022-06-18 to 2022-09-16\n",
      "Fetched data for 2022-03-20 to 2022-06-18\n",
      "Fetched data for 2021-12-20 to 2022-03-20\n",
      "Fetched data for 2021-09-21 to 2021-12-20\n",
      "Fetched data for 2021-06-23 to 2021-09-21\n",
      "Fetched data for 2021-03-25 to 2021-06-23\n",
      "Fetched data for 2020-12-25 to 2021-03-25\n",
      "Fetched data for 2020-09-26 to 2020-12-25\n",
      "Fetched data for 2020-06-28 to 2020-09-26\n",
      "Fetched data for 2020-03-30 to 2020-06-28\n",
      "Fetched data for 2019-12-31 to 2020-03-30\n",
      "Fetched data for 2019-10-02 to 2019-12-31\n",
      "Fetched data for 2019-07-04 to 2019-10-02\n",
      "Fetched data for 2019-04-05 to 2019-07-04\n",
      "Fetched data for 2019-01-05 to 2019-04-05\n",
      "Fetched data for 2018-10-07 to 2019-01-05\n",
      "Fetched data for 2018-07-09 to 2018-10-07\n",
      "Fetched data for 2018-04-10 to 2018-07-09\n",
      "Fetched data for 2018-01-10 to 2018-04-10\n",
      "Fetched data for 2017-10-12 to 2018-01-10\n",
      "Fetched data for 2017-07-14 to 2017-10-12\n",
      "Fetched data for 2017-04-15 to 2017-07-14\n",
      "Fetched data for 2017-01-15 to 2017-04-15\n",
      "Fetched data for 2016-10-17 to 2017-01-15\n",
      "Fetched data for 2016-07-19 to 2016-10-17\n",
      "Fetched data for 2016-04-20 to 2016-07-19\n",
      "Fetched data for 2016-01-21 to 2016-04-20\n",
      "Fetched data for 2015-10-23 to 2016-01-21\n",
      "Fetched data for 2015-07-25 to 2015-10-23\n",
      "Fetched data for 2015-04-26 to 2015-07-25\n",
      "Fetched data for 2015-01-26 to 2015-04-26\n",
      "Fetched data for 2014-10-28 to 2015-01-26\n",
      "Fetched data for 2014-07-30 to 2014-10-28\n",
      "Fetched data for 2014-05-01 to 2014-07-30\n",
      "Fetched data for 2014-01-31 to 2014-05-01\n",
      "Fetched data for 2013-11-02 to 2014-01-31\n",
      "Fetched data for 2013-08-04 to 2013-11-02\n",
      "Fetched data for 2013-05-06 to 2013-08-04\n",
      "Fetched data for 2013-02-05 to 2013-05-06\n",
      "Fetched data for 2012-11-07 to 2013-02-05\n",
      "Fetched data for 2012-08-09 to 2012-11-07\n",
      "Fetched data for 2012-05-11 to 2012-08-09\n",
      "Fetched data for 2012-02-11 to 2012-05-11\n",
      "Fetched data for 2011-11-13 to 2012-02-11\n",
      "Fetched data for 2011-08-15 to 2011-11-13\n",
      "Fetched data for 2011-05-17 to 2011-08-15\n",
      "Fetched data for 2011-02-16 to 2011-05-17\n",
      "Fetched data for 2010-11-18 to 2011-02-16\n",
      "Fetched data for 2010-08-20 to 2010-11-18\n",
      "Fetched data for 2010-05-22 to 2010-08-20\n",
      "Fetched data for 2010-02-21 to 2010-05-22\n",
      "Fetched data for 2009-11-23 to 2010-02-21\n",
      "Fetched data for 2009-08-25 to 2009-11-23\n",
      "Fetched data for 2009-05-27 to 2009-08-25\n",
      "Fetched data for 2009-02-26 to 2009-05-27\n",
      "Fetched data for 2008-11-28 to 2009-02-26\n",
      "Fetched data for 2008-08-30 to 2008-11-28\n",
      "Fetched data for 2008-06-01 to 2008-08-30\n",
      "Fetched data for 2008-03-03 to 2008-06-01\n",
      "Fetched data for 2007-12-04 to 2008-03-03\n",
      "Fetched data for 2007-09-05 to 2007-12-04\n",
      "Fetched data for 2007-06-07 to 2007-09-05\n",
      "Fetched data for 2007-03-09 to 2007-06-07\n",
      "Fetched data for 2006-12-09 to 2007-03-09\n",
      "Fetched data for 2006-09-10 to 2006-12-09\n",
      "Fetched data for 2006-06-12 to 2006-09-10\n",
      "Fetched data for 2006-03-14 to 2006-06-12\n",
      "Fetched data for 2005-12-14 to 2006-03-14\n",
      "Fetched data for 2005-09-15 to 2005-12-14\n",
      "Fetched data for 2005-06-17 to 2005-09-15\n",
      "Fetched data for 2005-03-19 to 2005-06-17\n",
      "Fetched data for 2004-12-19 to 2005-03-19\n",
      "Fetched data for 2004-09-20 to 2004-12-19\n",
      "Fetched data for 2004-06-22 to 2004-09-20\n",
      "Fetched data for 2004-03-24 to 2004-06-22\n",
      "Fetched data for 2003-12-25 to 2004-03-24\n",
      "Fetched data for 2003-09-26 to 2003-12-25\n",
      "Fetched data for 2003-06-28 to 2003-09-26\n",
      "Fetched data for 2003-03-30 to 2003-06-28\n",
      "Fetched data for 2002-12-30 to 2003-03-30\n",
      "Fetched data for 2002-10-01 to 2002-12-30\n",
      "Fetched data for 2002-07-03 to 2002-10-01\n",
      "Fetched data for 2002-04-04 to 2002-07-03\n",
      "Fetched data for 2002-01-04 to 2002-04-04\n",
      "Fetched data for 2001-10-06 to 2002-01-04\n",
      "Fetched data for 2001-07-08 to 2001-10-06\n",
      "Fetched data for 2001-04-09 to 2001-07-08\n",
      "Fetched data for 2001-01-09 to 2001-04-09\n",
      "Fetched data for 2000-10-11 to 2001-01-09\n",
      "Fetched data for 2000-07-13 to 2000-10-11\n",
      "Fetched data for 2000-04-14 to 2000-07-13\n",
      "Fetched data for 2000-01-15 to 2000-04-14\n",
      "Fetched data for 1999-10-17 to 2000-01-15\n",
      "Fetched data for 1999-07-19 to 1999-10-17\n",
      "Fetched data for 1999-04-20 to 1999-07-19\n",
      "Fetched data for 1999-01-20 to 1999-04-20\n",
      "Fetched data for 1998-10-22 to 1999-01-20\n",
      "Fetched data for 1998-07-24 to 1998-10-22\n",
      "Fetched data for 1998-04-25 to 1998-07-24\n",
      "Fetched data for 1998-01-25 to 1998-04-25\n",
      "Fetched data for 1997-10-27 to 1998-01-25\n",
      "Fetched data for 1997-07-29 to 1997-10-27\n",
      "Fetched data for 1997-04-30 to 1997-07-29\n",
      "Fetched data for 1997-01-30 to 1997-04-30\n",
      "Fetched data for 1996-11-01 to 1997-01-30\n",
      "Fetched data for 1996-08-03 to 1996-11-01\n",
      "Fetched data for 1996-05-05 to 1996-08-03\n",
      "Fetched data for 1996-02-05 to 1996-05-05\n",
      "Fetched data for 1995-11-07 to 1996-02-05\n",
      "Fetched data for 1995-08-09 to 1995-11-07\n",
      "Fetched data for 1995-05-11 to 1995-08-09\n",
      "Fetched data for 1995-02-10 to 1995-05-11\n",
      "Fetched data for 1994-11-12 to 1995-02-10\n",
      "Fetched data for 1994-08-14 to 1994-11-12\n",
      "Fetched data for 1994-05-16 to 1994-08-14\n",
      "Fetched data for 1994-02-15 to 1994-05-16\n",
      "Fetched data for 1993-11-17 to 1994-02-15\n",
      "Fetched data for 1993-08-19 to 1993-11-17\n",
      "Fetched data for 1993-05-21 to 1993-08-19\n",
      "Fetched data for 1993-02-20 to 1993-05-21\n",
      "Fetched data for 1992-11-22 to 1993-02-20\n",
      "Fetched data for 1992-08-24 to 1992-11-22\n",
      "Fetched data for 1992-05-26 to 1992-08-24\n",
      "Fetched data for 1992-02-26 to 1992-05-26\n",
      "Fetched data for 1991-11-28 to 1992-02-26\n",
      "Fetched data for 1991-08-30 to 1991-11-28\n",
      "Fetched data for 1991-06-01 to 1991-08-30\n",
      "Fetched data for 1991-03-03 to 1991-06-01\n",
      "Fetched data for 1990-12-03 to 1991-03-03\n",
      "Fetched data for 1990-09-04 to 1990-12-03\n",
      "Fetched data for 1990-06-06 to 1990-09-04\n",
      "Fetched data for 1990-03-08 to 1990-06-06\n",
      "Fetched data for 1989-12-08 to 1990-03-08\n",
      "Fetched data for 1989-09-09 to 1989-12-08\n",
      "Fetched data for 1989-06-11 to 1989-09-09\n",
      "Fetched data for 1989-03-13 to 1989-06-11\n",
      "Fetched data for 1988-12-13 to 1989-03-13\n",
      "Fetched data for 1988-09-14 to 1988-12-13\n",
      "Fetched data for 1988-06-16 to 1988-09-14\n",
      "Fetched data for 1988-03-18 to 1988-06-16\n",
      "Fetched data for 1987-12-19 to 1988-03-18\n",
      "Fetched data for 1987-09-20 to 1987-12-19\n",
      "Fetched data for 1987-06-22 to 1987-09-20\n",
      "Fetched data for 1987-03-24 to 1987-06-22\n",
      "Fetched data for 1986-12-24 to 1987-03-24\n",
      "Fetched data for 1986-09-25 to 1986-12-24\n",
      "Fetched data for 1986-06-27 to 1986-09-25\n",
      "Fetched data for 1986-03-29 to 1986-06-27\n",
      "Fetched data for 1985-12-29 to 1986-03-29\n",
      "Fetched data for 1985-09-30 to 1985-12-29\n",
      "Fetched data for 1985-07-02 to 1985-09-30\n",
      "Fetched data for 1985-04-03 to 1985-07-02\n",
      "Fetched data for 1985-01-03 to 1985-04-03\n",
      "Fetched data for 1984-10-05 to 1985-01-03\n",
      "Fetched data for 1984-07-07 to 1984-10-05\n",
      "Fetched data for 1984-04-08 to 1984-07-07\n",
      "Fetched data for 1984-01-09 to 1984-04-08\n",
      "Fetched data for 1983-10-11 to 1984-01-09\n",
      "Fetched data for 1983-07-13 to 1983-10-11\n",
      "Fetched data for 1983-04-14 to 1983-07-13\n",
      "Fetched data for 1983-01-14 to 1983-04-14\n",
      "Fetched data for 1982-10-16 to 1983-01-14\n",
      "Fetched data for 1982-07-18 to 1982-10-16\n",
      "Fetched data for 1982-04-19 to 1982-07-18\n",
      "Fetched data for 1982-01-19 to 1982-04-19\n",
      "Fetched data for 1981-10-21 to 1982-01-19\n",
      "Fetched data for 1981-07-23 to 1981-10-21\n",
      "Fetched data for 1981-04-24 to 1981-07-23\n",
      "Fetched data for 1981-01-24 to 1981-04-24\n",
      "Fetched data for 1980-12-12 to 1981-01-24\n",
      "Sector performance data written to historical_sector_performance.csv with 5131 records!\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://financialmodelingprep.com/api/v3/historical-sectors-performance\"\n",
    "\n",
    "# Define date variables\n",
    "end_date = \"1980-12-12\"\n",
    "current_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Function to subtract months from a date\n",
    "def subtract_months(start_date, months):\n",
    "    new_date = datetime.strptime(start_date, \"%Y-%m-%d\") - timedelta(days=months*30)\n",
    "    return new_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define CSV file name\n",
    "csv_file = \"historical_sector_performance.csv\"\n",
    "headers = [\n",
    "    \"date\",\n",
    "    \"basicMaterialsChangesPercentage\",\n",
    "    \"communicationServicesChangesPercentage\",\n",
    "    \"consumerCyclicalChangesPercentage\",\n",
    "    \"consumerDefensiveChangesPercentage\",\n",
    "    \"energyChangesPercentage\",\n",
    "    \"financialServicesChangesPercentage\",\n",
    "    \"healthcareChangesPercentage\",\n",
    "    \"industrialsChangesPercentage\",\n",
    "    \"realEstateChangesPercentage\",\n",
    "    \"technologyChangesPercentage\",\n",
    "    \"utilitiesChangesPercentage\"\n",
    "]\n",
    "formatted_data = []\n",
    "\n",
    "# Initialize the starting date (current date)\n",
    "current_from_date = current_date\n",
    "\n",
    "# Loop to fetch data in 3-month chunks\n",
    "while current_from_date > end_date:\n",
    "    # Calculate the \"from\" and \"to\" dates for the API call (3 months at a time)\n",
    "    current_to_date = current_from_date\n",
    "    current_from_date = subtract_months(current_from_date, 3)\n",
    "\n",
    "    # Ensure \"from_date\" does not go earlier than the end date\n",
    "    if current_from_date < end_date:\n",
    "        current_from_date = end_date\n",
    "\n",
    "    # Define API request parameters\n",
    "    params = {\n",
    "        \"from\": current_from_date,\n",
    "        \"to\": current_to_date,\n",
    "        \"apikey\": fmp_api_key\n",
    "    }\n",
    "\n",
    "    # Fetch sector performance data\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Handle HTTP errors\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if data is a list and iterate over it\n",
    "        if isinstance(data, list):  \n",
    "            for record in data:\n",
    "                formatted_data.append({\n",
    "                    \"date\": record.get(\"date\", \"\"),\n",
    "                    \"basicMaterialsChangesPercentage\": record.get(\"basicMaterialsChangesPercentages\", \"\"),\n",
    "                    \"communicationServicesChangesPercentage\": record.get(\"communicationServicesChangesPercentage\", \"\"),\n",
    "                    \"consumerCyclicalChangesPercentage\": record.get(\"consumerCyclicalChangesPercentage\", \"\"),\n",
    "                    \"consumerDefensiveChangesPercentage\": record.get(\"consumerDefensiveChangesPercentage\", \"\"),\n",
    "                    \"energyChangesPercentage\": record.get(\"energyChangesPercentage\", \"\"),\n",
    "                    \"financialServicesChangesPercentage\": record.get(\"financialServicesChangesPercentag\", \"\"),\n",
    "                    \"healthcareChangesPercentage\": record.get(\"healthcareChangesPercentage\", \"\"),\n",
    "                    \"industrialsChangesPercentage\": record.get(\"industrialsChangesPercentage\", \"\"),\n",
    "                    \"realEstateChangesPercentage\": record.get(\"realEstateChangesPercentage\", \"\"),\n",
    "                    \"technologyChangesPercentage\": record.get(\"technologyChangesPercentage\", \"\"),\n",
    "                    \"utilitiesChangesPercentage\": record.get(\"utilitiesChangesPercentage\", \"\"),\n",
    "                })\n",
    "            print(f\"Fetched data for {current_from_date} to {current_to_date}\")\n",
    "        else:\n",
    "            print(f\"Unexpected data format from API for {current_from_date} to {current_to_date}: {data}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching sector performance for {current_from_date} to {current_to_date}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Write data to CSV after all batches\n",
    "if formatted_data:\n",
    "    with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(formatted_data)\n",
    "\n",
    "    print(f\"Sector performance data written to {csv_file} with {len(formatted_data)} records!\")\n",
    "else:\n",
    "    print(\"No sector performance data found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
