{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMP News API Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully from .env file.\n"
     ]
    }
   ],
   "source": [
    "# loading in api file\n",
    "load_dotenv(r\"api.env\")\n",
    "\n",
    "# Obtaining key for API file\n",
    "fmp_api_key = os.getenv(\"fmp_api_key\")\n",
    "if not fmp_api_key:\n",
    "    raise ValueError(\"No API key set for fmp_api_key in .env file\")\n",
    "\n",
    "print(\"API key loaded successfully from .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up dictionaries in lists to facilitate looping through\n",
    "stock_news = [\n",
    "    {\"symbol\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"symbol\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"symbol\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"symbol\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"symbol\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"symbol\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "press_releases = [\n",
    "    {\"company\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"company\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"company\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"company\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"company\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"company\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]\n",
    "\n",
    "historical_social_sentiment = [\n",
    "    {\"symbol\": \"AMZN\", \"ipo_date\": \"1997-05-15\"},\n",
    "    {\"symbol\": \"AAPL\", \"ipo_date\": \"1980-12-12\"},\n",
    "    {\"symbol\": \"GOOG\", \"ipo_date\": \"2004-08-19\"},\n",
    "    {\"symbol\": \"MSFT\", \"ipo_date\": \"1986-03-13\"},\n",
    "    {\"symbol\": \"META\", \"ipo_date\": \"2012-05-18\"},\n",
    "    {\"symbol\": \"NVDA\", \"ipo_date\": \"1999-01-22\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock News Data Pull "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining today's date\n",
    "current_date =date.today()\n",
    "\n",
    "# Obtaining stock news data for each tech company\n",
    "for news in stock_news:\n",
    "\n",
    "    # setting up url path & parameter values\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/stock_news?\"\n",
    "    page = 0\n",
    "    limit = 2000\n",
    "\n",
    "    # creating empty list to capture news data\n",
    "    all_news = []\n",
    "\n",
    "    # looping through pages of data to obtain stock news data\n",
    "    while True:\n",
    "        \n",
    "        # applying parameter values to required params\n",
    "        params = {\n",
    "        \"apikey\" : fmp_api_key,\n",
    "        \"tickers\" : news['symbol'],\n",
    "        \"page\": page,\n",
    "        'from': news['ipo_date'],\n",
    "        'to': current_date,\n",
    "        'limit': limit\n",
    "        }\n",
    "\n",
    "        # requesting data\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {news['symbol']} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        # setting successfully requested data to variable data\n",
    "        data = response.json()\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # appending additional data to list\n",
    "        all_news.extend(data)\n",
    "\n",
    "        # moving to next page\n",
    "        page += 1\n",
    "\n",
    "    csv_file = f\"{news['symbol']}_news_data.csv\"\n",
    "    headers = [\"published_date\", \"Headline\", \"Brief\", \"URL\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    # writing all pulled news data to their respective companies in csv files\n",
    "    if all_news:\n",
    "        for record in all_news:\n",
    "\n",
    "            formats = ['%Y-%m-%d %I:%M:%S %p', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "            if 'publishedDate' in record:\n",
    "                for fmt in formats:\n",
    "                    try:\n",
    "                        record['publishedDate'] = datetime.strptime(record['publishedDate'], fmt).strftime('%d-%m-%Y')\n",
    "                        break\n",
    "                    except ValueError as e:\n",
    "                        continue\n",
    "\n",
    "            formatted_data.append({\n",
    "            \"published_date\": record.get(\"publishedDate\", \"\"),\n",
    "            \"Headline\": record.get(\"title\", \"\"),\n",
    "            \"Brief\": record.get(\"text\", \"\"),\n",
    "            \"URL\": record.get(\"url\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "        \n",
    "        print(f\"Data for {news['symbol']} written to {csv_file} with {len(formatted_data)} articles!\")\n",
    "    else:\n",
    "        print(f\"No data found for {news['symbol']}.\")\n",
    "\n",
    "print(\"All stock data successfully written to their respective CSV files.\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Press Release Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining stock press release data for each tech company\n",
    "for press_release in press_releases:\n",
    "\n",
    "    # setting up url path & parameter values\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/press-releases/{press_release['company']}?\"\n",
    "\n",
    "    # setting page to zero\n",
    "    page = 0\n",
    "\n",
    "    # creating empty list to capture all press release data\n",
    "    all_press_releases = []\n",
    "\n",
    "    max_pages = 300\n",
    "\n",
    "    # looping through pages of data to obtain stock press release data\n",
    "    while page < max_pages:\n",
    "\n",
    "        # applying parameter values to required params\n",
    "        params = {\n",
    "        \"apikey\" : fmp_api_key,\n",
    "        \"page\": page\n",
    "        }\n",
    "\n",
    "        # requesting data\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {press_release['company']} on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # setting successfully requested data to variable data\n",
    "        data = response.json()\n",
    "        \n",
    "        print(f\"Fetching data for {press_release['company']} - Page {page}...\")\n",
    "\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "\n",
    "        # appending additional data to list\n",
    "        all_press_releases.extend(data)\n",
    "\n",
    "        # moving to next page\n",
    "        page += 1\n",
    "\n",
    "    csv_file = f\"{press_release['company']}_press_release_data.csv\"\n",
    "    headers = [\"release_date\", \"Headline\", \"Brief\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    # writing all pulled press release data to their respective companies in csv files\n",
    "    if all_press_releases:\n",
    "        for record in all_press_releases:\n",
    "\n",
    "            formats = ['%Y-%m-%d %I:%M:%S %p', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "            if 'date' in record:\n",
    "                for fmt in formats:\n",
    "                    try:\n",
    "                        record['date'] = datetime.strptime(record['date'], fmt).strftime('%d-%m-%Y')\n",
    "                        break\n",
    "                    except ValueError as e:\n",
    "                        continue\n",
    "\n",
    "            formatted_data.append({\n",
    "            \"release_date\": record.get(\"date\", \"\"),\n",
    "            \"Headline\": record.get(\"title\", \"\"),\n",
    "            \"Brief\": record.get(\"text\", \"\")\n",
    "            })\n",
    "\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(formatted_data)\n",
    "        \n",
    "        print(f\"Data for {press_release['company']} written to {csv_file} with {len(formatted_data)} press releases!\")\n",
    "    else:\n",
    "        print(f\"No data found for {press_release['company']}.\")\n",
    "\n",
    "print(\"All stock data successfully written to their respective CSV files.\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating a function to clean text for VADER sentiment analysis\n",
    "def text_cleaning(text, dataset):\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace(r\"http\\S+|www\\S+\", \"\", regex=True) # removing URLS\n",
    "    text = text.str.replace(r\"[^a-zA-Z0-9$%.,!?'\\s-]\", \"\", regex=True) # keeping relevant characters\n",
    "    text = text.str.replace(r\"\\s+\", \" \", regex=True).str.strip() # removing extra spaces\n",
    "    text = text.str.replace(r\"\\b(\\d+)%\", r\"\\1 percent\", regex=True) # converting percentages\n",
    "    text = text.str.replace(r\"\\b(\\d+)M\\b\", r\"\\1 million\", regex=True)  # Convert M to million\n",
    "    text = text.str.replace(r\"\\b(\\d+)B\\b\", r\"\\1 billion\", regex=True)  # Convert B to billion\n",
    "\n",
    "    # splitting texts manually based on \".!?\"\n",
    "    sentences = text.str.split(r'[.!?]\\s+', regex=True)\n",
    "    \n",
    "    # creating column for the cleaned text and naming it 'cleaned'\n",
    "    dataset['cleaned'] = sentences\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up lists to loop through for sentiment score and weighted scores analysis\n",
    "stock_news_datasets = [\n",
    "    \"AMZN_news_data.csv\",\n",
    "    \"AAPL_news_data.csv\",\n",
    "    \"GOOG_news_data.csv\",\n",
    "    \"MSFT_news_data.csv\",\n",
    "    \"META_news_data.csv\",\n",
    "    \"NVDA_news_data.csv\"\n",
    "]\n",
    "\n",
    "news_press_releases_datasets = [\n",
    "    \"AMZN_press_release_data.csv\",\n",
    "    \"AAPL_press_release_data.csv\",\n",
    "    \"GOOG_press_release_data.csv\",\n",
    "    \"MSFT_press_release_data.csv\",\n",
    "    \"META_press_release_data.csv\",\n",
    "    \"NVDA_press_release_data.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to obtain sentiment scores\n",
    "def sentiment_score_calculator(datasets_lists):\n",
    "\n",
    "    # Initializing VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "    # looping through datasets \n",
    "    for file in datasets_lists:\n",
    "        \n",
    "        # reading in file \n",
    "        news_press_release = pd.read_csv(file)\n",
    "\n",
    "        # combining two text columns into one column to clean text in one go\n",
    "        news_press_release['raw'] = news_press_release.apply(lambda row: row['Headline'] if pd.isna(row['Brief']) \n",
    "                                                        else (row['Brief'] if pd.isna(row['Headline']) \n",
    "                                                            else row['Headline'] + \". \" + row['Brief']), axis=1)\n",
    "\n",
    "        # cleaning text cleaning function with VADER appliation\n",
    "        text_cleaning(news_press_release['raw'], news_press_release)\n",
    "\n",
    "\n",
    "        # creating empty lists\n",
    "        score_rating = []\n",
    "        score = []\n",
    "\n",
    "        # looping through each row in the column 'cleaned'\n",
    "        for row in news_press_release['cleaned']:\n",
    "            \n",
    "            row_score = 0\n",
    "\n",
    "            # looping through each sentence in current row and using vader to score each sentence\n",
    "            for sentence in row:\n",
    "                try:\n",
    "                    print(sentence)\n",
    "                    sentence_score = analyzer.polarity_scores(sentence)[\"compound\"] # vader polarity score analyzer\n",
    "                    print(sentence_score)\n",
    "                    print(\"Done with sentence_score in row, onto next sentence.\")\n",
    "\n",
    "                    # accumulating each sentence's score for current row\n",
    "                    row_score += sentence_score\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error processing sentence: {sentence}\\n{e}\")\n",
    "                    continue\n",
    "\n",
    "            # averaging row scores and appending to score\n",
    "            avg_row_score = row_score / len(row)\n",
    "            score.append(avg_row_score)\n",
    "\n",
    "            print(f\"row_score: {avg_row_score}\")\n",
    "            print(len(row))\n",
    "            print(\"end of row, onto next row.\")\n",
    "\n",
    "            # classifying averaged row scores into various score ratings\n",
    "            if avg_row_score > 0.05 and avg_row_score < 0.5:\n",
    "                score_rating.append(\"weakly_positive\")\n",
    "                print(\"Weakly Positive\")\n",
    "            elif avg_row_score > 0.5:\n",
    "                score_rating.append(\"strongly_positive\")\n",
    "                print(\"Strongly Positive\")\n",
    "            elif avg_row_score > -0.5 and avg_row_score < -0.05:\n",
    "                score_rating.append(\"weakly_negative\")\n",
    "                print(\"Weakly Negative\")\n",
    "            elif avg_row_score < -0.5:\n",
    "                score_rating.append(\"strongly_negative\")\n",
    "                print(\"Strongly Negative\")\n",
    "            else:\n",
    "                score_rating.append(\"neutral\")\n",
    "                print(\"Neutral\")\n",
    "\n",
    "        # creating columns for sentiment and score\n",
    "        news_press_release['sentiment'] = score_rating\n",
    "        news_press_release['score'] = score\n",
    "\n",
    "        # writing the updated vader sentiment scores to each file \n",
    "        news_press_release.to_csv(file, index=False)\n",
    "        print(f\"Finished updating {file} with sentiment and score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_sentiments(datasets_lists):\n",
    "\n",
    "    # looping through each dataset and applying aggregated sentiments\n",
    "    for file in datasets_lists:\n",
    "\n",
    "        # reading in dataset\n",
    "        news_press_release = pd.read_csv(file)\n",
    "\n",
    "        # Checking if either 'published_date' or 'release_date' exists and group by the first one found\n",
    "        date_column = 'published_date' if 'published_date' in news_press_release.columns else 'release_date'\n",
    "\n",
    "        # computing average score per unique publilshed_date\n",
    "        avg_scores = news_press_release.groupby(date_column)['score'].mean().reset_index()\n",
    "\n",
    "        # classifying weighted daily scores into classes\n",
    "\n",
    "        # Defining conditions for classification\n",
    "        conditions = [\n",
    "            avg_scores['score'] > 0.5,\n",
    "            (avg_scores['score'] > 0.05) & (avg_scores['score'] <= 0.5),\n",
    "            (avg_scores['score'] >= -0.05) & (avg_scores['score'] <= 0.05),\n",
    "            (avg_scores['score'] < -0.05) & (avg_scores['score'] >= -0.5),\n",
    "            avg_scores['score'] < -0.5\n",
    "        ]\n",
    "\n",
    "        # Defining corresponding classifications\n",
    "        classifications = [\n",
    "            \"strongly_positive\",\n",
    "            \"weakly_positive\",\n",
    "            \"neutral\",\n",
    "            \"weakly_negative\",\n",
    "            \"strongly_negative\"\n",
    "        ]\n",
    "\n",
    "        # Assigning classifications based on conditions\n",
    "        avg_scores['weighted_daily_sentiment'] = np.select(conditions, classifications, default=\"neutral\")\n",
    "        \n",
    "        # merging to assign classifications to each row\n",
    "        news_press_release = news_press_release.merge(avg_scores[[date_column, 'weighted_daily_sentiment']], on=date_column, how='left')\n",
    "\n",
    "        # writing to csv files\n",
    "        news_press_release.to_csv(file, index=False)\n",
    "        print(f\"Finished updating {file} with weighted daily sentiment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentiments score for news and press release datasets for each company\n",
    "sentiment_score_calculator(stock_news_datasets)\n",
    "sentiment_score_calculator(news_press_releases_datasets)\n",
    "\n",
    "# Calculating aggregated sentiments(daily) for news and press release datasets for each company\n",
    "aggregated_sentiments(stock_news_datasets)\n",
    "aggregated_sentiments(news_press_releases_datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
